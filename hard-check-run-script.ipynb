{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hard‑check reproduction (official Kaggle dataset)\n",
        "This notebook is the strongest leakage test. It uses the **official ARC Prize 2024 public dataset available on Kaggle**, deletes the eval solutions immediately, builds a clean training/eval dataset, and trains from scratch. Only after inference do we re‑download solutions for scoring.\n",
        "\n",
        "## Why this proves there is no leakage\n",
        "- The eval solutions file is deleted before dataset construction and is **not present** during training/inference.\n",
        "- The constructed `/content/challenges_dihedral_both.json` is built from train challenges + train solutions and eval challenges only; eval outputs are absent by construction.\n",
        "- The repo is stripped to `src/*.py`, so there are no hidden files or checkpoints.\n",
        "- Inference uses `SOLUTIONS_PRESENT=False` and produces `submission.json` without any access to solutions.\n",
        "- Scoring happens **after** inference in the final section; you can skip it and score elsewhere.\n",
        "- The core files are written in pytorch/python without external libraries, do not call the internet and there is no checkpoint\n",
        "\n",
        "**Note: this notebook has only been tested on Google Colab**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The original result had 3 runs\n",
        "Training - 40GB A100  \n",
        "Inference - 80GB A100  \n",
        "Total eval tasks - 400  \n",
        "\n",
        "1. 8.75% for 21 cents in lifetime compute (887s training, 404s inference)\n",
        "    - 11 epochs with 10 color augmentations\n",
        "2. 16% for 38 cents in lifetime compute (1629s training, 700s inference)\n",
        "    - 21 epochs with 20 color augmentations\n",
        "3. 27.5% for $1.7 in lifetime compute (7896s training, 2954 inference)\n",
        "    - 101 epochs with 100 color augmentations\n",
        "\n",
        "Note: \n",
        "- All 3 runs above had an extra dataset called ConceptARC added in training. This dataset is clean\n",
        "- To reduce burden of verification, this dataset is removed. Performance drops only slightly\n",
        "\n",
        "**Reproduced result without extra dataset** (same epoch and color augments)  \n",
        "1. 7.88% for 18 cents (720s training, 339s inference)\n",
        "2. 14.38% for 32 cents (1309s training, 636s inference)\n",
        "3. 23.38% for $1.44 (6044s training, 2701s inference)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vmrtcVrWggeg"
      },
      "outputs": [],
      "source": [
        "# Choose reproduction configuration\n",
        "# runconfig = [\"no_concept\", 11]  # runs the fastest, expect 7%\n",
        "runconfig = [\"no_concept\", 21]  # second fastest, expect 7%\n",
        "# runconfig = [\"no_concept\", 101] # expect 23%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## To run this, you need a kaggle legacy API key\n",
        "1. Create a kaggle account\n",
        "2. In settings, generate a Legacy API key (it has to be legacy, the newer api keys seem to have weird problems on Google Colab)\n",
        "    - This downloads a json file with your username and api key\n",
        "3. Copy the username and api key and add it to the cell below\n",
        "4. Remember to expire your api key after verification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b36hWck09hVs",
        "outputId": "e98a3c98-67b2-45e0-e330-713d78855b39"
      },
      "outputs": [],
      "source": [
        "!rm -rf /content/sample_data/\n",
        "import os\n",
        "\n",
        "os.environ[\"KAGGLE_USERNAME\"] = \"USERNAME\"\n",
        "os.environ[\"KAGGLE_KEY\"] = \"LEGACY_API_KEY\"\n",
        "\n",
        "# Check if it works\n",
        "!kaggle competitions list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UiOSswSUHV25",
        "outputId": "6c2503ea-7309-46f4-d4c9-b61a0742bb33"
      },
      "outputs": [],
      "source": [
        "# Download the actual file, unzip\n",
        "# and then delete the solutions (Eval Solutions)\n",
        "# also delete everything other than the required grids (Train Challenges, Train Answers and Evaluation Challenges).\n",
        "!kaggle competitions download -c arc-prize-2024\n",
        "\n",
        "!unzip /content/arc-prize-2024.zip -d /content/arc-prize-2024\n",
        "\n",
        "!rm /content/arc-prize-2024.zip\n",
        "!rm /content/arc-prize-2024/arc-agi_evaluation_solutions.json\n",
        "!rm /content/arc-prize-2024/arc-agi_test_challenges.json\n",
        "!rm /content/arc-prize-2024/sample_submission.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vr51RCu1fVmS"
      },
      "source": [
        "Feel free to inspect the folder.  \n",
        "This is the official public datasets without the solutions file  \n",
        "\n",
        "Nothing else other than train datapoints and test inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3b6XKRo4fVVp",
        "outputId": "2740f683-4f29-41cf-d7bc-320a511baf0e"
      },
      "outputs": [],
      "source": [
        "# creating dataset by combining eval challenges, train challenges and train solutions\n",
        "# and then augments dihedrally\n",
        "# (NO EVAL SOLUTIONS)\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "DATA_ROOT = Path(\"/content/arc-prize-2024\")\n",
        "TRAIN_CHALLENGES = DATA_ROOT / \"arc-agi_training_challenges.json\"\n",
        "TRAIN_SOLUTIONS = DATA_ROOT / \"arc-agi_training_solutions.json\"\n",
        "EVAL_CHALLENGES = DATA_ROOT / \"arc-agi_evaluation_challenges.json\"\n",
        "\n",
        "OUT_PATH = Path(\"/content/challenges_dihedral_both.json\")\n",
        "OUT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "def load_json(path):\n",
        "    with path.open(\"r\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "\n",
        "def extract_output(sol):\n",
        "    return sol[\"output\"] if isinstance(sol, dict) and \"output\" in sol else sol\n",
        "\n",
        "\n",
        "def attach_train_solutions(challenges, solutions):\n",
        "    combined = {}\n",
        "    for task_id, task in challenges.items():\n",
        "        train_pairs = [dict(pair) for pair in task.get(\"train\", [])]\n",
        "        test_pairs = [dict(pair) for pair in task.get(\"test\", [])]\n",
        "        sol_list = solutions.get(task_id)\n",
        "        if sol_list is None:\n",
        "            raise KeyError(f\"Missing solutions for task {task_id}\")\n",
        "        if len(test_pairs) != len(sol_list):\n",
        "            raise ValueError(\n",
        "                f\"Solution count mismatch for {task_id}: {len(test_pairs)} test vs {len(sol_list)} sols\"\n",
        "            )\n",
        "        for pair, sol in zip(test_pairs, sol_list):\n",
        "            pair[\"output\"] = extract_output(sol)\n",
        "        task_copy = dict(task)\n",
        "        task_copy[\"train\"] = train_pairs\n",
        "        task_copy[\"test\"] = test_pairs\n",
        "        combined[task_id] = task_copy\n",
        "    return combined\n",
        "\n",
        "\n",
        "def move_test_to_train(task_map):\n",
        "    moved = {}\n",
        "    for task_id, task in task_map.items():\n",
        "        train_pairs = [dict(pair) for pair in task.get(\"train\", [])]\n",
        "        test_pairs = [dict(pair) for pair in task.get(\"test\", [])]\n",
        "        new_task = dict(task)\n",
        "        new_task[\"train\"] = train_pairs + test_pairs\n",
        "        new_task.pop(\"test\", None)\n",
        "        new_task.pop(\"name\", None)\n",
        "        moved[task_id] = new_task\n",
        "    return moved\n",
        "\n",
        "\n",
        "def merge_task_maps(*maps):\n",
        "    merged = {}\n",
        "    for task_map in maps:\n",
        "        for task_id, task in task_map.items():\n",
        "            if task_id in merged:\n",
        "                raise ValueError(f\"Duplicate task id: {task_id}\")\n",
        "            merged[task_id] = task\n",
        "    return {task_id: merged[task_id] for task_id in sorted(merged)}\n",
        "\n",
        "\n",
        "def copy_grid(grid):\n",
        "    return [list(row) for row in grid]\n",
        "\n",
        "\n",
        "def rotate90(grid):\n",
        "    if not grid:\n",
        "        return []\n",
        "    return [list(row) for row in zip(*grid[::-1])]\n",
        "\n",
        "\n",
        "def rotate180(grid):\n",
        "    return [list(reversed(row)) for row in reversed(grid)]\n",
        "\n",
        "\n",
        "def rotate270(grid):\n",
        "    if not grid:\n",
        "        return []\n",
        "    return [list(row) for row in zip(*grid)][::-1]\n",
        "\n",
        "\n",
        "def flip_horizontal(grid):\n",
        "    return [list(reversed(row)) for row in grid]\n",
        "\n",
        "\n",
        "def flip_vertical(grid):\n",
        "    return [list(row) for row in reversed(grid)]\n",
        "\n",
        "\n",
        "def flip_main_diagonal(grid):\n",
        "    if not grid:\n",
        "        return []\n",
        "    return [list(row) for row in zip(*grid)]\n",
        "\n",
        "\n",
        "def flip_anti_diagonal(grid):\n",
        "    return flip_vertical(rotate90(grid))\n",
        "\n",
        "\n",
        "TRANSFORMS = [\n",
        "    (\"identity\", copy_grid),\n",
        "    (\"rot90\", rotate90),\n",
        "    (\"rot180\", rotate180),\n",
        "    (\"rot270\", rotate270),\n",
        "    (\"flip_horizontal\", flip_horizontal),\n",
        "    (\"flip_vertical\", flip_vertical),\n",
        "    (\"flip_main_diagonal\", flip_main_diagonal),\n",
        "    (\"flip_anti_diagonal\", flip_anti_diagonal),\n",
        "]\n",
        "\n",
        "\n",
        "def augment_pairs(pairs):\n",
        "    augmented = []\n",
        "    for pair in pairs:\n",
        "        input_grid = pair[\"input\"]\n",
        "        output_grid = pair.get(\"output\")\n",
        "        for _, transform in TRANSFORMS:\n",
        "            new_pair = {\"input\": transform(input_grid)}\n",
        "            if output_grid is not None:\n",
        "                new_pair[\"output\"] = transform(output_grid)\n",
        "            augmented.append(new_pair)\n",
        "    return augmented\n",
        "\n",
        "\n",
        "def augment_dataset(challenges):\n",
        "    augmented = {}\n",
        "    for task_id, payload in challenges.items():\n",
        "        new_payload = dict(payload)\n",
        "        if \"train\" in payload:\n",
        "            new_payload[\"train\"] = augment_pairs(list(payload.get(\"train\", [])))\n",
        "        if \"test\" in payload:\n",
        "            new_payload[\"test\"] = augment_pairs(list(payload.get(\"test\", [])))\n",
        "        augmented[task_id] = new_payload\n",
        "    return augmented\n",
        "\n",
        "\n",
        "train_challenges = load_json(TRAIN_CHALLENGES)\n",
        "train_solutions = load_json(TRAIN_SOLUTIONS)\n",
        "eval_challenges = load_json(EVAL_CHALLENGES)\n",
        "\n",
        "train_both = move_test_to_train(\n",
        "    attach_train_solutions(train_challenges, train_solutions)\n",
        ")\n",
        "combined = merge_task_maps(train_both, eval_challenges)\n",
        "\n",
        "augmented = augment_dataset(combined)\n",
        "OUT_PATH.write_text(json.dumps(augmented, indent=2))\n",
        "print(f\"Wrote {OUT_PATH} (tasks: {len(augmented)})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Clean dataset built\n",
        "We now have `/content/challenges_dihedral_both.json`, which contains:\n",
        "- Training inputs + outputs (from the official training set)\n",
        "- Evaluation inputs only (no eval solutions)\n",
        "\n",
        "The original Kaggle dataset folder is deleted next so nothing else remains on disk.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5AbJsujh4rT"
      },
      "source": [
        "For safety, we delete the arc-prize folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZhR7n6ciYMv"
      },
      "outputs": [],
      "source": [
        "!rm -rf /content/arc-prize-2024/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmwFTSGdgxoN"
      },
      "source": [
        "Now we download the model files defined in pytorch\n",
        "\n",
        "Feel free to inspect - there are no checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYRBFT39gJ5u",
        "outputId": "eaac9c9a-9341-4a89-9a1f-1582f7aac32d"
      },
      "outputs": [],
      "source": [
        "root_folder = \"content\"  # for colab\n",
        "\n",
        "%cd /$root_folder/\n",
        "!git clone -b clean --single-branch https://github.com/mvakde/mdlARC.git # `-b <branch_name> --single-branch` if branch\n",
        "%cd /$root_folder/mdlARC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_IZu2ptg_Q3"
      },
      "source": [
        "For safety, we delete every single file other than the 4 python source files\n",
        "\n",
        "You can inspect the source files, there's no hardcoding and there's no internet calls. No external libraries, just pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AojzIR03gduQ"
      },
      "outputs": [],
      "source": [
        "# delete everything\n",
        "!rm -rf /$root_folder/mdlARC/run-script.ipynb\n",
        "!rm -rf /$root_folder/mdlARC/sanitised-env-run-script.ipynb\n",
        "!rm -rf /$root_folder/mdlARC/dataset_building_scripts\n",
        "!rm -rf /$root_folder/mdlARC/readme.md\n",
        "!rm -rf /$root_folder/mdlARC/img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVFbol6SktPB",
        "outputId": "10d13ada-c087-4cb0-ebfb-1c4292b53080"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import argparse\n",
        "import importlib\n",
        "import sys\n",
        "\n",
        "PROJECT_ROOT = Path.cwd()\n",
        "SRC_DIR = PROJECT_ROOT / \"src\"\n",
        "if SRC_DIR.exists() and str(SRC_DIR) not in sys.path:\n",
        "    sys.path.insert(0, str(SRC_DIR))\n",
        "\n",
        "import utils, tinytransformer, train\n",
        "\n",
        "importlib.reload(utils)  # pick up code changes during iteration\n",
        "importlib.reload(tinytransformer)\n",
        "importlib.reload(train)\n",
        "\n",
        "args = {\n",
        "    # run config\n",
        "    \"num_workers\": 0,\n",
        "    \"device\": \"cuda\",  # 'cuda' | 'mps' | 'cpu'\n",
        "    \"do_validate\": False,\n",
        "    \"name\": \"arc1-cleanenv-30M-vvwide-bs32-101ep-100color-ccdb-18dec0430\",  # download file name\n",
        "    \"GPU\": \"A100-noaugreg\",  # just for logging purposes\n",
        "    # paths - must pass as Path(\"<path_to_dir>\")\n",
        "    \"train_log_file\": Path(\"runs/training_log.txt\"),\n",
        "    \"save_path\": Path(\"runs/tiny.pt\"),\n",
        "    \"checkpoint_path\": None,  # Path(\"runs/tiny.pt\"),  # or None to start from scratch\n",
        "    \"data_path\": Path(\"../challenges_dihedral_both.json\"),\n",
        "    # hyperparameters\n",
        "    \"epochs\": runconfig[1],\n",
        "    \"batch_size\": 32,\n",
        "    \"val_batch_size\": 300,\n",
        "    \"enable_color_aug_train\": True,\n",
        "    \"max_color_augments_train\": (runconfig[1] - 1),\n",
        "    \"color_aug_seed\": 42,\n",
        "    \"lr\": 3e-4,\n",
        "    \"weight_decay\": 0.01,\n",
        "    \"grad_clip\": 1.0,\n",
        "    \"dropout\": 0.1,\n",
        "    \"seed\": 42,\n",
        "    # Model Architecture\n",
        "    \"d_model\": 768,  # 128, 256, 512, 768 | 128, 384, 640\n",
        "    \"n_heads\": 12,  # 4, 8, 8/16, 12 | 4, 12, 10\n",
        "    \"d_ff\": 3072,  # 512, 1024, 2048, 3072 | 512, 1536, 2560\n",
        "    \"n_layers\": 4,  # 4, 6, 16, 16 | 24, 28, 24\n",
        "    # Visibility toggles\n",
        "    \"log_train_strings\": False,\n",
        "    \"log_train_limit\": 10,\n",
        "    \"log_inference_prompt\": False,\n",
        "}\n",
        "cfg = argparse.Namespace(**args)\n",
        "\n",
        "runs_dir = Path(\"runs\")\n",
        "runs_dir.mkdir(parents=True, exist_ok=True)\n",
        "with (runs_dir / \"config.txt\").open(\"w\") as f:\n",
        "    for k, v in args.items():\n",
        "        f.write(f\"{k}: {v}\\n\")\n",
        "\n",
        "model, dataset, dataloader, device, data_path = train.build_model_and_data(cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vaSnRxTl-oW"
      },
      "source": [
        "Train the model from scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMdms_ZWmDrh",
        "outputId": "caef1a4a-86d4-48c9-98b7-3b5b6dfe785f"
      },
      "outputs": [],
      "source": [
        "# Training only\n",
        "\n",
        "from time import perf_counter\n",
        "\n",
        "t_start = perf_counter()\n",
        "\n",
        "# ---\n",
        "# direct\n",
        "train.train_model(\n",
        "    cfg,\n",
        "    model=model,\n",
        "    dataloader=dataloader,\n",
        "    dataset=dataset,\n",
        "    device=device,\n",
        "    data_path=data_path,\n",
        ")\n",
        "\n",
        "\n",
        "# # periodic checkpointing\n",
        "# cfg.save_path = Path(f\"runs/tiny-{cfg.epochs}.pt\")\n",
        "# for i in range(3):\n",
        "#   if i != 0:\n",
        "#     cfg.checkpoint_path = cfg.save_path\n",
        "#     cfg.save_path = Path(f\"runs/tiny-{cfg.epochs*(i+1)}.pt\")\n",
        "#   train.train_model(cfg, model=model, dataloader=dataloader, dataset=dataset, device=device, data_path=data_path)\n",
        "# ---\n",
        "\n",
        "t_duration = perf_counter() - t_start\n",
        "print(f\"Training took {t_duration:.2f}s\")\n",
        "\n",
        "with open(Path(\"runs/timing.txt\"), \"w\") as f:\n",
        "    f.write(f\"Training: {t_duration:.4f} s\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WY3RPpEtmNEa",
        "outputId": "4b50cd79-8223-4097-d69a-ec0a2a87705d"
      },
      "outputs": [],
      "source": [
        "# cleaning up memory to run inference\n",
        "import gc\n",
        "import torch\n",
        "\n",
        "# 1. Delete global references to free memory\n",
        "# Deleting 'model' ensures Cell 4 reloads a fresh instance from the checkpoint,\n",
        "# preventing memory fragmentation or leftover gradients from training.\n",
        "for name in [\"model\", \"dataset\", \"dataloader\", \"optimizer\", \"scheduler\"]:\n",
        "    if name in globals():\n",
        "        del globals()[name]\n",
        "\n",
        "# 2. Reset compiled graph caches (crucial if torch.compile was used)\n",
        "if hasattr(torch, \"_dynamo\"):\n",
        "    torch._dynamo.reset()\n",
        "\n",
        "# 3. Force garbage collection and clear GPU memory\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.ipc_collect()\n",
        "\n",
        "print(f\"GPU cleaned. Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KsPC2GEQmP2C",
        "outputId": "75f31624-cb9c-47ab-a7f1-3af0adad9f7d"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import inference\n",
        "import tinytransformer\n",
        "import utils\n",
        "import sys\n",
        "import json\n",
        "import importlib\n",
        "from pathlib import Path\n",
        "from time import perf_counter\n",
        "\n",
        "# Reload modules to pick up changes\n",
        "importlib.reload(tinytransformer)\n",
        "importlib.reload(inference)\n",
        "importlib.reload(utils)\n",
        "\n",
        "# Define your paths constants\n",
        "PATH_BOTH = Path(\"../challenges_dihedral_both.json\")\n",
        "\n",
        "# Config List: (Run Name, Max Color Augments, Dataset Path)\n",
        "EVAL_CONFIGS = [(\"eval\", runconfig[1] - 1, PATH_BOTH)]\n",
        "\n",
        "# Global settings shared across runs\n",
        "EVAL_BATCH_SIZE = 1300\n",
        "SPLITS = [\"test\"]\n",
        "CHECKPOINT_PATH = Path(\"runs/tiny.pt\")\n",
        "SOLUTIONS_PRESENT = False\n",
        "EVAL_TASK_IDS = None  # Set to None to evaluate full dataset, or [\"00576224\", ...] for specific tasks\n",
        "LOG_CORRECT_GRIDS = False  # Print the actual grid, IDs, and augmentation indices for fully correct grids\n",
        "\n",
        "\n",
        "# Helper class for logging to file and console\n",
        "class TeeLogger(object):\n",
        "    def __init__(self, filepath):\n",
        "        self.terminal = sys.stdout\n",
        "        self.log = open(filepath, \"w\")\n",
        "\n",
        "    def write(self, message):\n",
        "        self.terminal.write(message)\n",
        "        self.log.write(message)\n",
        "\n",
        "    def flush(self):\n",
        "        self.terminal.flush()\n",
        "        self.log.flush()\n",
        "\n",
        "    def close(self):\n",
        "        self.log.close()\n",
        "\n",
        "\n",
        "def run_evaluation_pipeline(run_name, max_color_augments, dataset_path, device):\n",
        "    print(f\"\\n{'=' * 60}\")\n",
        "    print(f\"STARTING PIPELINE: {run_name} (Color Augs: {max_color_augments})\")\n",
        "    print(f\"{'=' * 60}\\n\")\n",
        "\n",
        "    # 1. Setup Directories\n",
        "    base_run_dir = Path(\"runs\") / run_name\n",
        "    base_run_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    eval_log_path = base_run_dir / \"eval_log.txt\"\n",
        "    aaivr_log_path = base_run_dir / \"aaivr.txt\"\n",
        "    submission_path = base_run_dir / \"submission.json\"\n",
        "\n",
        "    # 2. Update Config\n",
        "    cfg.checkpoint_path = CHECKPOINT_PATH\n",
        "    cfg.data_path = dataset_path\n",
        "    cfg.enable_color_aug_eval = max_color_augments > 0\n",
        "    cfg.max_color_augments_eval = max_color_augments\n",
        "\n",
        "    # 3. Build/Rebuild Model & Data\n",
        "    # We rebuild the dataloader every time to handle the different color augmentation settings\n",
        "    print(\"Building model and dataloader for config...\")\n",
        "\n",
        "    # Load checkpoint explicitly to pass to build function\n",
        "    checkpoint = torch.load(\n",
        "        cfg.checkpoint_path, map_location=device, weights_only=False\n",
        "    )\n",
        "\n",
        "    # Check if model exists in global scope to reuse weights, else create\n",
        "    global model\n",
        "    if \"model\" in globals() and model is not None:\n",
        "        model.load_state_dict(\n",
        "            checkpoint[\"model_state\"] if \"model_state\" in checkpoint else checkpoint,\n",
        "            strict=False,\n",
        "        )\n",
        "        model.eval()\n",
        "        # Rebuild only dataset/loader\n",
        "        _, dataset, dataloader, device, _ = train.build_model_and_data(\n",
        "            cfg, checkpoint=checkpoint\n",
        "        )\n",
        "    else:\n",
        "        model, dataset, dataloader, device, _ = train.build_model_and_data(cfg)\n",
        "\n",
        "    # 4. Run Inference (Logic from old Cell 3)\n",
        "    def log_eval(msg):\n",
        "        print(msg)\n",
        "        with open(eval_log_path, \"a\") as f:\n",
        "            f.write(msg + \"\\n\")\n",
        "\n",
        "    color_mappings_eval = None\n",
        "    color_apply_fn = None\n",
        "    if cfg.enable_color_aug_eval and cfg.max_color_augments_eval > 0:\n",
        "        color_seed = cfg.color_aug_seed or cfg.seed\n",
        "        color_mappings_eval = utils.generate_color_mapping_tensors(\n",
        "            cfg.max_color_augments_eval, color_seed\n",
        "        )\n",
        "        color_apply_fn = lambda split: True\n",
        "\n",
        "    evaluation = inference.evaluate_model_on_dataset(\n",
        "        model=model,\n",
        "        dataset=dataset,\n",
        "        device=device,\n",
        "        batch_size=EVAL_BATCH_SIZE,\n",
        "        log_prompts=args[\"log_inference_prompt\"],\n",
        "        splits=SPLITS,\n",
        "        color_mappings=color_mappings_eval,\n",
        "        color_apply_fn=color_apply_fn,\n",
        "        task_ids=EVAL_TASK_IDS,\n",
        "        include_targets=SOLUTIONS_PRESENT,\n",
        "    )\n",
        "\n",
        "    # Redirect stdout for AAIVR logging\n",
        "    if hasattr(sys.stdout, \"log\"):\n",
        "        sys.stdout = sys.stdout.terminal  # Reset if needed\n",
        "    sys.stdout = TeeLogger(str(aaivr_log_path))\n",
        "\n",
        "    try:\n",
        "        test_results = evaluation.get(\"test\", {}).get(\"results\", [])\n",
        "        dataset_has_dihedral_augments = \"dihedral_both\" in str(cfg.data_path)\n",
        "\n",
        "        aaivr_results = []\n",
        "        if test_results:\n",
        "            aaivr_results = utils.run_aaivr_on_results(\n",
        "                test_results,\n",
        "                is_dihedral_augmented=dataset_has_dihedral_augments,\n",
        "                color_aug_seed=cfg.color_aug_seed,\n",
        "                max_color_augments=cfg.max_color_augments_eval,\n",
        "            )\n",
        "\n",
        "            # # Print Stats (will go to console + aaivr.txt)\n",
        "            # utils.summarize_aaivr_pass_at_k(aaivr_results)\n",
        "            # if aaivr_results:\n",
        "            #     tasks_map = {}\n",
        "            #     for res in aaivr_results:\n",
        "            #         if res.task_id not in tasks_map:\n",
        "            #             tasks_map[res.task_id] = []\n",
        "            #         tasks_map[res.task_id].append(res)\n",
        "\n",
        "            #     arc_score = 0.0\n",
        "            #     total_tasks = len(tasks_map)\n",
        "\n",
        "            #     for t_id, pairs in tasks_map.items():\n",
        "            #         n_pairs = len(pairs)\n",
        "            #         if n_pairs > 0:\n",
        "            #             n_solved = sum(1 for p in pairs if p.pass_at_k)\n",
        "            #             arc_score += (n_solved / n_pairs)\n",
        "\n",
        "            #     max_score = total_tasks\n",
        "            #     pct = (arc_score / max_score * 100) if max_score > 0 else 0.0\n",
        "            #     print(f\"Official ARC style scoring: {arc_score:.2f}/{max_score} ({pct:.2f}%)\")\n",
        "        else:\n",
        "            print(\"No test results for AAIVR.\")\n",
        "\n",
        "    finally:\n",
        "        # Always restore stdout\n",
        "        if hasattr(sys.stdout, \"terminal\"):\n",
        "            sys.stdout.close()\n",
        "            sys.stdout = sys.stdout.terminal\n",
        "\n",
        "    # 6. Generate Submission (Logic from old Cell 5)\n",
        "    print(f\"Generating submission.json for {run_name}...\")\n",
        "    submission_data = {}\n",
        "    temp_grouping = {}\n",
        "\n",
        "    if aaivr_results:\n",
        "        for item in aaivr_results:\n",
        "            t_id = item.task_id\n",
        "            p_idx = item.original_pair_index\n",
        "            if t_id not in temp_grouping:\n",
        "                temp_grouping[t_id] = {}\n",
        "\n",
        "            top_grids = item.selected_outputs[:2]\n",
        "            if not top_grids:\n",
        "                top_grids = [[[0]]]  # Fallback\n",
        "\n",
        "            pair_dict = {\n",
        "                \"attempt_1\": top_grids[0],\n",
        "                \"attempt_2\": top_grids[1] if len(top_grids) > 1 else top_grids[0],\n",
        "            }\n",
        "            temp_grouping[t_id][p_idx] = pair_dict\n",
        "\n",
        "        for t_id, pairs_map in temp_grouping.items():\n",
        "            sorted_indices = sorted(pairs_map.keys())\n",
        "            submission_data[t_id] = [pairs_map[idx] for idx in sorted_indices]\n",
        "\n",
        "    with open(submission_path, \"w\") as f:\n",
        "        json.dump(submission_data, f)\n",
        "\n",
        "    print(f\"Finished {run_name}. Submission saved to {submission_path}\")\n",
        "\n",
        "\n",
        "# --- Execute the Loop (Modified with Timing) ---\n",
        "timing_path = Path(\"runs/timing.txt\")\n",
        "\n",
        "for name, aug_count, d_path in EVAL_CONFIGS:  # <--- Unpack 3 items\n",
        "    t_start = perf_counter()\n",
        "\n",
        "    run_evaluation_pipeline(name, aug_count, d_path, device)\n",
        "\n",
        "    t_duration = perf_counter() - t_start\n",
        "    print(f\"Run {name} took {t_duration:.2f}s\")\n",
        "\n",
        "    with open(timing_path, \"a\") as f:\n",
        "        f.write(f\"Evaluation {name}: {t_duration:.4f} s\\n\")\n",
        "\n",
        "print(\"\\nAll evaluation runs completed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "B7Z22D7LpB2K",
        "outputId": "8e079b9b-2d4c-4b29-d529-0c9bc0c12658"
      },
      "outputs": [],
      "source": [
        "# visualisation WITHOUT loading solutions\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "EVAL_SUB_FOLDER = EVAL_CONFIGS[0][0]\n",
        "submission_file = Path(f\"runs/{EVAL_SUB_FOLDER}/submission.json\")\n",
        "\n",
        "if not submission_file.exists():\n",
        "    print(f\"Error: Could not find submission file: {submission_file}\")\n",
        "else:\n",
        "    # Submission-only visualization (attempts without ground truth)\n",
        "    with open(submission_file, \"r\") as f:\n",
        "        subs = json.load(f)\n",
        "\n",
        "    print(f\"Visualizing submissions for {len(subs)} tasks (no solutions)...\")\n",
        "\n",
        "    for task_id, attempts_list in subs.items():\n",
        "        for i, attempts in enumerate(attempts_list):\n",
        "            att1 = attempts.get(\"attempt_1\")\n",
        "            att2 = attempts.get(\"attempt_2\")\n",
        "\n",
        "            grids_to_plot = []\n",
        "            if att1 is not None:\n",
        "                grids_to_plot.append(att1)\n",
        "            if att2 is not None:\n",
        "                grids_to_plot.append(att2)\n",
        "\n",
        "            if not grids_to_plot:\n",
        "                print(f\"Skipping {task_id} pair {i} (no attempts)\")\n",
        "                continue\n",
        "\n",
        "            header = f\"Task: {task_id} | Pair: {i} | Status: submission-only\"\n",
        "            print(f\"Plotting {header}\")\n",
        "\n",
        "            try:\n",
        "                utils.plot_grids(grids_to_plot, title=header)\n",
        "            except Exception as e:\n",
        "                print(f\"Skipping plot for {task_id} due to error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWY0sxcTqRqN"
      },
      "source": [
        "## Stop here for a strict no‑solutions run\n",
        "At this point, the model has already produced `runs/<run_name>/submission.json` without any access to solutions. **This means the run is clean!**.\n",
        "\n",
        "**The next 2 cells score the submission and visualise differences with ground truth. This requires downloading the solutions**\n",
        "\n",
        "If you want a strict no‑solutions audit, stop here and score the submission yourself manually"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36jsnT-TqRFU",
        "outputId": "a9471fd7-3ce6-4ee2-fda0-be4cc6f1aed5"
      },
      "outputs": [],
      "source": [
        "# Download dataset, unzip and delete everything except solutions\n",
        "%cd /content\n",
        "!kaggle competitions download -c arc-prize-2024\n",
        "\n",
        "!unzip /content/arc-prize-2024.zip -d /content/arc-prize-2024\n",
        "\n",
        "!rm /content/arc-prize-2024.zip\n",
        "!rm /content/arc-prize-2024/arc-agi_training_challenges.json\n",
        "!rm /content/arc-prize-2024/arc-agi_training_solutions.json\n",
        "!rm /content/arc-prize-2024/arc-agi_evaluation_challenges.json\n",
        "!rm /content/arc-prize-2024/arc-agi_test_challenges.json\n",
        "!rm /content/arc-prize-2024/sample_submission.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sREwkBg5pDO3",
        "outputId": "abbd8116-891e-407a-b9fd-a2fde8fdcecc"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Replace these with your actual file paths\n",
        "SOLUTIONS_FILE = Path(\"arc-prize-2024/arc-agi_evaluation_solutions.json\")\n",
        "SUBMISSION_FILE = Path(f\"mdlARC/runs/{EVAL_SUB_FOLDER}/submission.json\")\n",
        "\n",
        "\n",
        "def score(sol_path, sub_path):\n",
        "    with open(sol_path, \"r\") as f:\n",
        "        solutions = json.load(f)\n",
        "\n",
        "    with open(sub_path, \"r\") as f:\n",
        "        submissions = json.load(f)\n",
        "\n",
        "    calc_score = 0.0\n",
        "    max_total_score = len(solutions)\n",
        "    fully_solved_tasks = []\n",
        "\n",
        "    for task_id, ground_truth_grids in solutions.items():\n",
        "        # If task is missing in submission, score is 0 for that task\n",
        "        if task_id not in submissions:\n",
        "            continue\n",
        "\n",
        "        task_attempts = submissions[task_id]\n",
        "\n",
        "        # Determine number of pairs to score for this task\n",
        "        num_pairs = len(ground_truth_grids)\n",
        "        pairs_solved = 0\n",
        "\n",
        "        # Iterate through each test pair in the task\n",
        "        for i in range(min(len(task_attempts), num_pairs)):\n",
        "            truth = ground_truth_grids[i]\n",
        "            attempts = task_attempts[i]\n",
        "\n",
        "            # Check if either attempt matches the ground truth\n",
        "            # Python lists compare by value (deep equality) automatically\n",
        "            if attempts.get(\"attempt_1\") == truth or attempts.get(\"attempt_2\") == truth:\n",
        "                pairs_solved += 1\n",
        "\n",
        "        # Add fractional score (e.g., 1/2 = 0.5)\n",
        "        if num_pairs > 0:\n",
        "            calc_score += pairs_solved / num_pairs\n",
        "            if pairs_solved == num_pairs:\n",
        "                fully_solved_tasks.append(task_id)\n",
        "\n",
        "    percentage = 100 * (calc_score / max_total_score) if max_total_score > 0 else 0.0\n",
        "    print(f\"Official ARC style scoring: {calc_score}/{max_total_score} ({percentage}%)\")\n",
        "    print(f\"Fully correct tasks ({len(fully_solved_tasks)}):\")\n",
        "    for task_id in fully_solved_tasks:\n",
        "        print(task_id)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    score(SOLUTIONS_FILE, SUBMISSION_FILE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZB6jd6ASpGGT",
        "outputId": "508cad30-8ccc-4de5-cdc1-315f9a7de1c6"
      },
      "outputs": [],
      "source": [
        "# Visualise and compare the differences between the ground truth solutions and the correct answers\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "EVAL_SUB_FOLDER = EVAL_CONFIGS[0][0]\n",
        "submission_file = Path(f\"mdlARC/runs/{EVAL_SUB_FOLDER}/submission.json\")\n",
        "solutions_file = Path(\"arc-prize-2024/arc-agi_evaluation_solutions.json\")\n",
        "\n",
        "\n",
        "if not solutions_file.exists():\n",
        "    print(f\"Error: Could not find solutions file for compare mode:\\n{solutions_file}\")\n",
        "else:\n",
        "    # Load Data\n",
        "    with open(submission_file, \"r\") as f:\n",
        "        subs = json.load(f)\n",
        "    with open(solutions_file, \"r\") as f:\n",
        "        sols = json.load(f)\n",
        "\n",
        "    print(f\"Visualizing comparison for {len(subs)} tasks...\")\n",
        "\n",
        "    for task_id, attempts_list in subs.items():\n",
        "        # Get Ground Truth (list of grids)\n",
        "        if task_id not in sols:\n",
        "            print(f\"Warning: Task {task_id} not found in solutions.json\")\n",
        "            continue\n",
        "\n",
        "        gt_grids = sols[task_id]\n",
        "        print(gt_grids)\n",
        "        for i, attempts in enumerate(attempts_list):\n",
        "            if i >= len(gt_grids):\n",
        "                break\n",
        "\n",
        "            # 1. Retrieve Grids\n",
        "            gt = gt_grids[i]\n",
        "            att1 = attempts.get(\"attempt_1\")\n",
        "            att2 = attempts.get(\"attempt_2\")\n",
        "\n",
        "            # 2. Check Correctness\n",
        "            pass1 = (att1 == gt) if att1 is not None else False\n",
        "            pass2 = (att2 == gt) if att2 is not None else False\n",
        "\n",
        "            if pass1 and pass2:\n",
        "                status = \"Pass - both\"\n",
        "            elif pass1:\n",
        "                status = \"Pass - 1\"\n",
        "            elif pass2:\n",
        "                status = \"Pass - 2\"\n",
        "            else:\n",
        "                status = \"Fail\"\n",
        "\n",
        "            # 3. Visualize\n",
        "            # Construct list: [Ground Truth, Attempt 1, Attempt 2]\n",
        "            grids_to_plot = [gt]\n",
        "            if att1 is not None:\n",
        "                grids_to_plot.append(att1)\n",
        "            if att2 is not None:\n",
        "                grids_to_plot.append(att2)\n",
        "\n",
        "            header = f\"Task: {task_id} | Pair: {i} | Status: {status}\"\n",
        "            print(f\"Plotting {header}\")\n",
        "\n",
        "            # utils.plot_grids handles the matplotlib figure creation\n",
        "            try:\n",
        "                utils.plot_grids(grids_to_plot, title=header)\n",
        "            except Exception as e:\n",
        "                print(f\"Skipping plot for {task_id} due to error: {e}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
