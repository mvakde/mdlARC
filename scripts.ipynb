{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ed5944",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/mdlARC/\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "import importlib\n",
    "import utils, train\n",
    "\n",
    "importlib.reload(utils)  # pick up code changes during iteration\n",
    "importlib.reload(train)\n",
    "\n",
    "args = {\n",
    "    # run config\n",
    "    \"num_workers\": 0,\n",
    "    \"device\": \"cuda\",  # 'cuda' | 'mps' | 'cpu'\n",
    "    # paths - must pass as Path(\"<path_to_dir>\")\n",
    "    \"save_path\": Path(\"runs/tiny.pt\"),\n",
    "    \"checkpoint_path\": None,  # Path(\"runs/tiny.pt\"),  # or None to start from scratch\n",
    "    # \"data_path\": Path(\"assets/script-tests/grouped-tasks-00d62c1b/challenges.json\" ),\n",
    "    \"data_path\": Path(\"assets/script-tests/grouped-tasks/challenges.json\"),\n",
    "    # \"data_path\": Path(\"assets/ARC-1/grouped-tasks/training/challenges.json\"),\n",
    "    # \"data_path\": Path(\"assets/ARC-2/grouped-tasks/training/challenges.json\"),\n",
    "    # hyperparameters\n",
    "    \"epochs\": 5,\n",
    "    \"batch_size\": 110,\n",
    "    \"val_batch_size\": 500,\n",
    "    \"lr\": 3e-4,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"grad_clip\": 1.0,\n",
    "    \"seed\": 42,\n",
    "    # Visibility toggles\n",
    "    \"log_train_strings\": False,\n",
    "    \"log_train_limit\": 10,\n",
    "    \"log_inference_prompt\": False,\n",
    "}\n",
    "cfg = argparse.Namespace(**args)\n",
    "model, dataset, dataloader, device, data_path = train.build_model_and_data(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6ac355",
   "metadata": {
    "tags": [
     "train"
    ]
   },
   "outputs": [],
   "source": [
    "# Training only\n",
    "train.train_model(\n",
    "    cfg,\n",
    "    model=model,\n",
    "    dataloader=dataloader,\n",
    "    dataset=dataset,\n",
    "    device=device,\n",
    "    data_path=data_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a215afbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference + visualisation check\n",
    "import inference\n",
    "from utils import plot_grids, split_grids_from_tokens, tokens_to_string\n",
    "\n",
    "importlib.reload(utils)\n",
    "importlib.reload(inference)\n",
    "\n",
    "task_ids_list = [\"00d62c1b\", \"e0fb7511\", \"00576224\", \"3aa6fb7a\"]  # always pass as list\n",
    "selected_split = \"test\"\n",
    "# selected_split = \"train\"\n",
    "pair_idx = 0\n",
    "visualise = True\n",
    "\n",
    "results = inference.run_batched_inference(\n",
    "    model=model,\n",
    "    dataset=dataset,\n",
    "    task_ids=task_ids_list,\n",
    "    device=device,\n",
    "    split=selected_split,\n",
    "    pair_index=pair_idx,\n",
    "    include_targets=True,\n",
    ")\n",
    "\n",
    "if not results:\n",
    "    print(\"No inference results were produced.\")\n",
    "for res in results:\n",
    "    print(f\"\\nTask {res['task_id']} pair {res['pair_index']} ({selected_split})\")\n",
    "    print(\"Prompt tokens:\", tokens_to_string(res[\"prompt_tokens\"]))\n",
    "    print(\"Generated output tokens:\", tokens_to_string(res[\"output_tokens\"]))\n",
    "    if res.get(\"target_output_tokens\"):\n",
    "        print(\"Target output tokens:\", tokens_to_string(res[\"target_output_tokens\"]))\n",
    "    print(\"Predicted grid:\")\n",
    "    for row in res[\"output_grid\"]:\n",
    "        print(row)\n",
    "    if res.get(\"target_grid\"):\n",
    "        print(\"Target grid:\")\n",
    "        for row in res[\"target_grid\"]:\n",
    "            print(row)\n",
    "    if visualise:\n",
    "        prompt_grids = split_grids_from_tokens(res[\"prompt_tokens\"])\n",
    "        input_grid = prompt_grids[0] if prompt_grids else []\n",
    "        to_plot = [input_grid, res[\"output_grid\"]]\n",
    "        if res.get(\"target_grid\"):\n",
    "            to_plot.append(res[\"target_grid\"])\n",
    "        plot_grids(\n",
    "            to_plot,\n",
    "            title=f\"{res['task_id']} pair {res['pair_index']} ({selected_split})\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "single-inference",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full dataset evaluation\n",
    "import inference\n",
    "\n",
    "importlib.reload(inference)\n",
    "\n",
    "EVAL_BATCH_SIZE = 1300\n",
    "\n",
    "evaluation = inference.evaluate_model_on_dataset(\n",
    "    model=model,\n",
    "    dataset=dataset,\n",
    "    device=device,\n",
    "    batch_size=EVAL_BATCH_SIZE,\n",
    "    log_prompts=args[\"log_inference_prompt\"],\n",
    ")\n",
    "\n",
    "for split in (\"train\", \"test\"):\n",
    "    summary = evaluation.get(split, {}).get(\"summary\", {})\n",
    "    total = summary.get(\"total_sequences\", 0)\n",
    "    shape_ok = summary.get(\"num_shape_correct\", 0)\n",
    "    avg_pixel_acc = summary.get(\"avg_pixel_accuracy\", 0.0)\n",
    "    fully_correct = summary.get(\"num_fully_correct\", 0)\n",
    "\n",
    "    print(f\"\\nSplit: {split}\")\n",
    "    print(f\"  sequences evaluated: {total}\")\n",
    "    print(f\"  correct output grid shapes: {shape_ok} / {total}\")\n",
    "    if shape_ok > 0:\n",
    "        print(f\"  avg pixel accuracy (shape-correct only): {avg_pixel_acc:.4f}\")\n",
    "    else:\n",
    "        print(\"  avg pixel accuracy (shape-correct only): n/a\")\n",
    "    print(f\"  fully correct output grids: {fully_correct} / {total}\")\n",
    "\n",
    "    if split == \"test\":\n",
    "        correct_outputs = summary.get(\"fully_correct_results\", [])\n",
    "        print(\"  fully correct test outputs (task_id, pair_index, grid):\")\n",
    "        if not correct_outputs:\n",
    "            print(\"    (none)\")\n",
    "        for res in correct_outputs:\n",
    "            grid = res.get(\"output_grid\", [])\n",
    "            print(f\"    - {res.get('task_id')} pair {res.get('pair_index')}: {grid}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9324fb1",
   "metadata": {},
   "source": [
    "# Large scale training run (periodic checkpointing and evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd755c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ef4b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "%cd /content/mdlARC/\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "import importlib\n",
    "import utils, train\n",
    "\n",
    "importlib.reload(utils)  # pick up code changes during iteration\n",
    "importlib.reload(train)\n",
    "\n",
    "args = {\n",
    "    # run config\n",
    "    \"num_workers\": 0,\n",
    "    \"device\": \"cuda\",  # 'cuda' | 'mps' | 'cpu'\n",
    "    # paths - must pass as Path(\"<path_to_dir>\")\n",
    "    \"train_log_file\": Path(\"runs/train_log.txt\"),\n",
    "    # \"data_path\": Path(\"assets/script-tests/grouped-tasks-00d62c1b/challenges.json\" ),\n",
    "    # \"data_path\": Path(\"assets/script-tests/grouped-tasks/challenges.json\"),\n",
    "    # \"data_path\": Path(\"assets/ARC-1/grouped-tasks/training/challenges.json\"),\n",
    "    \"data_path\": Path(\"assets/ARC-2/grouped-tasks/training/challenges.json\"),\n",
    "    # hyperparameters\n",
    "    \"batch_size\": 140,\n",
    "    \"val_batch_size\": 500,\n",
    "    \"lr\": 3e-4,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"grad_clip\": 1.0,\n",
    "    \"seed\": 42,\n",
    "    # Visibility toggles\n",
    "    \"log_train_strings\": False,\n",
    "    \"log_train_limit\": 10,\n",
    "    \"log_inference_prompt\": False,\n",
    "}\n",
    "cfg = argparse.Namespace(**args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9983a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training with periodic checkpointing\n",
    "\n",
    "cfg.epochs = 500\n",
    "n_cycles = 10\n",
    "\n",
    "# Make base 0 if training from scratch. Otherwise, last checkpoint number\n",
    "cfg.base = 600\n",
    "\n",
    "for i in range(n_cycles):\n",
    "    cfg.save_path = Path(f\"runs/tiny-{(i + 1) * cfg.epochs + cfg.base}.pt\")\n",
    "\n",
    "    if cfg.base == 0 and i == 0:\n",
    "        cfg.checkpoint_path = None\n",
    "        reuse_dataset = None\n",
    "    else:\n",
    "        cfg.checkpoint_path = Path(f\"runs/tiny-{i * cfg.epochs + cfg.base}.pt\")\n",
    "        try:\n",
    "            reuse_dataset = dataset\n",
    "        except NameError:\n",
    "            reuse_dataset = None  # if `dataset` was deleted from memory\n",
    "\n",
    "    model, dataset, dataloader, device, data_path = train.build_model_and_data(\n",
    "        cfg, reuse_dataset=reuse_dataset\n",
    "    )\n",
    "    train.train_model(\n",
    "        cfg,\n",
    "        model=model,\n",
    "        dataloader=dataloader,\n",
    "        dataset=dataset,\n",
    "        device=device,\n",
    "        data_path=data_path,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd32c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clearing gpu memory before running inference\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c05dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate all the checkpoints\n",
    "import json\n",
    "import inference\n",
    "\n",
    "importlib.reload(inference)\n",
    "\n",
    "EVAL_BATCH_SIZE = 1000\n",
    "eval_log_path = Path(\"runs/eval_log.txt\")\n",
    "\n",
    "# only for debugging. keep commented out\n",
    "# cfg.base = 200\n",
    "# cfg.epochs = 400\n",
    "# n_cycles = 2\n",
    "\n",
    "try:\n",
    "    reuse_dataset = dataset\n",
    "except NameError:\n",
    "    reuse_dataset = None\n",
    "\n",
    "for i in range(n_cycles):\n",
    "    cfg.checkpoint_path = Path(f\"runs/tiny-{(i + 1) * cfg.epochs + cfg.base}.pt\")\n",
    "    model, dataset, dataloader, device, data_path = train.build_model_and_data(\n",
    "        cfg, reuse_dataset=reuse_dataset\n",
    "    )\n",
    "    evaluation = inference.evaluate_model_on_dataset(\n",
    "        model=model,\n",
    "        dataset=dataset,\n",
    "        device=device,\n",
    "        batch_size=EVAL_BATCH_SIZE,\n",
    "        log_prompts=cfg.log_inference_prompt,\n",
    "    )\n",
    "\n",
    "    formatted_eval = inference.group_eval_sequences_by_task(evaluation)\n",
    "\n",
    "    json_path = cfg.checkpoint_path.with_suffix(\".json\")\n",
    "    with open(json_path, \"w\") as f:\n",
    "        json.dump(formatted_eval, f, indent=2)\n",
    "    print(f\"Saved grouped task sequences to: {json_path}\")\n",
    "\n",
    "    with open(eval_log_path, \"a\") as f:\n",
    "        f.write(f\"--- Checkpoint: {(i + 1) * cfg.epochs + cfg.base} ---\\n\")\n",
    "        for split_name, data in evaluation.items():\n",
    "            summary = data[\"summary\"]\n",
    "            log_line = (\n",
    "                f\"Split: {split_name:<6} | \"\n",
    "                f\"Total: {summary['total_sequences']:<4} | \"\n",
    "                f\"Shape Correct: {summary['num_shape_correct']:<4} | \"\n",
    "                f\"Fully Correct: {summary['num_fully_correct']:<4} | \"\n",
    "                f\"Pixel Acc: {summary['avg_pixel_accuracy']:.4f}\"\n",
    "            )\n",
    "            print(log_line)\n",
    "            f.write(log_line + \"\\n\")\n",
    "            if split_name == \"test\":\n",
    "                correct_outputs = summary.get(\"fully_correct_results\", [])\n",
    "                log_line = \"  fully correct test outputs (task_id, pair_index, grid):\"\n",
    "                if not correct_outputs:\n",
    "                    log_line = log_line + \"\\n    (none)\"\n",
    "                for res in correct_outputs:\n",
    "                    grid = res.get(\"output_grid\", [])\n",
    "                    log_line = log_line + (\n",
    "                        f\"\\n    - {res.get('task_id')} pair {res.get('pair_index')}: {grid}\"\n",
    "                    )\n",
    "                print(log_line)\n",
    "                f.write(log_line + \"\\n\")\n",
    "\n",
    "        f.write(\"\\n\\n\")\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abec176",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%d%m%y-%H%M%S\")\n",
    "\n",
    "src_dir = \"/content/mdlARC/runs\"  # folder to back up\n",
    "zip_base = \"/content/mdlARC/runs_compressed\"  # zip will become this + '.zip'\n",
    "dst_zip = f\"/content/drive/MyDrive/run_{timestamp}.zip\"\n",
    "\n",
    "# Create zip in /content\n",
    "shutil.make_archive(zip_base, \"zip\", src_dir)\n",
    "\n",
    "# Copy zip into Drive\n",
    "shutil.copy2(zip_base + \".zip\", dst_zip)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
