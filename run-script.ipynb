{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e6aae73",
   "metadata": {},
   "source": [
    "## 29% using a 29M transformer\n",
    "Steps to reproduce:  \n",
    "1. Upload this script to google colab or modal\n",
    "2. (optional) If you want to save checkpoints and results, mount your google drive (colab) / your volume (modal)\n",
    "3. Click run-all\n",
    "\n",
    "This script ensures there's no data contamination. \n",
    "This produces a `submission.json` file. The `submission.json` file\n",
    "\n",
    "\n",
    "Notes:\n",
    "1. The config in this notebook has been tuned for an 80GB A100  \n",
    "2. Actual results were obtained by running this exact file in 2 phases.  \n",
    "    - Training on a 40GB A100\n",
    "    - Take the final checkpoint, and run the inference on an 80GB A100\n",
    "\n",
    "This will work on smaller GPUs too, but will take longer to train  \n",
    "For very constrained environments, disable the \"do_validate\" flag. This avoids checking the validation loss every epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93163d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# root_folder, mount_folder = \"root\", \"mnt/mithil-arc\" # for modal - REPLACE /mithil-arc WITH YOUR VOLUME NAME\n",
    "root_folder, mount_folder = \"content\", \"content/drive/MyDrive\"  # for colab\n",
    "\n",
    "%cd /$root_folder/\n",
    "!git clone https://github.com/mvakde/mdlARC.git # `-b <branch_name> --single-branch` if branch\n",
    "%cd /$root_folder/mdlARC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44cfa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python dataset_building_scripts/build_datasets.py --datasets arc1 conceptarc  --splits train eval --with-solutions --cleanup none\n",
    "!python dataset_building_scripts/augment_dataset_dihedral.py\n",
    "\n",
    "!rm -rf /$root_folder/mdlARC/run-script.ipynb\n",
    "!rm -rf /$root_folder/mdlARC/sanitised-env-run-script.ipynb\n",
    "!rm -rf /$root_folder/mdlARC/ultra-sanitised-env-run-script.ipynb\n",
    "!rm -rf /$root_folder/mdlARC/dataset_building_scripts\n",
    "!rm -rf /$root_folder/mdlARC/readme.md\n",
    "!rm -rf /$root_folder/mdlARC/img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475c6868",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import argparse\n",
    "import importlib\n",
    "import sys\n",
    "\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "SRC_DIR = PROJECT_ROOT / \"src\"\n",
    "if SRC_DIR.exists() and str(SRC_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC_DIR))\n",
    "\n",
    "import utils, tinytransformer, train\n",
    "\n",
    "importlib.reload(utils)  # pick up code changes during iteration\n",
    "importlib.reload(tinytransformer)\n",
    "importlib.reload(train)\n",
    "\n",
    "args = {\n",
    "    # run config\n",
    "    \"num_workers\": 0,\n",
    "    \"device\": \"cuda\",  # 'cuda' | 'mps' | 'cpu'\n",
    "    \"do_validate\": False,\n",
    "    \"name\": \"arc1-cleanenv-30M-vvwide-bs32-101ep-100color-ccdb-18dec0430\",  # download file name\n",
    "    \"GPU\": \"A100-noaugreg\",  # just for logging purposes\n",
    "    # paths - must pass as Path(\"<path_to_dir>\")\n",
    "    \"train_log_file\": Path(\"runs/training_log.txt\"),\n",
    "    \"save_path\": Path(\"runs/tiny.pt\"),\n",
    "    \"checkpoint_path\": None,  # Path(\"runs/tiny.pt\"),  # or None to start from scratch\n",
    "    \"data_path\": Path(\"assets/challenges_dihedral_both.json\"),\n",
    "    # hyperparameters\n",
    "    \"epochs\": 101,\n",
    "    \"batch_size\": 32,\n",
    "    \"val_batch_size\": 300,\n",
    "    \"enable_color_aug_train\": True,\n",
    "    \"max_color_augments_train\": 100,\n",
    "    \"color_aug_seed\": 42,\n",
    "    \"lr\": 3e-4,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"grad_clip\": 1.0,\n",
    "    \"dropout\": 0.1,\n",
    "    \"seed\": 42,\n",
    "    # Model Architecture\n",
    "    \"d_model\": 768,  # 128, 256, 512, 768 | 128, 384, 640\n",
    "    \"n_heads\": 12,  # 4, 8, 8/16, 12 | 4, 12, 10\n",
    "    \"d_ff\": 3072,  # 512, 1024, 2048, 3072 | 512, 1536, 2560\n",
    "    \"n_layers\": 4,  # 4, 6, 16, 16 | 24, 28, 24\n",
    "    # Visibility toggles\n",
    "    \"log_train_strings\": False,\n",
    "    \"log_train_limit\": 10,\n",
    "    \"log_inference_prompt\": False,\n",
    "    \"inference_temperature\": None,\n",
    "    \"inference_top_k\": None,\n",
    "}\n",
    "cfg = argparse.Namespace(**args)\n",
    "\n",
    "runs_dir = Path(\"runs\")\n",
    "runs_dir.mkdir(parents=True, exist_ok=True)\n",
    "with (runs_dir / \"config.txt\").open(\"w\") as f:\n",
    "    for k, v in args.items():\n",
    "        f.write(f\"{k}: {v}\\n\")\n",
    "\n",
    "model, dataset, dataloader, device, data_path = train.build_model_and_data(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfa7324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training only\n",
    "\n",
    "from time import perf_counter\n",
    "\n",
    "t_start = perf_counter()\n",
    "train.train_model(cfg,model=model,dataloader=dataloader,dataset=dataset,device=device,data_path=data_path)\n",
    "t_duration = perf_counter() - t_start\n",
    "\n",
    "print(f\"Training took {t_duration:.2f}s\")\n",
    "with open(Path(\"runs/timing.txt\"), \"w\") as f:\n",
    "    f.write(f\"Training: {t_duration:.4f} s\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c84a83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning up memory to run inference\n",
    "utils.cleanup_memory(globals())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d36c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data immediately in case eval fails\n",
    "archive_state = utils.save_run_archive(\n",
    "    cfg.name, root_folder, mount_folder, globals_dict=globals()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283309b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "SRC_DIR = PROJECT_ROOT / \"src\"\n",
    "if SRC_DIR.exists() and str(SRC_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC_DIR))\n",
    "\n",
    "import torch\n",
    "import evaluations\n",
    "import aaivr\n",
    "import tinytransformer\n",
    "import utils\n",
    "import json\n",
    "import importlib\n",
    "from time import perf_counter\n",
    "\n",
    "# Reload modules to pick up changes\n",
    "importlib.reload(tinytransformer)\n",
    "importlib.reload(evaluations)\n",
    "importlib.reload(aaivr)\n",
    "importlib.reload(utils)\n",
    "\n",
    "# Define your paths constants\n",
    "PATH_BOTH = Path(\"assets/challenges_dihedral_both.json\")\n",
    "\n",
    "# Config List: (Run Name, Max Color Augments, Dataset Path)\n",
    "EVAL_CONFIGS = [\n",
    "    # (\"eval_125color_both\", 125, PATH_BOTH),\n",
    "    (\"eval_100color_both\", 100, PATH_BOTH)\n",
    "    # (\"eval_10color_both\", 10, PATH_BOTH),\n",
    "    # (\"eval_0color_both\", 0, PATH_BOTH),\n",
    "    # (\"eval_0color_train\", 0, PATH_TRAIN) # <--- Uses TRAIN path (No Geom TTA on Test)\n",
    "]\n",
    "\n",
    "# Global settings shared across runs\n",
    "EVAL_BATCH_SIZE = 1300\n",
    "SPLITS = [\"test\"]\n",
    "CHECKPOINT_PATH = Path(\"runs/tiny.pt\")\n",
    "SOLUTIONS_PRESENT = False\n",
    "EVAL_TASK_IDS = None  # Set to None to evaluate full dataset, or [\"00576224\", ...] for specific tasks\n",
    "LOG_CORRECT_GRIDS = False  # Print the actual grid, IDs, and augmentation indices for fully correct grids\n",
    "\n",
    "\n",
    "# Helper class for logging to file and console\n",
    "class TeeLogger(object):\n",
    "    def __init__(self, filepath):\n",
    "        self.terminal = sys.stdout\n",
    "        self.log = open(filepath, \"w\")\n",
    "\n",
    "    def write(self, message):\n",
    "        self.terminal.write(message)\n",
    "        self.log.write(message)\n",
    "\n",
    "    def flush(self):\n",
    "        self.terminal.flush()\n",
    "        self.log.flush()\n",
    "\n",
    "    def close(self):\n",
    "        self.log.close()\n",
    "\n",
    "\n",
    "def run_evaluation_pipeline(run_name, max_color_augments, dataset_path, device):\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"STARTING PIPELINE: {run_name} (Color Augs: {max_color_augments})\")\n",
    "    print(f\"{'=' * 60}\\n\")\n",
    "\n",
    "    # 1. Setup Directories\n",
    "    base_run_dir = Path(\"runs\") / run_name\n",
    "    base_run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    eval_log_path = base_run_dir / \"eval_log.txt\"\n",
    "    aaivr_log_path = base_run_dir / \"aaivr.txt\"\n",
    "    submission_path = base_run_dir / \"submission.json\"\n",
    "\n",
    "    # 2. Update Config\n",
    "    cfg.checkpoint_path = CHECKPOINT_PATH\n",
    "    cfg.data_path = dataset_path\n",
    "    cfg.enable_color_aug_eval = max_color_augments > 0\n",
    "    cfg.max_color_augments_eval = max_color_augments\n",
    "\n",
    "    # 3. Build/Rebuild Model & Data\n",
    "    # We rebuild the dataloader every time to handle the different color augmentation settings\n",
    "    print(\"Building model and dataloader for config...\")\n",
    "\n",
    "    # Load checkpoint explicitly to pass to build function\n",
    "    checkpoint = torch.load(\n",
    "        cfg.checkpoint_path, map_location=device, weights_only=False\n",
    "    )\n",
    "\n",
    "    # Check if model exists in global scope to reuse weights, else create\n",
    "    global model\n",
    "    if \"model\" in globals() and model is not None:\n",
    "        model.load_state_dict(\n",
    "            checkpoint[\"model_state\"] if \"model_state\" in checkpoint else checkpoint,\n",
    "            strict=False,\n",
    "        )\n",
    "        model.eval()\n",
    "        # Rebuild only dataset/loader\n",
    "        _, dataset, dataloader, device, _ = train.build_model_and_data(\n",
    "            cfg, checkpoint=checkpoint\n",
    "        )\n",
    "    else:\n",
    "        model, dataset, dataloader, device, _ = train.build_model_and_data(cfg)\n",
    "\n",
    "    # 4. Run Inference (Logic from old Cell 3)\n",
    "    def log_eval(msg):\n",
    "        print(msg)\n",
    "        with open(eval_log_path, \"a\") as f:\n",
    "            f.write(msg + \"\\n\")\n",
    "\n",
    "    color_mappings_eval = None\n",
    "    color_apply_fn = None\n",
    "    if cfg.enable_color_aug_eval and cfg.max_color_augments_eval > 0:\n",
    "        color_seed = cfg.color_aug_seed or cfg.seed\n",
    "        color_mappings_eval = utils.generate_color_mapping_tensors(\n",
    "            cfg.max_color_augments_eval, color_seed\n",
    "        )\n",
    "        color_apply_fn = lambda split: True\n",
    "\n",
    "    evaluation = evaluations.evaluate_model_on_dataset(\n",
    "        model=model,\n",
    "        dataset=dataset,\n",
    "        device=device,\n",
    "        batch_size=EVAL_BATCH_SIZE,\n",
    "        log_prompts=args[\"log_inference_prompt\"],\n",
    "        temperature=args[\"inference_temperature\"],\n",
    "        top_k=args[\"inference_top_k\"],\n",
    "        splits=SPLITS,\n",
    "        color_mappings=color_mappings_eval,\n",
    "        color_apply_fn=color_apply_fn,\n",
    "        task_ids=EVAL_TASK_IDS,\n",
    "        include_targets=SOLUTIONS_PRESENT,\n",
    "    )\n",
    "\n",
    "    # Log Inference Stats\n",
    "    log_eval(f\"\\n-- {cfg.epochs}ep {max_color_augments}color --\\n\")\n",
    "    for split in SPLITS:\n",
    "        summary = evaluation.get(split, {}).get(\"summary\", {})\n",
    "        total = summary.get(\"total_sequences\", 0)\n",
    "        shape_ok = summary.get(\"num_shape_correct\", 0)\n",
    "        fully_correct = summary.get(\"num_fully_correct\", 0)\n",
    "        avg_pixel_acc = summary.get(\"avg_pixel_accuracy\", 0.0)\n",
    "\n",
    "        log_eval(\n",
    "            f\"Split: {split} | Seq: {total} | Shape OK: {shape_ok} | Fully Correct: {fully_correct} | Pixel Acc: {avg_pixel_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "        if LOG_CORRECT_GRIDS and fully_correct > 0:\n",
    "            log_eval(f\"  [Correct Grids Details for {split}]\")\n",
    "\n",
    "            # Determine if THIS split has dihedral augmentations\n",
    "            # Train is augmented if \"dihedral\" is anywhere in the name\n",
    "            # Test is augmented ONLY if \"dihedral_both\" is in the name\n",
    "            is_dihedral_split = (split == \"train\" and \"dihedral\" in data_path_str) or (\n",
    "                split == \"test\" and \"dihedral_both\" in data_path_str\n",
    "            )\n",
    "\n",
    "            correct_results = summary.get(\"fully_correct_results\", [])\n",
    "            for res in correct_results:\n",
    "                raw_idx = res.get(\"pair_index\", 0)\n",
    "\n",
    "                # Decode indices based on split properties\n",
    "                if is_dihedral_split:\n",
    "                    pair_id = raw_idx // 8\n",
    "                    dihedral_id = raw_idx % 8\n",
    "                else:\n",
    "                    pair_id = raw_idx\n",
    "                    dihedral_id = 0\n",
    "\n",
    "                color_id = res.get(\"color_permutation_index\", 0)\n",
    "                grid = res.get(\"output_grid\", [])\n",
    "\n",
    "                log_eval(\n",
    "                    f\"    T:{res.get('task_id')} | Pair:{pair_id} | Dihedral:{dihedral_id} | Color:{color_id} -> {grid}\"\n",
    "                )\n",
    "\n",
    "    # 5. Run AAIVR (Logic from old Cell 4)\n",
    "    print(f\"Running AAIVR for {run_name}...\")\n",
    "\n",
    "    # Redirect stdout for AAIVR logging\n",
    "    if hasattr(sys.stdout, \"log\"):\n",
    "        sys.stdout = sys.stdout.terminal  # Reset if needed\n",
    "    sys.stdout = TeeLogger(str(aaivr_log_path))\n",
    "\n",
    "    try:\n",
    "        test_results = evaluation.get(\"test\", {}).get(\"results\", [])\n",
    "        dataset_has_dihedral_augments = \"dihedral_both\" in str(cfg.data_path)\n",
    "\n",
    "        aaivr_results = []\n",
    "        if test_results:\n",
    "            aaivr_results = aaivr.run_aaivr_on_results(\n",
    "                test_results,\n",
    "                is_dihedral_augmented=dataset_has_dihedral_augments,\n",
    "                color_aug_seed=cfg.color_aug_seed,\n",
    "                max_color_augments=cfg.max_color_augments_eval,\n",
    "            )\n",
    "\n",
    "            # Print Stats (will go to console + aaivr.txt)\n",
    "            aaivr.summarize_aaivr_pass_at_k(aaivr_results)\n",
    "            if aaivr_results:\n",
    "                tasks_map = {}\n",
    "                for res in aaivr_results:\n",
    "                    if res.task_id not in tasks_map:\n",
    "                        tasks_map[res.task_id] = []\n",
    "                    tasks_map[res.task_id].append(res)\n",
    "\n",
    "                arc_score = 0.0\n",
    "                total_tasks = len(tasks_map)\n",
    "\n",
    "                for t_id, pairs in tasks_map.items():\n",
    "                    n_pairs = len(pairs)\n",
    "                    if n_pairs > 0:\n",
    "                        n_solved = sum(1 for p in pairs if p.pass_at_k)\n",
    "                        arc_score += n_solved / n_pairs\n",
    "\n",
    "                max_score = total_tasks\n",
    "                pct = (arc_score / max_score * 100) if max_score > 0 else 0.0\n",
    "                print(\n",
    "                    f\"Official ARC style scoring: {arc_score:.2f}/{max_score} ({pct:.2f}%)\"\n",
    "                )\n",
    "        else:\n",
    "            print(\"No test results for AAIVR.\")\n",
    "\n",
    "    finally:\n",
    "        # Always restore stdout\n",
    "        if hasattr(sys.stdout, \"terminal\"):\n",
    "            sys.stdout.close()\n",
    "            sys.stdout = sys.stdout.terminal\n",
    "\n",
    "    # 6. Generate Submission (Logic from old Cell 5)\n",
    "    print(f\"Generating submission.json for {run_name}...\")\n",
    "    submission_data = {}\n",
    "    temp_grouping = {}\n",
    "\n",
    "    if aaivr_results:\n",
    "        for item in aaivr_results:\n",
    "            t_id = item.task_id\n",
    "            p_idx = item.original_pair_index\n",
    "            if t_id not in temp_grouping:\n",
    "                temp_grouping[t_id] = {}\n",
    "\n",
    "            top_grids = item.selected_outputs[:2]\n",
    "            if not top_grids:\n",
    "                top_grids = [[[0]]]  # Fallback\n",
    "\n",
    "            pair_dict = {\n",
    "                \"attempt_1\": top_grids[0],\n",
    "                \"attempt_2\": top_grids[1] if len(top_grids) > 1 else top_grids[0],\n",
    "            }\n",
    "            temp_grouping[t_id][p_idx] = pair_dict\n",
    "\n",
    "        for t_id, pairs_map in temp_grouping.items():\n",
    "            sorted_indices = sorted(pairs_map.keys())\n",
    "            submission_data[t_id] = [pairs_map[idx] for idx in sorted_indices]\n",
    "\n",
    "    with open(submission_path, \"w\") as f:\n",
    "        json.dump(submission_data, f)\n",
    "\n",
    "    print(f\"Finished {run_name}. Submission saved to {submission_path}\")\n",
    "\n",
    "\n",
    "# --- Execute the Loop (Modified with Timing) ---\n",
    "timing_path = Path(\"runs/timing.txt\")\n",
    "\n",
    "for name, aug_count, d_path in EVAL_CONFIGS:  # <--- Unpack 3 items\n",
    "    t_start = perf_counter()\n",
    "\n",
    "    run_evaluation_pipeline(name, aug_count, d_path, device)\n",
    "\n",
    "    t_duration = perf_counter() - t_start\n",
    "    print(f\"Run {name} took {t_duration:.2f}s\")\n",
    "\n",
    "    with open(timing_path, \"a\") as f:\n",
    "        f.write(f\"Evaluation {name}: {t_duration:.4f} s\\n\")\n",
    "\n",
    "print(\"\\nAll evaluation runs completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb77a5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# refresh Drive zip\n",
    "archive_state = utils.update_run_archive(\n",
    "    cfg.name, root_folder, mount_folder, globals_dict=globals()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4ce6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation\n",
    "EVAL_SUB_FOLDER = \"eval_100color_both\"\n",
    "VIS_MODE = \"!\"  # \"!\" = compare vs solutions, \"submission\" = attempts-only\n",
    "utils.visualize_eval_submissions(\n",
    "    EVAL_SUB_FOLDER, mode=VIS_MODE, solutions_file=\"assets/solutions.json\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}