{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "3e6aae73",
            "metadata": {},
            "source": [
                "## Breaking Pareto Frontier\n",
                "Steps to reproduce:  \n",
                "1. Upload this script to google colab or modal\n",
                "2. (optional) If you want to save checkpoints and results, mount your google drive (colab) / your volume (modal)\n",
                "3. Click run-all\n",
                "\n",
                "\n",
                "Notes:\n",
                "1. The config in this notebook has been tuned for an 80GB A100  \n",
                "2. Actual results were obtained by running this exact file in 2 phases.  \n",
                "    - Training on a 40GB A100\n",
                "    - Take the final checkpoint, and run the inference on an 80GB A100\n",
                "\n",
                "This will work on smaller GPUs too, but will take longer to train  \n",
                "For very constrained environments, disable the \"do_validate\" flag. This avoids checking the validation loss every epoch"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "93163d18",
            "metadata": {},
            "outputs": [],
            "source": [
                "# root_folder, mount_folder = \"root\", \"mnt/mithil-arc\" # for modal\n",
                "root_folder, mount_folder = \"content\", \"content/drive/MyDrive\"  # for colab\n",
                "\n",
                "%cd /$root_folder/\n",
                "!git clone https://github.com/mvakde/mdlARC.git # `-b <branch_name> --single-branch` if branch\n",
                "%cd /$root_folder/mdlARC"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "475c6868",
            "metadata": {},
            "outputs": [],
            "source": [
                "from pathlib import Path\n",
                "import argparse\n",
                "import importlib\n",
                "import utils, tinytransformer, train\n",
                "\n",
                "importlib.reload(utils)  # pick up code changes during iteration\n",
                "importlib.reload(tinytransformer)\n",
                "importlib.reload(train)\n",
                "\n",
                "args = {\n",
                "    # run config\n",
                "    \"num_workers\": 0,\n",
                "    \"device\": \"cuda\",  # 'cuda' | 'mps' | 'cpu'\n",
                "    \"do_validate\": True,\n",
                "    \"name\": \"arc1-30M-vvwide-bs32-101ep-100color-ccdb-18dec0430\",  # download file name\n",
                "    \"GPU\": \"A100-noaugreg\",\n",
                "    # paths - must pass as Path(\"<path_to_dir>\")\n",
                "    \"train_log_file\": Path(\"runs/training_log.txt\"),\n",
                "    \"save_path\": Path(\"runs/tiny.pt\"),\n",
                "    \"checkpoint_path\": None,  # Path(\"runs/tiny.pt\"),  # or None to start from scratch\n",
                "    # \"data_path\": Path(\"assets/script-tests/grouped-tasks-00576224/challenges.json\"),\n",
                "    # \"data_path\": Path(\"assets/ARC-2/grouped-tasks/concept_plus_combined_dihedral_train/challenges.json\"),  # this dataset has dihedral augments only on the train sequences (use this for training)\n",
                "    \"data_path\": Path(\n",
                "        \"assets/ARC-1/grouped-tasks/concept_plus_combined_dihedral_both/challenges.json\"\n",
                "    ),  # this has dihedral augments on train and test sequences (only use for evaluation)\n",
                "    # hyperparameters\n",
                "    \"epochs\": 101,\n",
                "    \"batch_size\": 32,\n",
                "    \"val_batch_size\": 300,\n",
                "    \"enable_color_aug_train\": True,\n",
                "    \"max_color_augments_train\": 100,\n",
                "    \"color_aug_seed\": 42,\n",
                "    \"lr\": 3e-4,\n",
                "    \"weight_decay\": 0.01,\n",
                "    \"grad_clip\": 1.0,\n",
                "    \"dropout\": 0.1,\n",
                "    \"seed\": 42,\n",
                "    # Model Architecture\n",
                "    \"d_model\": 768,  # 128, 256, 512, 768 | 128, 384, 640\n",
                "    \"n_heads\": 12,  # 4, 8, 8/16, 12 | 4, 12, 10\n",
                "    \"d_ff\": 3072,  # 512, 1024, 2048, 3072 | 512, 1536, 2560\n",
                "    \"n_layers\": 4,  # 4, 6, 16, 16 | 24, 28, 24\n",
                "    # Loss masking\n",
                "    \"mask_input_loss\": False,  # If True, only compute loss on output tokens (mask input loss)\n",
                "    # Visibility toggles\n",
                "    \"log_train_strings\": False,\n",
                "    \"log_train_limit\": 10,\n",
                "    \"log_inference_prompt\": False,\n",
                "}\n",
                "cfg = argparse.Namespace(**args)\n",
                "\n",
                "runs_dir = Path(\"runs\")\n",
                "runs_dir.mkdir(parents=True, exist_ok=True)\n",
                "with (runs_dir / \"config.txt\").open(\"w\") as f:\n",
                "    for k, v in args.items():\n",
                "        f.write(f\"{k}: {v}\\n\")\n",
                "\n",
                "model, dataset, dataloader, device, data_path = train.build_model_and_data(cfg)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8cfa7324",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Training only\n",
                "\n",
                "from time import perf_counter\n",
                "\n",
                "t_start = perf_counter()\n",
                "\n",
                "# ---\n",
                "# direct\n",
                "train.train_model(\n",
                "    cfg,\n",
                "    model=model,\n",
                "    dataloader=dataloader,\n",
                "    dataset=dataset,\n",
                "    device=device,\n",
                "    data_path=data_path,\n",
                ")\n",
                "\n",
                "\n",
                "# # periodic checkpointing\n",
                "# cfg.save_path = Path(f\"runs/tiny-{cfg.epochs}.pt\")\n",
                "# for i in range(3):\n",
                "#   if i != 0:\n",
                "#     cfg.checkpoint_path = cfg.save_path\n",
                "#     cfg.save_path = Path(f\"runs/tiny-{cfg.epochs*(i+1)}.pt\")\n",
                "#   train.train_model(cfg, model=model, dataloader=dataloader, dataset=dataset, device=device, data_path=data_path)\n",
                "# ---\n",
                "\n",
                "t_duration = perf_counter() - t_start\n",
                "print(f\"Training took {t_duration:.2f}s\")\n",
                "\n",
                "with open(Path(\"runs/timing.txt\"), \"w\") as f:\n",
                "    f.write(f\"Training: {t_duration:.4f} s\\n\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4c84a83a",
            "metadata": {},
            "outputs": [],
            "source": [
                "# cleaning up memory to run inference\n",
                "import gc\n",
                "import torch\n",
                "\n",
                "# 1. Delete global references to free memory\n",
                "# Deleting 'model' ensures Cell 4 reloads a fresh instance from the checkpoint,\n",
                "# preventing memory fragmentation or leftover gradients from training.\n",
                "for name in [\"model\", \"dataset\", \"dataloader\", \"optimizer\", \"scheduler\"]:\n",
                "    if name in globals():\n",
                "        del globals()[name]\n",
                "\n",
                "# 2. Reset compiled graph caches (crucial if torch.compile was used)\n",
                "if hasattr(torch, \"_dynamo\"):\n",
                "    torch._dynamo.reset()\n",
                "\n",
                "# 3. Force garbage collection and clear GPU memory\n",
                "gc.collect()\n",
                "torch.cuda.empty_cache()\n",
                "torch.cuda.ipc_collect()\n",
                "\n",
                "print(f\"GPU cleaned. Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d0d36c78",
            "metadata": {},
            "outputs": [],
            "source": [
                "# save data immediately in case eval fails\n",
                "from datetime import datetime\n",
                "from pathlib import Path\n",
                "import shutil\n",
                "\n",
                "# Reusable paths (keep these for cell 2)\n",
                "SRC_DIR = Path(f\"/{root_folder}/mdlARC/runs\")\n",
                "ZIP_BASE = Path(f\"/{root_folder}/mdlARC\") / f\"runs-{cfg.name}\"  # no .zip here\n",
                "LOCAL_ZIP = ZIP_BASE.with_suffix(\".zip\")\n",
                "MOUNT_DIR = Path(f\"/{mount_folder}\")\n",
                "\n",
                "timestamp = datetime.now().strftime(\"%d%m%y-%H%M%S\")\n",
                "LAST_DRIVE_ZIP = MOUNT_DIR / f\"runs-{cfg.name}-{timestamp}.zip\"\n",
                "\n",
                "# Clean local zip if it exists\n",
                "LOCAL_ZIP.unlink(missing_ok=True)\n",
                "\n",
                "print(f\"Zipping {SRC_DIR} ...\")\n",
                "shutil.make_archive(str(ZIP_BASE), \"zip\", str(SRC_DIR))\n",
                "\n",
                "print(f\"Copying to Drive: {LAST_DRIVE_ZIP}\")\n",
                "shutil.copy2(str(LOCAL_ZIP), str(LAST_DRIVE_ZIP))\n",
                "\n",
                "# Optional: delete local zip to save space\n",
                "LOCAL_ZIP.unlink(missing_ok=True)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "283309b0",
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import inference\n",
                "import tinytransformer\n",
                "import utils\n",
                "import sys\n",
                "import json\n",
                "import importlib\n",
                "from pathlib import Path\n",
                "from time import perf_counter\n",
                "\n",
                "# Reload modules to pick up changes\n",
                "importlib.reload(tinytransformer)\n",
                "importlib.reload(inference)\n",
                "importlib.reload(utils)\n",
                "\n",
                "# Define your paths constants\n",
                "PATH_BOTH = Path(\n",
                "    \"assets/ARC-1/grouped-tasks/concept_plus_combined_dihedral_both/challenges.json\"\n",
                ")\n",
                "PATH_TRAIN = Path(\n",
                "    \"assets/ARC-1/grouped-tasks/concept_plus_combined_dihedral_train/challenges.json\"\n",
                ")\n",
                "\n",
                "# Config List: (Run Name, Max Color Augments, Dataset Path)\n",
                "EVAL_CONFIGS = [\n",
                "    # (\"eval_125color_both\", 125, PATH_BOTH),\n",
                "    (\"eval_100color_both\", 100, PATH_BOTH)\n",
                "    # (\"eval_10color_both\", 10, PATH_BOTH),\n",
                "    # (\"eval_0color_both\", 0, PATH_BOTH),\n",
                "    # (\"eval_0color_train\", 0, PATH_TRAIN) # <--- Uses TRAIN path (No Geom TTA on Test)\n",
                "]\n",
                "\n",
                "# Global settings shared across runs\n",
                "EVAL_BATCH_SIZE = 1300\n",
                "SPLITS = [\"test\"]\n",
                "CHECKPOINT_PATH = Path(\"runs/tiny.pt\")\n",
                "SOLUTIONS_PRESENT = True\n",
                "EVAL_TASK_IDS = None  # Set to None to evaluate full dataset, or [\"00576224\", ...] for specific tasks\n",
                "LOG_CORRECT_GRIDS = False  # Print the actual grid, IDs, and augmentation indices for fully correct grids\n",
                "\n",
                "\n",
                "# Helper class for logging to file and console\n",
                "class TeeLogger(object):\n",
                "    def __init__(self, filepath):\n",
                "        self.terminal = sys.stdout\n",
                "        self.log = open(filepath, \"w\")\n",
                "\n",
                "    def write(self, message):\n",
                "        self.terminal.write(message)\n",
                "        self.log.write(message)\n",
                "\n",
                "    def flush(self):\n",
                "        self.terminal.flush()\n",
                "        self.log.flush()\n",
                "\n",
                "    def close(self):\n",
                "        self.log.close()\n",
                "\n",
                "\n",
                "def run_evaluation_pipeline(run_name, max_color_augments, dataset_path, device):\n",
                "    print(f\"\\n{'=' * 60}\")\n",
                "    print(f\"STARTING PIPELINE: {run_name} (Color Augs: {max_color_augments})\")\n",
                "    print(f\"{'=' * 60}\\n\")\n",
                "\n",
                "    # 1. Setup Directories\n",
                "    base_run_dir = Path(\"runs\") / run_name\n",
                "    base_run_dir.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "    eval_log_path = base_run_dir / \"eval_log.txt\"\n",
                "    aaivr_log_path = base_run_dir / \"aaivr.txt\"\n",
                "    submission_path = base_run_dir / \"submission.json\"\n",
                "\n",
                "    # 2. Update Config\n",
                "    cfg.checkpoint_path = CHECKPOINT_PATH\n",
                "    cfg.data_path = dataset_path\n",
                "    cfg.enable_color_aug_eval = max_color_augments > 0\n",
                "    cfg.max_color_augments_eval = max_color_augments\n",
                "\n",
                "    # 3. Build/Rebuild Model & Data\n",
                "    # We rebuild the dataloader every time to handle the different color augmentation settings\n",
                "    print(\"Building model and dataloader for config...\")\n",
                "\n",
                "    # Load checkpoint explicitly to pass to build function\n",
                "    checkpoint = torch.load(\n",
                "        cfg.checkpoint_path, map_location=device, weights_only=False\n",
                "    )\n",
                "\n",
                "    # Check if model exists in global scope to reuse weights, else create\n",
                "    global model\n",
                "    if \"model\" in globals() and model is not None:\n",
                "        model.load_state_dict(\n",
                "            checkpoint[\"model_state\"] if \"model_state\" in checkpoint else checkpoint,\n",
                "            strict=False,\n",
                "        )\n",
                "        model.eval()\n",
                "        # Rebuild only dataset/loader\n",
                "        _, dataset, dataloader, device, _ = train.build_model_and_data(\n",
                "            cfg, checkpoint=checkpoint\n",
                "        )\n",
                "    else:\n",
                "        model, dataset, dataloader, device, _ = train.build_model_and_data(cfg)\n",
                "\n",
                "    # 4. Run Inference (Logic from old Cell 3)\n",
                "    def log_eval(msg):\n",
                "        print(msg)\n",
                "        with open(eval_log_path, \"a\") as f:\n",
                "            f.write(msg + \"\\n\")\n",
                "\n",
                "    color_mappings_eval = None\n",
                "    color_apply_fn = None\n",
                "    if cfg.enable_color_aug_eval and cfg.max_color_augments_eval > 0:\n",
                "        color_seed = cfg.color_aug_seed or cfg.seed\n",
                "        color_mappings_eval = utils.generate_color_mapping_tensors(\n",
                "            cfg.max_color_augments_eval, color_seed\n",
                "        )\n",
                "        color_apply_fn = lambda split: True\n",
                "\n",
                "    evaluation = inference.evaluate_model_on_dataset(\n",
                "        model=model,\n",
                "        dataset=dataset,\n",
                "        device=device,\n",
                "        batch_size=EVAL_BATCH_SIZE,\n",
                "        log_prompts=args[\"log_inference_prompt\"],\n",
                "        splits=SPLITS,\n",
                "        color_mappings=color_mappings_eval,\n",
                "        color_apply_fn=color_apply_fn,\n",
                "        task_ids=EVAL_TASK_IDS,\n",
                "        include_targets=SOLUTIONS_PRESENT,\n",
                "    )\n",
                "\n",
                "    # Log Inference Stats\n",
                "    log_eval(f\"\\n-- {cfg.epochs}ep {max_color_augments}color --\\n\")\n",
                "    for split in SPLITS:\n",
                "        summary = evaluation.get(split, {}).get(\"summary\", {})\n",
                "        total = summary.get(\"total_sequences\", 0)\n",
                "        shape_ok = summary.get(\"num_shape_correct\", 0)\n",
                "        fully_correct = summary.get(\"num_fully_correct\", 0)\n",
                "        avg_pixel_acc = summary.get(\"avg_pixel_accuracy\", 0.0)\n",
                "\n",
                "        log_eval(\n",
                "            f\"Split: {split} | Seq: {total} | Shape OK: {shape_ok} | Fully Correct: {fully_correct} | Pixel Acc: {avg_pixel_acc:.4f}\"\n",
                "        )\n",
                "\n",
                "        if LOG_CORRECT_GRIDS and fully_correct > 0:\n",
                "            log_eval(f\"  [Correct Grids Details for {split}]\")\n",
                "\n",
                "            # Determine if THIS split has dihedral augmentations\n",
                "            # Train is augmented if \"dihedral\" is anywhere in the name\n",
                "            # Test is augmented ONLY if \"dihedral_both\" is in the name\n",
                "            is_dihedral_split = (split == \"train\" and \"dihedral\" in data_path_str) or (\n",
                "                split == \"test\" and \"dihedral_both\" in data_path_str\n",
                "            )\n",
                "\n",
                "            correct_results = summary.get(\"fully_correct_results\", [])\n",
                "            for res in correct_results:\n",
                "                raw_idx = res.get(\"pair_index\", 0)\n",
                "\n",
                "                # Decode indices based on split properties\n",
                "                if is_dihedral_split:\n",
                "                    pair_id = raw_idx // 8\n",
                "                    dihedral_id = raw_idx % 8\n",
                "                else:\n",
                "                    pair_id = raw_idx\n",
                "                    dihedral_id = 0\n",
                "\n",
                "                color_id = res.get(\"color_permutation_index\", 0)\n",
                "                grid = res.get(\"output_grid\", [])\n",
                "\n",
                "                log_eval(\n",
                "                    f\"    T:{res.get('task_id')} | Pair:{pair_id} | Dihedral:{dihedral_id} | Color:{color_id} -> {grid}\"\n",
                "                )\n",
                "\n",
                "    # 5. Run AAIVR (Logic from old Cell 4)\n",
                "    print(f\"Running AAIVR for {run_name}...\")\n",
                "\n",
                "    # Redirect stdout for AAIVR logging\n",
                "    if hasattr(sys.stdout, \"log\"):\n",
                "        sys.stdout = sys.stdout.terminal  # Reset if needed\n",
                "    sys.stdout = TeeLogger(str(aaivr_log_path))\n",
                "\n",
                "    try:\n",
                "        test_results = evaluation.get(\"test\", {}).get(\"results\", [])\n",
                "        dataset_has_dihedral_augments = \"dihedral_both\" in str(cfg.data_path)\n",
                "\n",
                "        aaivr_results = []\n",
                "        if test_results:\n",
                "            aaivr_results = utils.run_aaivr_on_results(\n",
                "                test_results,\n",
                "                is_dihedral_augmented=dataset_has_dihedral_augments,\n",
                "                color_aug_seed=cfg.color_aug_seed,\n",
                "                max_color_augments=cfg.max_color_augments_eval,\n",
                "            )\n",
                "\n",
                "            # Print Stats (will go to console + aaivr.txt)\n",
                "            utils.summarize_aaivr_pass_at_k(aaivr_results)\n",
                "            if aaivr_results:\n",
                "                tasks_map = {}\n",
                "                for res in aaivr_results:\n",
                "                    if res.task_id not in tasks_map:\n",
                "                        tasks_map[res.task_id] = []\n",
                "                    tasks_map[res.task_id].append(res)\n",
                "\n",
                "                arc_score = 0.0\n",
                "                total_tasks = len(tasks_map)\n",
                "\n",
                "                for t_id, pairs in tasks_map.items():\n",
                "                    n_pairs = len(pairs)\n",
                "                    if n_pairs > 0:\n",
                "                        n_solved = sum(1 for p in pairs if p.pass_at_k)\n",
                "                        arc_score += n_solved / n_pairs\n",
                "\n",
                "                max_score = total_tasks\n",
                "                pct = (arc_score / max_score * 100) if max_score > 0 else 0.0\n",
                "                print(\n",
                "                    f\"Official ARC style scoring: {arc_score:.2f}/{max_score} ({pct:.2f}%)\"\n",
                "                )\n",
                "        else:\n",
                "            print(\"No test results for AAIVR.\")\n",
                "\n",
                "    finally:\n",
                "        # Always restore stdout\n",
                "        if hasattr(sys.stdout, \"terminal\"):\n",
                "            sys.stdout.close()\n",
                "            sys.stdout = sys.stdout.terminal\n",
                "\n",
                "    # 6. Generate Submission (Logic from old Cell 5)\n",
                "    print(f\"Generating submission.json for {run_name}...\")\n",
                "    submission_data = {}\n",
                "    temp_grouping = {}\n",
                "\n",
                "    if aaivr_results:\n",
                "        for item in aaivr_results:\n",
                "            t_id = item.task_id\n",
                "            p_idx = item.original_pair_index\n",
                "            if t_id not in temp_grouping:\n",
                "                temp_grouping[t_id] = {}\n",
                "\n",
                "            top_grids = item.selected_outputs[:2]\n",
                "            if not top_grids:\n",
                "                top_grids = [[[0]]]  # Fallback\n",
                "\n",
                "            pair_dict = {\n",
                "                \"attempt_1\": top_grids[0],\n",
                "                \"attempt_2\": top_grids[1] if len(top_grids) > 1 else top_grids[0],\n",
                "            }\n",
                "            temp_grouping[t_id][p_idx] = pair_dict\n",
                "\n",
                "        for t_id, pairs_map in temp_grouping.items():\n",
                "            sorted_indices = sorted(pairs_map.keys())\n",
                "            submission_data[t_id] = [pairs_map[idx] for idx in sorted_indices]\n",
                "\n",
                "    with open(submission_path, \"w\") as f:\n",
                "        json.dump(submission_data, f)\n",
                "\n",
                "    print(f\"Finished {run_name}. Submission saved to {submission_path}\")\n",
                "\n",
                "\n",
                "# --- Execute the Loop (Modified with Timing) ---\n",
                "timing_path = Path(\"runs/timing.txt\")\n",
                "\n",
                "for name, aug_count, d_path in EVAL_CONFIGS:  # <--- Unpack 3 items\n",
                "    t_start = perf_counter()\n",
                "\n",
                "    run_evaluation_pipeline(name, aug_count, d_path, device)\n",
                "\n",
                "    t_duration = perf_counter() - t_start\n",
                "    print(f\"Run {name} took {t_duration:.2f}s\")\n",
                "\n",
                "    with open(timing_path, \"a\") as f:\n",
                "        f.write(f\"Evaluation {name}: {t_duration:.4f} s\\n\")\n",
                "\n",
                "print(\"\\nAll evaluation runs completed.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "fb77a5a1",
            "metadata": {},
            "outputs": [],
            "source": [
                "from datetime import datetime\n",
                "from pathlib import Path\n",
                "import shutil\n",
                "\n",
                "# If kernel restarted and LAST_DRIVE_ZIP isn't defined, fall back to \"latest matching zip\"\n",
                "if \"LAST_DRIVE_ZIP\" not in globals():\n",
                "    pattern = f\"runs-{cfg.name}-*.zip\"\n",
                "    matches = sorted(\n",
                "        MOUNT_DIR.glob(pattern), key=lambda p: p.stat().st_mtime, reverse=True\n",
                "    )\n",
                "    LAST_DRIVE_ZIP = matches[0] if matches else None\n",
                "\n",
                "# Delete the previous zip on Drive (if any)\n",
                "if LAST_DRIVE_ZIP and Path(LAST_DRIVE_ZIP).exists():\n",
                "    print(f\"Deleting old Drive zip: {LAST_DRIVE_ZIP}\")\n",
                "    Path(LAST_DRIVE_ZIP).unlink()\n",
                "\n",
                "# Create a new timestamped zip\n",
                "timestamp = datetime.now().strftime(\"%d%m%y-%H%M%S\")\n",
                "NEW_DRIVE_ZIP = MOUNT_DIR / f\"runs-{cfg.name}-{timestamp}.zip\"\n",
                "\n",
                "LOCAL_ZIP.unlink(missing_ok=True)\n",
                "\n",
                "print(f\"Zipping UPDATED {SRC_DIR} ...\")\n",
                "shutil.make_archive(str(ZIP_BASE), \"zip\", str(SRC_DIR))\n",
                "\n",
                "print(f\"Copying NEW zip to Drive: {NEW_DRIVE_ZIP}\")\n",
                "shutil.copy2(str(LOCAL_ZIP), str(NEW_DRIVE_ZIP))\n",
                "\n",
                "# Update pointer for the next run\n",
                "LAST_DRIVE_ZIP = NEW_DRIVE_ZIP\n",
                "\n",
                "# Optional: delete local zip\n",
                "LOCAL_ZIP.unlink(missing_ok=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2a4ce6cd",
            "metadata": {},
            "outputs": [],
            "source": [
                "# visualisation\n",
                "import json\n",
                "import matplotlib.pyplot as plt\n",
                "from pathlib import Path\n",
                "\n",
                "EVAL_SUB_FOLDER = \"eval_100color_both\"  # \"eval_Ncolor\" -> replace N\n",
                "\n",
                "submission_file = Path(f\"runs/{EVAL_SUB_FOLDER}/submission.json\")\n",
                "# solutions_file = Path(\"assets/ARC-1/grouped-tasks/evaluation/solutions.json\")\n",
                "solutions_file = Path(\"assets/ARC-1/grouped-tasks/evaluation/solutions.json\")\n",
                "\n",
                "if not submission_file.exists() or not solutions_file.exists():\n",
                "    print(\n",
                "        f\"Error: Could not find one of the files:\\n{submission_file}\\n{solutions_file}\"\n",
                "    )\n",
                "else:\n",
                "    # Load Data\n",
                "    with open(submission_file, \"r\") as f:\n",
                "        subs = json.load(f)\n",
                "    with open(solutions_file, \"r\") as f:\n",
                "        sols = json.load(f)\n",
                "\n",
                "    print(f\"Visualizing comparison for {len(subs)} tasks...\")\n",
                "\n",
                "    for task_id, attempts_list in subs.items():\n",
                "        # Get Ground Truth (list of grids)\n",
                "        if task_id not in sols:\n",
                "            print(f\"Warning: Task {task_id} not found in solutions.json\")\n",
                "            continue\n",
                "\n",
                "        gt_grids = sols[task_id]\n",
                "        print(gt_grids)\n",
                "        for i, attempts in enumerate(attempts_list):\n",
                "            if i >= len(gt_grids):\n",
                "                break\n",
                "\n",
                "            # 1. Retrieve Grids\n",
                "            gt = gt_grids[i]\n",
                "            att1 = attempts.get(\"attempt_1\")\n",
                "            att2 = attempts.get(\"attempt_2\")\n",
                "\n",
                "            # 2. Check Correctness\n",
                "            pass1 = (att1 == gt) if att1 is not None else False\n",
                "            pass2 = (att2 == gt) if att2 is not None else False\n",
                "\n",
                "            if pass1 and pass2:\n",
                "                status = \"Pass - both\"\n",
                "            elif pass1:\n",
                "                status = \"Pass - 1\"\n",
                "            elif pass2:\n",
                "                status = \"Pass - 2\"\n",
                "            else:\n",
                "                status = \"Fail\"\n",
                "\n",
                "            # 3. Visualize\n",
                "            # Construct list: [Ground Truth, Attempt 1, Attempt 2]\n",
                "            grids_to_plot = [gt]\n",
                "            if att1 is not None:\n",
                "                grids_to_plot.append(att1)\n",
                "            if att2 is not None:\n",
                "                grids_to_plot.append(att2)\n",
                "\n",
                "            header = f\"Task: {task_id} | Pair: {i} | Status: {status}\"\n",
                "            print(f\"Plotting {header}\")\n",
                "\n",
                "            # utils.plot_grids handles the matplotlib figure creation\n",
                "            try:\n",
                "                utils.plot_grids(grids_to_plot, title=header)\n",
                "            except Exception as e:\n",
                "                print(f\"Skipping plot for {task_id} due to error: {e}\")"
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}