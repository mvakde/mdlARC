{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e6aae73",
   "metadata": {},
   "source": [
    "## 29% using a 29M transformer\n",
    "Steps to reproduce:  \n",
    "1. Upload this script to google colab or modal\n",
    "2. (optional) If you want to save checkpoints and results, mount your google drive (colab) / your volume (modal)\n",
    "3. Click run-all\n",
    "\n",
    "This script ensures there's no data contamination. \n",
    "This produces a `submission.json` file. The `submission.json` file\n",
    "\n",
    "\n",
    "Notes:\n",
    "1. The config in this notebook has been tuned for an 80GB A100  \n",
    "2. Actual results were obtained by running this exact file in 2 phases.  \n",
    "    - Training on a 40GB A100\n",
    "    - Take the final checkpoint, and run the inference on an 80GB A100\n",
    "\n",
    "This will work on smaller GPUs too, but will take longer to train  \n",
    "For very constrained environments, disable the \"do_validate\" flag. This avoids checking the validation loss every epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93163d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# root_folder, mount_folder = \"root\", \"mnt/mithil-arc\" # for modal - REPLACE /mithil-arc WITH YOUR VOLUME NAME\n",
    "root_folder, mount_folder = \"content\", \"content/drive/MyDrive\"  # for colab\n",
    "\n",
    "%cd /$root_folder/\n",
    "!git clone https://github.com/mvakde/mdlARC.git # `-b <branch_name> --single-branch` if branch\n",
    "%cd /$root_folder/mdlARC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44cfa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python dataset_building_scripts/build_datasets.py --datasets arc1 conceptarc  --splits train eval --with-solutions --cleanup none\n",
    "!python dataset_building_scripts/augment_dataset_dihedral.py\n",
    "\n",
    "!rm -rf /$root_folder/mdlARC/run-script.ipynb\n",
    "!rm -rf /$root_folder/mdlARC/sanitised-env-run-script.ipynb\n",
    "!rm -rf /$root_folder/mdlARC/ultra-sanitised-env-run-script.ipynb\n",
    "!rm -rf /$root_folder/mdlARC/dataset_building_scripts\n",
    "!rm -rf /$root_folder/mdlARC/readme.md\n",
    "!rm -rf /$root_folder/mdlARC/img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475c6868",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import argparse\n",
    "import importlib\n",
    "import sys\n",
    "\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "SRC_DIR = PROJECT_ROOT / \"src\"\n",
    "if SRC_DIR.exists() and str(SRC_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC_DIR))\n",
    "\n",
    "import utils, tinytransformer, train\n",
    "\n",
    "importlib.reload(utils)  # pick up code changes during iteration\n",
    "importlib.reload(tinytransformer)\n",
    "importlib.reload(train)\n",
    "\n",
    "args = {\n",
    "    # run config\n",
    "    \"num_workers\": 0,\n",
    "    \"device\": \"cuda\",  # 'cuda' | 'mps' | 'cpu'\n",
    "    \"do_validate\": False,\n",
    "    \"name\": \"arc1-cleanenv-30M-vvwide-bs32-101ep-100color-ccdb-18dec0430\",  # download file name\n",
    "    \"GPU\": \"A100-noaugreg\",  # just for logging purposes\n",
    "    # paths - must pass as Path(\"<path_to_dir>\")\n",
    "    \"train_log_file\": Path(\"runs/training_log.txt\"),\n",
    "    \"save_path\": Path(\"runs/tiny.pt\"),\n",
    "    \"checkpoint_path\": None,  # Path(\"runs/tiny.pt\"),  # or None to start from scratch\n",
    "    \"data_path\": Path(\"assets/challenges_dihedral_both.json\"),\n",
    "    # hyperparameters\n",
    "    \"epochs\": 101,\n",
    "    \"batch_size\": 32,\n",
    "    \"val_batch_size\": 300,\n",
    "    \"enable_color_aug_train\": True,\n",
    "    \"max_color_augments_train\": 100,\n",
    "    \"color_aug_seed\": 42,\n",
    "    \"lr\": 3e-4,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"grad_clip\": 1.0,\n",
    "    \"dropout\": 0.1,\n",
    "    \"seed\": 42,\n",
    "    # Model Architecture\n",
    "    \"d_model\": 768,  # 128, 256, 512, 768 | 128, 384, 640\n",
    "    \"n_heads\": 12,  # 4, 8, 8/16, 12 | 4, 12, 10\n",
    "    \"d_ff\": 3072,  # 512, 1024, 2048, 3072 | 512, 1536, 2560\n",
    "    \"n_layers\": 4,  # 4, 6, 16, 16 | 24, 28, 24\n",
    "    # Visibility toggles\n",
    "    \"log_train_strings\": False,\n",
    "    \"log_train_limit\": 10,\n",
    "    \"log_inference_prompt\": False,\n",
    "    \"inference_temperature\": None,\n",
    "    \"inference_top_k\": None,\n",
    "}\n",
    "cfg = argparse.Namespace(**args)\n",
    "\n",
    "runs_dir = Path(\"runs\")\n",
    "runs_dir.mkdir(parents=True, exist_ok=True)\n",
    "with (runs_dir / \"config.txt\").open(\"w\") as f:\n",
    "    for k, v in args.items():\n",
    "        f.write(f\"{k}: {v}\\n\")\n",
    "\n",
    "model, dataset, dataloader, device, data_path = train.build_model_and_data(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfa7324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training only\n",
    "\n",
    "from time import perf_counter\n",
    "\n",
    "t_start = perf_counter()\n",
    "train.train_model(cfg,model=model,dataloader=dataloader,dataset=dataset,device=device,data_path=data_path)\n",
    "t_duration = perf_counter() - t_start\n",
    "\n",
    "print(f\"Training took {t_duration:.2f}s\")\n",
    "with open(Path(\"runs/timing.txt\"), \"w\") as f:\n",
    "    f.write(f\"Training: {t_duration:.4f} s\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c84a83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning up memory to run inference\n",
    "utils.cleanup_memory(globals())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d36c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data immediately in case eval fails\n",
    "archive_state = utils.save_run_archive(\n",
    "    cfg.name, root_folder, mount_folder, globals_dict=globals()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283309b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import importlib\n",
    "import evaluations\n",
    "import utils\n",
    "importlib.reload(evaluations)\n",
    "importlib.reload(utils)\n",
    "\n",
    "PATH_BOTH = Path(\"assets/challenges_dihedral_both.json\")\n",
    "\n",
    "EVAL_CONFIGS = [\n",
    "    # (\"eval_125color_both\", 125, PATH_BOTH),\n",
    "    (\"eval_100color_both\", 100, PATH_BOTH),\n",
    "    # (\"eval_10color_both\", 10, PATH_BOTH),\n",
    "    # (\"eval_0color_both\", 0, PATH_BOTH),\n",
    "    # (\"eval_0color_train\", 0, PATH_TRAIN)  # Uses TRAIN path (No Geom TTA on Test)\n",
    "]\n",
    "\n",
    "EVAL_BATCH_SIZE = 1300\n",
    "SPLITS = [\"test\"]\n",
    "CHECKPOINT_PATH = Path(\"runs/tiny.pt\")\n",
    "SOLUTIONS_PRESENT = False\n",
    "EVAL_TASK_IDS = None  # Set to None to evaluate full dataset, or [\"00576224\", ...] for specific tasks\n",
    "LOG_CORRECT_GRIDS = False  # Print the actual grid, IDs, and augmentation indices for fully correct grids\n",
    "\n",
    "evaluations.run_evaluation_configs(\n",
    "    cfg,\n",
    "    EVAL_CONFIGS,\n",
    "    eval_batch_size=EVAL_BATCH_SIZE,\n",
    "    splits=SPLITS,\n",
    "    checkpoint_path=CHECKPOINT_PATH,\n",
    "    include_targets=SOLUTIONS_PRESENT,\n",
    "    task_ids=EVAL_TASK_IDS,\n",
    "    log_correct_grids=LOG_CORRECT_GRIDS,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb77a5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# refresh Drive zip\n",
    "archive_state = utils.update_run_archive(\n",
    "    cfg.name, root_folder, mount_folder, globals_dict=globals()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4ce6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation\n",
    "EVAL_SUB_FOLDER = \"eval_100color_both\"\n",
    "VIS_MODE = \"!\"  # \"!\" = compare vs solutions, \"submission\" = attempts-only\n",
    "utils.visualize_eval_submissions(\n",
    "    EVAL_SUB_FOLDER, mode=VIS_MODE, solutions_file=\"assets/solutions.json\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
