{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e001210",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/mdlARC/\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "import importlib\n",
    "import utils, train\n",
    "\n",
    "# Reload order matters: utils first, then train (train imports utils)\n",
    "importlib.reload(utils)\n",
    "importlib.reload(train)  # pick up code changes during iteration\n",
    "\n",
    "# Editable arguments\n",
    "args = {\n",
    "    # \"data_path\": Path(\"assets/script-tests/grouped-tasks-00d62c1b/challenges.json\" ),\n",
    "    # \"data_path\": Path(\"assets/script-tests/grouped-tasks/challenges.json\"),\n",
    "    # \"data_path\": Path(\"assets/ARC-1/grouped-tasks/training/challenges.json\"),\n",
    "    \"data_path\": Path(\"assets/ARC-2/grouped-tasks/training/challenges.json\"),\n",
    "    \"batch_size\": 110,\n",
    "    \"epochs\": 200,\n",
    "    \"lr\": 3e-3,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"grad_clip\": 1.0,\n",
    "    \"num_workers\": 0,\n",
    "    \"device\": \"cuda\",  # 'cuda' | 'mps' | 'cpu'\n",
    "    \"seed\": 42,\n",
    "    \"save_path\": Path(\"runs/tiny.pt\"),\n",
    "    \"checkpoint_path\": None,  # Path('runs/tiny.pt') to load else None\n",
    "    \"eval_only\": False,\n",
    "    \"inference_task_id\": \"e0fb7511\",  # \"3aa6fb7a\",  \"00d62c1b\", \"e0fb7511\" '00576224' to run single inference\n",
    "    \"inference_pair_index\": 0,\n",
    "    # Visibility toggles\n",
    "    \"log_train_strings\": False,\n",
    "    \"log_train_limit\": 10,\n",
    "    \"log_inference_prompt\": True,\n",
    "    \"log_eval_strings\": True,\n",
    "    \"log_eval_limit\": 10,\n",
    "    \"plot_inference_grids\": True,\n",
    "}\n",
    "\n",
    "\n",
    "def make_namespace(d):\n",
    "    # Ensure Path types for known path-like keys\n",
    "    for k in [\"data_path\", \"save_path\", \"checkpoint_path\"]:\n",
    "        if d.get(k) is not None and not isinstance(d[k], Path):\n",
    "            d[k] = Path(d[k])\n",
    "    return argparse.Namespace(**d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6ac355",
   "metadata": {
    "tags": [
     "train"
    ]
   },
   "outputs": [],
   "source": [
    "# Training only\n",
    "cfg = dict(args)\n",
    "cfg[\"eval_only\"] = False\n",
    "ns = make_namespace(cfg)\n",
    "model, dataset, dataloader, device, data_path = train.build_model_and_data(ns)\n",
    "train.train_model(\n",
    "    ns,\n",
    "    model=model,\n",
    "    dataloader=dataloader,\n",
    "    dataset=dataset,\n",
    "    device=device,\n",
    "    data_path=data_path,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5926483b",
   "metadata": {
    "tags": [
     "eval"
    ]
   },
   "outputs": [],
   "source": [
    "# Optional: Single-example inference by task id and pair index\n",
    "# Set args['inference_task_id'] above (e.g., '00576224'), then run this cell.\n",
    "cfg = dict(args)\n",
    "cfg[\"eval_only\"] = True\n",
    "ns = make_namespace(cfg)\n",
    "model, dataset, dataloader, device, data_path = train.build_model_and_data(ns)\n",
    "assert cfg[\"inference_task_id\"] is not None, \"Set inference_task_id in args first.\"\n",
    "cfg[\"inference_task_id\"] = (\n",
    "    \"0520fde7\"  # \"3aa6fb7a\",  \"00d62c1b\", \"e0fb7511\" '00576224', \"0520fde7\" to run single inference\n",
    ")\n",
    "\n",
    "train.run_inference(\n",
    "    model=model,\n",
    "    dataset=dataset,\n",
    "    task_id=cfg[\"inference_task_id\"],\n",
    "    pair_index=cfg[\"inference_pair_index\"],\n",
    "    device=device,\n",
    "    log_prompt=cfg[\"log_inference_prompt\"],\n",
    "    plot_grids_flag=cfg[\"plot_inference_grids\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6989da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test generation on a train pair (input -> predicted output), compare to ground-truth\n",
    "%cd /content/mdlARC/\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "# Local imports from this repo\n",
    "from train import load_checkpoint, resolve_device, greedy_generate\n",
    "from tinytransformer import TinyTransformer, TinyTransformerConfig\n",
    "from utils import (\n",
    "    ARCExampleDataset,\n",
    "    MAX_SEQ_LEN,\n",
    "    IO_SEPARATOR_TOKEN_ID,\n",
    "    extract_output_tokens,\n",
    "    tokens_to_grid,\n",
    "    split_grids_from_tokens,\n",
    "    tokens_to_string,\n",
    "    plot_grids,\n",
    ")\n",
    "\n",
    "# ---- Configuration (edit these) ----\n",
    "CHECKPOINT_PATH = \"runs/tiny.pt\"  # e.g., \"runs/ckpt.pt\"; or None to test random weights\n",
    "DATA_PATH = \"assets/ARC-1/grouped-tasks/training/challenges.json\"  # or \"assets/ARC-2/grouped-tasks/training/challenges.json\"\n",
    "DEVICE = \"cuda\"  # \"cuda\", \"mps\", or \"cpu\" (auto-fallback if unavailable)\n",
    "TASK_ID = \"00d62c1b\"  # e.g., \"00d62c1b\"; if None, picks the first available train pair\n",
    "PAIR_INDEX = 0\n",
    "PLOT = True\n",
    "# ------------------------------------\n",
    "\n",
    "# Load checkpoint (optional)\n",
    "ckpt = load_checkpoint(Path(CHECKPOINT_PATH)) if CHECKPOINT_PATH else None\n",
    "device = resolve_device(DEVICE)\n",
    "\n",
    "# Keep dataset tasks aligned with checkpoint (so example_embedding size matches)\n",
    "task_whitelist = ckpt.get(\"task_ids\") if ckpt and \"task_ids\" in ckpt else None\n",
    "\n",
    "# Build dataset (includes outputs so we can compare)\n",
    "dataset = ARCExampleDataset(\n",
    "    json_path=Path(DATA_PATH),\n",
    "    splits=(\"train\", \"test\"),\n",
    "    include_outputs=True,\n",
    "    max_seq_len=MAX_SEQ_LEN,\n",
    "    task_whitelist=task_whitelist,\n",
    ")\n",
    "\n",
    "# Build model config (prefer the one saved in checkpoint)\n",
    "if ckpt and \"config\" in ckpt:\n",
    "    cfg = TinyTransformerConfig(**ckpt[\"config\"])\n",
    "else:\n",
    "    cfg = TinyTransformerConfig(num_examples=dataset.num_examples)\n",
    "\n",
    "model = TinyTransformer(cfg).to(device)\n",
    "if ckpt:\n",
    "    model.load_state_dict(ckpt[\"model_state\"], strict=False)\n",
    "\n",
    "# Select a train example\n",
    "if TASK_ID is None:\n",
    "    ex = next(dataset.iter_examples(split=\"train\", has_output=True))\n",
    "else:\n",
    "    ex = next(\n",
    "        e\n",
    "        for e in dataset.iter_examples(split=\"train\", has_output=True)\n",
    "        if e.task_id == TASK_ID and e.pair_index == PAIR_INDEX\n",
    "    )\n",
    "\n",
    "# Build prompt: input tokens up to and including the separator\n",
    "seq = ex.tokens.tolist()\n",
    "try:\n",
    "    sep_ix = seq.index(IO_SEPARATOR_TOKEN_ID)\n",
    "except ValueError:\n",
    "    raise RuntimeError(\"Selected train example is missing <input_output_separator>.\")\n",
    "prompt_tokens = seq[: sep_ix + 1]\n",
    "\n",
    "# Generate\n",
    "generated = greedy_generate(\n",
    "    model=model,\n",
    "    prompt_tokens=torch.tensor(prompt_tokens, dtype=torch.long),\n",
    "    example_id=ex.example_id,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Decode prediction and reference\n",
    "gen_tokens_after_sep = extract_output_tokens(generated.tolist())\n",
    "predicted_grid = tokens_to_grid(gen_tokens_after_sep)\n",
    "all_grids = split_grids_from_tokens(ex.tokens.tolist())\n",
    "reference_grid = all_grids[1] if len(all_grids) > 1 else []\n",
    "\n",
    "# Log results\n",
    "print(f\"Task: {ex.task_id} | Pair: {ex.pair_index}\")\n",
    "print(\"\\nPrompt (string):\")\n",
    "print(tokens_to_string(prompt_tokens))\n",
    "print(\"\\nGenerated output (string):\")\n",
    "print(tokens_to_string(gen_tokens_after_sep))\n",
    "\n",
    "print(\"\\nPredicted grid:\")\n",
    "if predicted_grid:\n",
    "    for row in predicted_grid:\n",
    "        print(row)\n",
    "else:\n",
    "    print(\"<empty>\")\n",
    "\n",
    "print(\"\\nReference grid:\")\n",
    "if reference_grid:\n",
    "    for row in reference_grid:\n",
    "        print(row)\n",
    "else:\n",
    "    print(\"<empty>\")\n",
    "\n",
    "print(\"\\nExact grid match:\", predicted_grid == reference_grid)\n",
    "\n",
    "# Optional plotting: input grid vs predicted output\n",
    "if PLOT:\n",
    "    try:\n",
    "        input_grid = all_grids[0] if all_grids else []\n",
    "        to_plot = [input_grid, predicted_grid]\n",
    "        plot_grids(to_plot, title=f\"task {ex.task_id} pair {ex.pair_index}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Plotting failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval-only across test pairs (requires a checkpoint or weights already in memory)\n",
    "cfg = dict(args)\n",
    "cfg[\"eval_only\"] = True\n",
    "ns = make_namespace(cfg)\n",
    "model, dataset, dataloader, device, data_path = train.build_model_and_data(ns)\n",
    "train.evaluate_model(\n",
    "    ns, model=model, dataset=dataset, device=device, data_path=data_path\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0a1dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train + Eval combo (convenience)\n",
    "cfg = dict(args)\n",
    "cfg[\"eval_only\"] = False\n",
    "ns = make_namespace(cfg)\n",
    "model, dataset, dataloader, device, data_path = train.build_model_and_data(ns)\n",
    "train.train_model(\n",
    "    ns,\n",
    "    model=model,\n",
    "    dataloader=dataloader,\n",
    "    dataset=dataset,\n",
    "    device=device,\n",
    "    data_path=data_path,\n",
    ")\n",
    "train.evaluate_model(\n",
    "    ns, model=model, dataset=dataset, device=device, data_path=data_path\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}