{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f99df6f",
   "metadata": {},
   "source": [
    "# Sanitized reproduction (no Kaggle account)\n",
    "This notebook is a clean, self‑contained reproduction that **does not require Kaggle credentials**. It downloads the official ARC datasets from GitHub, constructs a training/eval dataset that excludes eval solutions, trains the model from scratch, and runs inference on eval inputs only.\n",
    "\n",
    "## Why this proves there is no leakage\n",
    "- The dataset is built from ARC‑1 training pairs (inputs + outputs), ARC‑1 eval **inputs only**, and optional ConceptARC. Eval solutions are never downloaded.\n",
    "- After building `assets/challenges_dihedral_both.json`, every file (other than core model files) is deleted, so there’s nowhere for solutions to hide.\n",
    "- Training starts from scratch (`checkpoint_path=None`) and uses only the cleaned dataset file.\n",
    "- Inference runs with `SOLUTIONS_PRESENT=False` and produces `submission.json` without reading any solutions.\n",
    "- If you want to be extra strict, delete the final visualization cell and score the submission yourself.\n",
    "\n",
    "FYI:\n",
    "- The core files do not make any calls to the internet, or use extra libraries (written in pure pytorch), and there's no loading of checkpoints\n",
    "\n",
    "## What to change\n",
    "- Pick a runconfig (with or without ConceptARC) in the config cell below.\n",
    "- Reduce `EVAL_BATCH_SIZE` if your GPU is smaller than A100.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbcfb30",
   "metadata": {},
   "source": [
    "Steps:\n",
    "- Upload this on colab/modal\n",
    "    - No need to mount your google drive or modal volume\n",
    "- Decide which experiment you want to reproduce (list below) and modify config accordingly\n",
    "- Choose A100\n",
    "- hit run all\n",
    "\n",
    "The original run used an extra dataset called conceptARC. This dataset is clean. But to reduce the burden of verification, I add a config where this dataset is not used (performance reduces a bit)\n",
    "\n",
    "Configs\n",
    "- Run with ConceptARC for 11 epochs (10 color augments) - about 8-9%\n",
    "- Run with ConceptARC for 101 epochs (100 color augments) - 25-28%\n",
    "- Run without ConceptARC for 11 epochs (10 color augments) - about 7%\n",
    "- Run without ConceptARC for 101 epochs (100 color augments) - about 23%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faac0923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose runconfig\n",
    "runconfig = [\"concept\", 11]  # expect 8-9%\n",
    "# runconfig = [\"concept\", 21]  # expect 16%\n",
    "# runconfig = [\"concept\", 101] # expect 25-28%\n",
    "\n",
    "# This runconfig has less verification burden by removing exrtra dataset, but perf drops slightly\n",
    "# runconfig = [\"no_concept\", 11] # expect 7%\n",
    "# runconfig = [\"no_concept\", 11] # expect 14%\n",
    "# runconfig = [\"no_concept\", 101] # expect 23%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5607152d",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_folder = \"root\"\n",
    "# root_folder = \"content\" # for colab\n",
    "\n",
    "%cd /$root_folder/\n",
    "!git clone -b clean --single-branch https://github.com/mvakde/mdlARC.git # `-b <branch_name> --single-branch` if branch\n",
    "%cd /$root_folder/mdlARC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f7973f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build clean train dataset in assets WITHOUT solutions or other clutter\n",
    "!python dataset_building_scripts/download_datasets.py\n",
    "!python dataset_building_scripts/group_arc_tasks.py\n",
    "if runconfig[0] == \"concept\":\n",
    "    !python dataset_building_scripts/build_datasets.py --config concept_arc1_clean\n",
    "else:\n",
    "    !python dataset_building_scripts/build_datasets.py --config arc1_clean\n",
    "!python dataset_building_scripts/augment_dataset_dihedral.py\n",
    "\n",
    "# Delete all files, especially solutions\n",
    "!find \"assets\" -mindepth 1 ! -path \"assets/challenges_dihedral_both.json\" -exec rm -rf -- {} +\n",
    "!rm -rf /$root_folder/mdlARC/run-script.ipynb\n",
    "!rm -rf /$root_folder/mdlARC/sanitised-env-run-script.ipynb\n",
    "!rm -rf /$root_folder/mdlARC/dataset_building_scripts\n",
    "!rm -rf /$root_folder/mdlARC/readme.md\n",
    "!rm -rf /$root_folder/mdlARC/img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1095cef3",
   "metadata": {},
   "source": [
    "## Data is now “solution‑free”\n",
    "At this point, the only dataset file we keep is `assets/challenges_dihedral_both.json`.\n",
    "- It contains **train inputs/outputs** and **eval inputs only**.\n",
    "- All other files and folders (including anything that could contain solutions) are deleted.\n",
    "\n",
    "You can inspect this file before continuing if you want to verify it manually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2c9571",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import argparse\n",
    "import importlib\n",
    "import sys\n",
    "\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "SRC_DIR = PROJECT_ROOT / \"src\"\n",
    "if SRC_DIR.exists() and str(SRC_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC_DIR))\n",
    "\n",
    "import utils, tinytransformer, train\n",
    "\n",
    "importlib.reload(utils)  # pick up code changes during iteration\n",
    "importlib.reload(tinytransformer)\n",
    "importlib.reload(train)\n",
    "\n",
    "args = {\n",
    "    # run config\n",
    "    \"num_workers\": 0,\n",
    "    \"device\": \"cuda\",  # 'cuda' | 'mps' | 'cpu'\n",
    "    \"do_validate\": False,\n",
    "    \"name\": \"arc1-cleanenv-30M-vvwide-bs32-101ep-100color-ccdb-18dec0430\",  # download file name\n",
    "    \"GPU\": \"A100-noaugreg\",  # just for logging purposes\n",
    "    # paths - must pass as Path(\"<path_to_dir>\")\n",
    "    \"train_log_file\": Path(\"runs/training_log.txt\"),\n",
    "    \"save_path\": Path(\"runs/tiny.pt\"),\n",
    "    \"checkpoint_path\": None,  # Path(\"runs/tiny.pt\"),  # or None to start from scratch\n",
    "    \"data_path\": Path(\"assets/challenges_dihedral_both.json\"),\n",
    "    # hyperparameters\n",
    "    \"epochs\": runconfig[1],\n",
    "    \"batch_size\": 32,\n",
    "    \"val_batch_size\": 300,\n",
    "    \"enable_color_aug_train\": True,\n",
    "    \"max_color_augments_train\": (runconfig[1] - 1),\n",
    "    \"color_aug_seed\": 42,\n",
    "    \"lr\": 3e-4,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"grad_clip\": 1.0,\n",
    "    \"dropout\": 0.1,\n",
    "    \"seed\": 42,\n",
    "    # Model Architecture\n",
    "    \"d_model\": 768,  # 128, 256, 512, 768 | 128, 384, 640\n",
    "    \"n_heads\": 12,  # 4, 8, 8/16, 12 | 4, 12, 10\n",
    "    \"d_ff\": 3072,  # 512, 1024, 2048, 3072 | 512, 1536, 2560\n",
    "    \"n_layers\": 4,  # 4, 6, 16, 16 | 24, 28, 24\n",
    "    # Visibility toggles\n",
    "    \"log_train_strings\": False,\n",
    "    \"log_train_limit\": 10,\n",
    "    \"log_inference_prompt\": False,\n",
    "}\n",
    "cfg = argparse.Namespace(**args)\n",
    "\n",
    "runs_dir = Path(\"runs\")\n",
    "runs_dir.mkdir(parents=True, exist_ok=True)\n",
    "with (runs_dir / \"config.txt\").open(\"w\") as f:\n",
    "    for k, v in args.items():\n",
    "        f.write(f\"{k}: {v}\\n\")\n",
    "\n",
    "model, dataset, dataloader, device, data_path = train.build_model_and_data(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0088a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training only\n",
    "\n",
    "from time import perf_counter\n",
    "\n",
    "t_start = perf_counter()\n",
    "\n",
    "# ---\n",
    "# direct\n",
    "train.train_model(\n",
    "    cfg,\n",
    "    model=model,\n",
    "    dataloader=dataloader,\n",
    "    dataset=dataset,\n",
    "    device=device,\n",
    "    data_path=data_path,\n",
    ")\n",
    "\n",
    "\n",
    "# # periodic checkpointing\n",
    "# cfg.save_path = Path(f\"runs/tiny-{cfg.epochs}.pt\")\n",
    "# for i in range(3):\n",
    "#   if i != 0:\n",
    "#     cfg.checkpoint_path = cfg.save_path\n",
    "#     cfg.save_path = Path(f\"runs/tiny-{cfg.epochs*(i+1)}.pt\")\n",
    "#   train.train_model(cfg, model=model, dataloader=dataloader, dataset=dataset, device=device, data_path=data_path)\n",
    "# ---\n",
    "\n",
    "t_duration = perf_counter() - t_start\n",
    "print(f\"Training took {t_duration:.2f}s\")\n",
    "\n",
    "with open(Path(\"runs/timing.txt\"), \"w\") as f:\n",
    "    f.write(f\"Training: {t_duration:.4f} s\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbdab1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning up memory to run inference\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "# 1. Delete global references to free memory\n",
    "# Deleting 'model' ensures Cell 4 reloads a fresh instance from the checkpoint,\n",
    "# preventing memory fragmentation or leftover gradients from training.\n",
    "for name in [\"model\", \"dataset\", \"dataloader\", \"optimizer\", \"scheduler\"]:\n",
    "    if name in globals():\n",
    "        del globals()[name]\n",
    "\n",
    "# 2. Reset compiled graph caches (crucial if torch.compile was used)\n",
    "if hasattr(torch, \"_dynamo\"):\n",
    "    torch._dynamo.reset()\n",
    "\n",
    "# 3. Force garbage collection and clear GPU memory\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n",
    "\n",
    "print(f\"GPU cleaned. Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcd927d",
   "metadata": {},
   "source": [
    "## Submission is generated without solutions\n",
    "The next cells run inference and generate `runs/<run_name>/submission.json` with **no access to eval solutions**.  \n",
    "Everything stays solution‑free unless you choose to add your own scoring later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27492f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import inference\n",
    "import tinytransformer\n",
    "import utils\n",
    "import sys\n",
    "import json\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "from time import perf_counter\n",
    "\n",
    "# Reload modules to pick up changes\n",
    "importlib.reload(tinytransformer)\n",
    "importlib.reload(inference)\n",
    "importlib.reload(utils)\n",
    "\n",
    "# Define your paths constants\n",
    "PATH_BOTH = Path(\"assets/challenges_dihedral_both.json\")\n",
    "\n",
    "# Config List: (Run Name, Max Color Augments, Dataset Path)\n",
    "EVAL_CONFIGS = [(\"eval\", runconfig[1] - 1, PATH_BOTH)]\n",
    "\n",
    "# Global settings shared across runs\n",
    "EVAL_BATCH_SIZE = 1300\n",
    "SPLITS = [\"test\"]\n",
    "CHECKPOINT_PATH = Path(\"runs/tiny.pt\")\n",
    "SOLUTIONS_PRESENT = False\n",
    "EVAL_TASK_IDS = None  # Set to None to evaluate full dataset, or [\"00576224\", ...] for specific tasks\n",
    "LOG_CORRECT_GRIDS = False  # Print the actual grid, IDs, and augmentation indices for fully correct grids\n",
    "\n",
    "\n",
    "# Helper class for logging to file and console\n",
    "class TeeLogger(object):\n",
    "    def __init__(self, filepath):\n",
    "        self.terminal = sys.stdout\n",
    "        self.log = open(filepath, \"w\")\n",
    "\n",
    "    def write(self, message):\n",
    "        self.terminal.write(message)\n",
    "        self.log.write(message)\n",
    "\n",
    "    def flush(self):\n",
    "        self.terminal.flush()\n",
    "        self.log.flush()\n",
    "\n",
    "    def close(self):\n",
    "        self.log.close()\n",
    "\n",
    "\n",
    "def run_evaluation_pipeline(run_name, max_color_augments, dataset_path, device):\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"STARTING PIPELINE: {run_name} (Color Augs: {max_color_augments})\")\n",
    "    print(f\"{'=' * 60}\\n\")\n",
    "\n",
    "    # 1. Setup Directories\n",
    "    base_run_dir = Path(\"runs\") / run_name\n",
    "    base_run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    eval_log_path = base_run_dir / \"eval_log.txt\"\n",
    "    aaivr_log_path = base_run_dir / \"aaivr.txt\"\n",
    "    submission_path = base_run_dir / \"submission.json\"\n",
    "\n",
    "    # 2. Update Config\n",
    "    cfg.checkpoint_path = CHECKPOINT_PATH\n",
    "    cfg.data_path = dataset_path\n",
    "    cfg.enable_color_aug_eval = max_color_augments > 0\n",
    "    cfg.max_color_augments_eval = max_color_augments\n",
    "\n",
    "    # 3. Build/Rebuild Model & Data\n",
    "    # We rebuild the dataloader every time to handle the different color augmentation settings\n",
    "    print(\"Building model and dataloader for config...\")\n",
    "\n",
    "    # Load checkpoint explicitly to pass to build function\n",
    "    checkpoint = torch.load(\n",
    "        cfg.checkpoint_path, map_location=device, weights_only=False\n",
    "    )\n",
    "\n",
    "    # Check if model exists in global scope to reuse weights, else create\n",
    "    global model\n",
    "    if \"model\" in globals() and model is not None:\n",
    "        model.load_state_dict(\n",
    "            checkpoint[\"model_state\"] if \"model_state\" in checkpoint else checkpoint,\n",
    "            strict=False,\n",
    "        )\n",
    "        model.eval()\n",
    "        # Rebuild only dataset/loader\n",
    "        _, dataset, dataloader, device, _ = train.build_model_and_data(\n",
    "            cfg, checkpoint=checkpoint\n",
    "        )\n",
    "    else:\n",
    "        model, dataset, dataloader, device, _ = train.build_model_and_data(cfg)\n",
    "\n",
    "    # 4. Run Inference (Logic from old Cell 3)\n",
    "    def log_eval(msg):\n",
    "        print(msg)\n",
    "        with open(eval_log_path, \"a\") as f:\n",
    "            f.write(msg + \"\\n\")\n",
    "\n",
    "    color_mappings_eval = None\n",
    "    color_apply_fn = None\n",
    "    if cfg.enable_color_aug_eval and cfg.max_color_augments_eval > 0:\n",
    "        color_seed = cfg.color_aug_seed or cfg.seed\n",
    "        color_mappings_eval = utils.generate_color_mapping_tensors(\n",
    "            cfg.max_color_augments_eval, color_seed\n",
    "        )\n",
    "        color_apply_fn = lambda split: True\n",
    "\n",
    "    evaluation = inference.evaluate_model_on_dataset(\n",
    "        model=model,\n",
    "        dataset=dataset,\n",
    "        device=device,\n",
    "        batch_size=EVAL_BATCH_SIZE,\n",
    "        log_prompts=args[\"log_inference_prompt\"],\n",
    "        splits=SPLITS,\n",
    "        color_mappings=color_mappings_eval,\n",
    "        color_apply_fn=color_apply_fn,\n",
    "        task_ids=EVAL_TASK_IDS,\n",
    "        include_targets=SOLUTIONS_PRESENT,\n",
    "    )\n",
    "\n",
    "    # Redirect stdout for AAIVR logging\n",
    "    if hasattr(sys.stdout, \"log\"):\n",
    "        sys.stdout = sys.stdout.terminal  # Reset if needed\n",
    "    sys.stdout = TeeLogger(str(aaivr_log_path))\n",
    "\n",
    "    try:\n",
    "        test_results = evaluation.get(\"test\", {}).get(\"results\", [])\n",
    "        dataset_has_dihedral_augments = \"dihedral_both\" in str(cfg.data_path)\n",
    "\n",
    "        aaivr_results = []\n",
    "        if test_results:\n",
    "            aaivr_results = utils.run_aaivr_on_results(\n",
    "                test_results,\n",
    "                is_dihedral_augmented=dataset_has_dihedral_augments,\n",
    "                color_aug_seed=cfg.color_aug_seed,\n",
    "                max_color_augments=cfg.max_color_augments_eval,\n",
    "            )\n",
    "\n",
    "            # # Print Stats (will go to console + aaivr.txt)\n",
    "            # utils.summarize_aaivr_pass_at_k(aaivr_results)\n",
    "            # if aaivr_results:\n",
    "            #     tasks_map = {}\n",
    "            #     for res in aaivr_results:\n",
    "            #         if res.task_id not in tasks_map:\n",
    "            #             tasks_map[res.task_id] = []\n",
    "            #         tasks_map[res.task_id].append(res)\n",
    "\n",
    "            #     arc_score = 0.0\n",
    "            #     total_tasks = len(tasks_map)\n",
    "\n",
    "            #     for t_id, pairs in tasks_map.items():\n",
    "            #         n_pairs = len(pairs)\n",
    "            #         if n_pairs > 0:\n",
    "            #             n_solved = sum(1 for p in pairs if p.pass_at_k)\n",
    "            #             arc_score += (n_solved / n_pairs)\n",
    "\n",
    "            #     max_score = total_tasks\n",
    "            #     pct = (arc_score / max_score * 100) if max_score > 0 else 0.0\n",
    "            #     print(f\"Official ARC style scoring: {arc_score:.2f}/{max_score} ({pct:.2f}%)\")\n",
    "        else:\n",
    "            print(\"No test results for AAIVR.\")\n",
    "\n",
    "    finally:\n",
    "        # Always restore stdout\n",
    "        if hasattr(sys.stdout, \"terminal\"):\n",
    "            sys.stdout.close()\n",
    "            sys.stdout = sys.stdout.terminal\n",
    "\n",
    "    # 6. Generate Submission (Logic from old Cell 5)\n",
    "    print(f\"Generating submission.json for {run_name}...\")\n",
    "    submission_data = {}\n",
    "    temp_grouping = {}\n",
    "\n",
    "    if aaivr_results:\n",
    "        for item in aaivr_results:\n",
    "            t_id = item.task_id\n",
    "            p_idx = item.original_pair_index\n",
    "            if t_id not in temp_grouping:\n",
    "                temp_grouping[t_id] = {}\n",
    "\n",
    "            top_grids = item.selected_outputs[:2]\n",
    "            if not top_grids:\n",
    "                top_grids = [[[0]]]  # Fallback\n",
    "\n",
    "            pair_dict = {\n",
    "                \"attempt_1\": top_grids[0],\n",
    "                \"attempt_2\": top_grids[1] if len(top_grids) > 1 else top_grids[0],\n",
    "            }\n",
    "            temp_grouping[t_id][p_idx] = pair_dict\n",
    "\n",
    "        for t_id, pairs_map in temp_grouping.items():\n",
    "            sorted_indices = sorted(pairs_map.keys())\n",
    "            submission_data[t_id] = [pairs_map[idx] for idx in sorted_indices]\n",
    "\n",
    "    with open(submission_path, \"w\") as f:\n",
    "        json.dump(submission_data, f)\n",
    "\n",
    "    print(f\"Finished {run_name}. Submission saved to {submission_path}\")\n",
    "\n",
    "\n",
    "# --- Execute the Loop (Modified with Timing) ---\n",
    "timing_path = Path(\"runs/timing.txt\")\n",
    "\n",
    "for name, aug_count, d_path in EVAL_CONFIGS:  # <--- Unpack 3 items\n",
    "    t_start = perf_counter()\n",
    "\n",
    "    run_evaluation_pipeline(name, aug_count, d_path, device)\n",
    "\n",
    "    t_duration = perf_counter() - t_start\n",
    "    print(f\"Run {name} took {t_duration:.2f}s\")\n",
    "\n",
    "    with open(timing_path, \"a\") as f:\n",
    "        f.write(f\"Evaluation {name}: {t_duration:.4f} s\\n\")\n",
    "\n",
    "print(\"\\nAll evaluation runs completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cab738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare mode requires the solutions file; submission mode does not\n",
    "# visualisation\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "EVAL_SUB_FOLDER = EVAL_CONFIGS[0][0]\n",
    "VIS_MODE = \"submission\"  # \"!\" = compare vs solutions, \"submission\" = attempts-only\n",
    "\n",
    "submission_file = Path(f\"runs/{EVAL_SUB_FOLDER}/submission.json\")\n",
    "# solutions_file = Path(\"assets/solutions.json\")\n",
    "\n",
    "if not submission_file.exists():\n",
    "    print(f\"Error: Could not find submission file: {submission_file}\")\n",
    "elif VIS_MODE == \"!\":\n",
    "    if not solutions_file.exists():\n",
    "        print(\n",
    "            f\"Error: Could not find solutions file for compare mode:\\n{solutions_file}\"\n",
    "        )\n",
    "    else:\n",
    "        # Load Data\n",
    "        with open(submission_file, \"r\") as f:\n",
    "            subs = json.load(f)\n",
    "        with open(solutions_file, \"r\") as f:\n",
    "            sols = json.load(f)\n",
    "\n",
    "        print(f\"Visualizing comparison for {len(subs)} tasks...\")\n",
    "\n",
    "        for task_id, attempts_list in subs.items():\n",
    "            # Get Ground Truth (list of grids)\n",
    "            if task_id not in sols:\n",
    "                print(f\"Warning: Task {task_id} not found in solutions.json\")\n",
    "                continue\n",
    "\n",
    "            gt_grids = sols[task_id]\n",
    "            print(gt_grids)\n",
    "            for i, attempts in enumerate(attempts_list):\n",
    "                if i >= len(gt_grids):\n",
    "                    break\n",
    "\n",
    "                # 1. Retrieve Grids\n",
    "                gt = gt_grids[i]\n",
    "                att1 = attempts.get(\"attempt_1\")\n",
    "                att2 = attempts.get(\"attempt_2\")\n",
    "\n",
    "                # 2. Check Correctness\n",
    "                pass1 = (att1 == gt) if att1 is not None else False\n",
    "                pass2 = (att2 == gt) if att2 is not None else False\n",
    "\n",
    "                if pass1 and pass2:\n",
    "                    status = \"Pass - both\"\n",
    "                elif pass1:\n",
    "                    status = \"Pass - 1\"\n",
    "                elif pass2:\n",
    "                    status = \"Pass - 2\"\n",
    "                else:\n",
    "                    status = \"Fail\"\n",
    "\n",
    "                # 3. Visualize\n",
    "                # Construct list: [Ground Truth, Attempt 1, Attempt 2]\n",
    "                grids_to_plot = [gt]\n",
    "                if att1 is not None:\n",
    "                    grids_to_plot.append(att1)\n",
    "                if att2 is not None:\n",
    "                    grids_to_plot.append(att2)\n",
    "\n",
    "                header = f\"Task: {task_id} | Pair: {i} | Status: {status}\"\n",
    "                print(f\"Plotting {header}\")\n",
    "\n",
    "                # utils.plot_grids handles the matplotlib figure creation\n",
    "                try:\n",
    "                    utils.plot_grids(grids_to_plot, title=header)\n",
    "                except Exception as e:\n",
    "                    print(f\"Skipping plot for {task_id} due to error: {e}\")\n",
    "else:\n",
    "    # Submission-only visualization (attempts without ground truth)\n",
    "    with open(submission_file, \"r\") as f:\n",
    "        subs = json.load(f)\n",
    "\n",
    "    print(f\"Visualizing submissions for {len(subs)} tasks (no solutions)...\")\n",
    "\n",
    "    for task_id, attempts_list in subs.items():\n",
    "        for i, attempts in enumerate(attempts_list):\n",
    "            att1 = attempts.get(\"attempt_1\")\n",
    "            att2 = attempts.get(\"attempt_2\")\n",
    "\n",
    "            grids_to_plot = []\n",
    "            if att1 is not None:\n",
    "                grids_to_plot.append(att1)\n",
    "            if att2 is not None:\n",
    "                grids_to_plot.append(att2)\n",
    "\n",
    "            if not grids_to_plot:\n",
    "                print(f\"Skipping {task_id} pair {i} (no attempts)\")\n",
    "                continue\n",
    "\n",
    "            header = f\"Task: {task_id} | Pair: {i} | Status: submission-only\"\n",
    "            print(f\"Plotting {header}\")\n",
    "\n",
    "            try:\n",
    "                utils.plot_grids(grids_to_plot, title=header)\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping plot for {task_id} due to error: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
