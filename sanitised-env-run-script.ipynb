{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cbcfb30",
   "metadata": {},
   "source": [
    "This notebook is to prove that there is no dataleak\n",
    "\n",
    "If you're using a GPU smaller than A100 80GB, reduce eval batch size\n",
    "\n",
    "Steps:\n",
    "- Upload this on colab/modal and run\n",
    "    - No need to mount your google drive or modal volume\n",
    "- Decide which experiment you want to reproduce (list below) and modify config accordingly\n",
    "- hit run all\n",
    "\n",
    "How the script is clean:\n",
    "1. Downloads official dataset from ARC's github repo (and ConceptARC)\n",
    "2. Programmatically construct the dataset with all grids other than test outputs (challenges_dihedral_both.json)\n",
    "3. We delete the solutions file\n",
    "4. We also delete every single file and folder not needed for the run. Only the core files remain, which can be inspected - these do not make any calls to the internet\n",
    "5. After training and inference, it generates a `submission.json` file. This is clean because we create it without access to the solutions or the internet.\n",
    "6. The submission.json file is visualised for you to inspect \n",
    "7. You can also download the submission.json file and manually calculate the scores (which is a good double check since I am not providing the verification code)\n",
    "\n",
    "To avoid complaints about using an external dataset (ConceptARC) for augmentation, I add a new training config without ConceptARC (so only the official dataset). This performs slightly worse than the original (23%) for the same epochs (technically smaller dataset means lesser flops, so remains to be seen if it matches original performance at same number of flops)\n",
    "\n",
    "If this checks out, the only possible place a leak can exist is if the `challenges_dihedral_both.json` file somewhow has the solutions. I have manually checked multiple times and there is no leak. 2 other people have checked for me too. You can inspect the file yourself\n",
    "\n",
    "I'll think of a way to make this easier\n",
    "\n",
    "Configs:\n",
    "- Run without ConceptARC for 11 epochs (10 color augments) - about 7%\n",
    "- Run without ConceptARC for 101 epochs (100 color augments) - about 23%\n",
    "- Run with ConceptARC for 11 epochs (10 color augments) - about 8-9%\n",
    "- Run with ConceptARC for 101 epochs (100 color augments) - 25-28%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5607152d",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_folder = \"root\"\n",
    "# root_folder = \"content\" # for colab\n",
    "\n",
    "%cd /$root_folder/\n",
    "!git clone -b clean --single-branch https://github.com/mvakde/mdlARC.git # `-b <branch_name> --single-branch` if branch\n",
    "%cd /$root_folder/mdlARC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faac0923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose runconfig\n",
    "runconfig = [\"concept\", 11]  # runs the fastest, expect 7%\n",
    "# runconfig = [\"concept\", 101] # expect 23%\n",
    "# runconfig = [\"no_concept\", 11] # expect 8-9%\n",
    "# runconfig = [\"no_concept\", 101] # expect 25-28%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f7973f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build clean train dataset in assets without solutions or other clutter\n",
    "!python dataset_building_scripts/download_datasets.py\n",
    "!python dataset_building_scripts/group_arc_tasks.py\n",
    "if runconfig[0] == \"concept\":\n",
    "    !python dataset_building_scripts/build_datasets.py --config concept_arc1_clean\n",
    "else:\n",
    "    !python dataset_building_scripts/build_datasets.py --config arc1_clean\n",
    "!python dataset_building_scripts/augment_dataset_dihedral.py\n",
    "\n",
    "!find \"assets\" -mindepth 1 ! -path \"assets/challenges_dihedral_both.json\" -exec rm -rf -- {} +\n",
    "!rm -rf /$root_folder/mdlARC/run-script.ipynb\n",
    "!rm -rf /$root_folder/mdlARC/sanitised-env-run-script.ipynb\n",
    "!rm -rf /$root_folder/mdlARC/dataset_building_scripts\n",
    "!rm -rf /$root_folder/mdlARC/readme.md\n",
    "!rm -rf /$root_folder/mdlARC/img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2c9571",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import argparse\n",
    "import importlib\n",
    "import sys\n",
    "\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "SRC_DIR = PROJECT_ROOT / \"src\"\n",
    "if SRC_DIR.exists() and str(SRC_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC_DIR))\n",
    "\n",
    "import utils, tinytransformer, train\n",
    "\n",
    "importlib.reload(utils)  # pick up code changes during iteration\n",
    "importlib.reload(tinytransformer)\n",
    "importlib.reload(train)\n",
    "\n",
    "args = {\n",
    "    # run config\n",
    "    \"num_workers\": 0,\n",
    "    \"device\": \"cuda\",  # 'cuda' | 'mps' | 'cpu'\n",
    "    \"do_validate\": False,\n",
    "    \"name\": \"arc1-cleanenv-30M-vvwide-bs32-101ep-100color-ccdb-18dec0430\",  # download file name\n",
    "    \"GPU\": \"A100-noaugreg\",  # just for logging purposes\n",
    "    # paths - must pass as Path(\"<path_to_dir>\")\n",
    "    \"train_log_file\": Path(\"runs/training_log.txt\"),\n",
    "    \"save_path\": Path(\"runs/tiny.pt\"),\n",
    "    \"checkpoint_path\": None,  # Path(\"runs/tiny.pt\"),  # or None to start from scratch\n",
    "    \"data_path\": Path(\"assets/challenges_dihedral_both.json\"),\n",
    "    # hyperparameters\n",
    "    \"epochs\": runconfig[1],\n",
    "    \"batch_size\": 32,\n",
    "    \"val_batch_size\": 300,\n",
    "    \"enable_color_aug_train\": True,\n",
    "    \"max_color_augments_train\": (runconfig[1] - 1),\n",
    "    \"color_aug_seed\": 42,\n",
    "    \"lr\": 3e-4,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"grad_clip\": 1.0,\n",
    "    \"dropout\": 0.1,\n",
    "    \"seed\": 42,\n",
    "    # Model Architecture\n",
    "    \"d_model\": 768,  # 128, 256, 512, 768 | 128, 384, 640\n",
    "    \"n_heads\": 12,  # 4, 8, 8/16, 12 | 4, 12, 10\n",
    "    \"d_ff\": 3072,  # 512, 1024, 2048, 3072 | 512, 1536, 2560\n",
    "    \"n_layers\": 4,  # 4, 6, 16, 16 | 24, 28, 24\n",
    "    # Visibility toggles\n",
    "    \"log_train_strings\": False,\n",
    "    \"log_train_limit\": 10,\n",
    "    \"log_inference_prompt\": False,\n",
    "}\n",
    "cfg = argparse.Namespace(**args)\n",
    "\n",
    "runs_dir = Path(\"runs\")\n",
    "runs_dir.mkdir(parents=True, exist_ok=True)\n",
    "with (runs_dir / \"config.txt\").open(\"w\") as f:\n",
    "    for k, v in args.items():\n",
    "        f.write(f\"{k}: {v}\\n\")\n",
    "\n",
    "model, dataset, dataloader, device, data_path = train.build_model_and_data(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0088a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training only\n",
    "\n",
    "from time import perf_counter\n",
    "\n",
    "t_start = perf_counter()\n",
    "\n",
    "# ---\n",
    "# direct\n",
    "train.train_model(\n",
    "    cfg,\n",
    "    model=model,\n",
    "    dataloader=dataloader,\n",
    "    dataset=dataset,\n",
    "    device=device,\n",
    "    data_path=data_path,\n",
    ")\n",
    "\n",
    "\n",
    "# # periodic checkpointing\n",
    "# cfg.save_path = Path(f\"runs/tiny-{cfg.epochs}.pt\")\n",
    "# for i in range(3):\n",
    "#   if i != 0:\n",
    "#     cfg.checkpoint_path = cfg.save_path\n",
    "#     cfg.save_path = Path(f\"runs/tiny-{cfg.epochs*(i+1)}.pt\")\n",
    "#   train.train_model(cfg, model=model, dataloader=dataloader, dataset=dataset, device=device, data_path=data_path)\n",
    "# ---\n",
    "\n",
    "t_duration = perf_counter() - t_start\n",
    "print(f\"Training took {t_duration:.2f}s\")\n",
    "\n",
    "with open(Path(\"runs/timing.txt\"), \"w\") as f:\n",
    "    f.write(f\"Training: {t_duration:.4f} s\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbdab1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning up memory to run inference\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "# 1. Delete global references to free memory\n",
    "# Deleting 'model' ensures Cell 4 reloads a fresh instance from the checkpoint,\n",
    "# preventing memory fragmentation or leftover gradients from training.\n",
    "for name in [\"model\", \"dataset\", \"dataloader\", \"optimizer\", \"scheduler\"]:\n",
    "    if name in globals():\n",
    "        del globals()[name]\n",
    "\n",
    "# 2. Reset compiled graph caches (crucial if torch.compile was used)\n",
    "if hasattr(torch, \"_dynamo\"):\n",
    "    torch._dynamo.reset()\n",
    "\n",
    "# 3. Force garbage collection and clear GPU memory\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n",
    "\n",
    "print(f\"GPU cleaned. Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27492f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import inference\n",
    "import tinytransformer\n",
    "import utils\n",
    "import sys\n",
    "import json\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "from time import perf_counter\n",
    "\n",
    "# Reload modules to pick up changes\n",
    "importlib.reload(tinytransformer)\n",
    "importlib.reload(inference)\n",
    "importlib.reload(utils)\n",
    "\n",
    "# Define your paths constants\n",
    "PATH_BOTH = Path(\"assets/challenges_dihedral_both.json\")\n",
    "\n",
    "# Config List: (Run Name, Max Color Augments, Dataset Path)\n",
    "EVAL_CONFIGS = [(\"eval\", runconfig[1] - 1, PATH_BOTH)]\n",
    "\n",
    "# Global settings shared across runs\n",
    "EVAL_BATCH_SIZE = 1300\n",
    "SPLITS = [\"test\"]\n",
    "CHECKPOINT_PATH = Path(\"runs/tiny.pt\")\n",
    "SOLUTIONS_PRESENT = False\n",
    "EVAL_TASK_IDS = None  # Set to None to evaluate full dataset, or [\"00576224\", ...] for specific tasks\n",
    "LOG_CORRECT_GRIDS = False  # Print the actual grid, IDs, and augmentation indices for fully correct grids\n",
    "\n",
    "\n",
    "# Helper class for logging to file and console\n",
    "class TeeLogger(object):\n",
    "    def __init__(self, filepath):\n",
    "        self.terminal = sys.stdout\n",
    "        self.log = open(filepath, \"w\")\n",
    "\n",
    "    def write(self, message):\n",
    "        self.terminal.write(message)\n",
    "        self.log.write(message)\n",
    "\n",
    "    def flush(self):\n",
    "        self.terminal.flush()\n",
    "        self.log.flush()\n",
    "\n",
    "    def close(self):\n",
    "        self.log.close()\n",
    "\n",
    "\n",
    "def run_evaluation_pipeline(run_name, max_color_augments, dataset_path, device):\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"STARTING PIPELINE: {run_name} (Color Augs: {max_color_augments})\")\n",
    "    print(f\"{'=' * 60}\\n\")\n",
    "\n",
    "    # 1. Setup Directories\n",
    "    base_run_dir = Path(\"runs\") / run_name\n",
    "    base_run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    eval_log_path = base_run_dir / \"eval_log.txt\"\n",
    "    aaivr_log_path = base_run_dir / \"aaivr.txt\"\n",
    "    submission_path = base_run_dir / \"submission.json\"\n",
    "\n",
    "    # 2. Update Config\n",
    "    cfg.checkpoint_path = CHECKPOINT_PATH\n",
    "    cfg.data_path = dataset_path\n",
    "    cfg.enable_color_aug_eval = max_color_augments > 0\n",
    "    cfg.max_color_augments_eval = max_color_augments\n",
    "\n",
    "    # 3. Build/Rebuild Model & Data\n",
    "    # We rebuild the dataloader every time to handle the different color augmentation settings\n",
    "    print(\"Building model and dataloader for config...\")\n",
    "\n",
    "    # Load checkpoint explicitly to pass to build function\n",
    "    checkpoint = torch.load(\n",
    "        cfg.checkpoint_path, map_location=device, weights_only=False\n",
    "    )\n",
    "\n",
    "    # Check if model exists in global scope to reuse weights, else create\n",
    "    global model\n",
    "    if \"model\" in globals() and model is not None:\n",
    "        model.load_state_dict(\n",
    "            checkpoint[\"model_state\"] if \"model_state\" in checkpoint else checkpoint,\n",
    "            strict=False,\n",
    "        )\n",
    "        model.eval()\n",
    "        # Rebuild only dataset/loader\n",
    "        _, dataset, dataloader, device, _ = train.build_model_and_data(\n",
    "            cfg, checkpoint=checkpoint\n",
    "        )\n",
    "    else:\n",
    "        model, dataset, dataloader, device, _ = train.build_model_and_data(cfg)\n",
    "\n",
    "    # 4. Run Inference (Logic from old Cell 3)\n",
    "    def log_eval(msg):\n",
    "        print(msg)\n",
    "        with open(eval_log_path, \"a\") as f:\n",
    "            f.write(msg + \"\\n\")\n",
    "\n",
    "    color_mappings_eval = None\n",
    "    color_apply_fn = None\n",
    "    if cfg.enable_color_aug_eval and cfg.max_color_augments_eval > 0:\n",
    "        color_seed = cfg.color_aug_seed or cfg.seed\n",
    "        color_mappings_eval = utils.generate_color_mapping_tensors(\n",
    "            cfg.max_color_augments_eval, color_seed\n",
    "        )\n",
    "        color_apply_fn = lambda split: True\n",
    "\n",
    "    evaluation = inference.evaluate_model_on_dataset(\n",
    "        model=model,\n",
    "        dataset=dataset,\n",
    "        device=device,\n",
    "        batch_size=EVAL_BATCH_SIZE,\n",
    "        log_prompts=args[\"log_inference_prompt\"],\n",
    "        splits=SPLITS,\n",
    "        color_mappings=color_mappings_eval,\n",
    "        color_apply_fn=color_apply_fn,\n",
    "        task_ids=EVAL_TASK_IDS,\n",
    "        include_targets=SOLUTIONS_PRESENT,\n",
    "    )\n",
    "\n",
    "    # Redirect stdout for AAIVR logging\n",
    "    if hasattr(sys.stdout, \"log\"):\n",
    "        sys.stdout = sys.stdout.terminal  # Reset if needed\n",
    "    sys.stdout = TeeLogger(str(aaivr_log_path))\n",
    "\n",
    "    try:\n",
    "        test_results = evaluation.get(\"test\", {}).get(\"results\", [])\n",
    "        dataset_has_dihedral_augments = \"dihedral_both\" in str(cfg.data_path)\n",
    "\n",
    "        aaivr_results = []\n",
    "        if test_results:\n",
    "            aaivr_results = utils.run_aaivr_on_results(\n",
    "                test_results,\n",
    "                is_dihedral_augmented=dataset_has_dihedral_augments,\n",
    "                color_aug_seed=cfg.color_aug_seed,\n",
    "                max_color_augments=cfg.max_color_augments_eval,\n",
    "            )\n",
    "\n",
    "            # # Print Stats (will go to console + aaivr.txt)\n",
    "            # utils.summarize_aaivr_pass_at_k(aaivr_results)\n",
    "            # if aaivr_results:\n",
    "            #     tasks_map = {}\n",
    "            #     for res in aaivr_results:\n",
    "            #         if res.task_id not in tasks_map:\n",
    "            #             tasks_map[res.task_id] = []\n",
    "            #         tasks_map[res.task_id].append(res)\n",
    "\n",
    "            #     arc_score = 0.0\n",
    "            #     total_tasks = len(tasks_map)\n",
    "\n",
    "            #     for t_id, pairs in tasks_map.items():\n",
    "            #         n_pairs = len(pairs)\n",
    "            #         if n_pairs > 0:\n",
    "            #             n_solved = sum(1 for p in pairs if p.pass_at_k)\n",
    "            #             arc_score += (n_solved / n_pairs)\n",
    "\n",
    "            #     max_score = total_tasks\n",
    "            #     pct = (arc_score / max_score * 100) if max_score > 0 else 0.0\n",
    "            #     print(f\"Official ARC style scoring: {arc_score:.2f}/{max_score} ({pct:.2f}%)\")\n",
    "        else:\n",
    "            print(\"No test results for AAIVR.\")\n",
    "\n",
    "    finally:\n",
    "        # Always restore stdout\n",
    "        if hasattr(sys.stdout, \"terminal\"):\n",
    "            sys.stdout.close()\n",
    "            sys.stdout = sys.stdout.terminal\n",
    "\n",
    "    # 6. Generate Submission (Logic from old Cell 5)\n",
    "    print(f\"Generating submission.json for {run_name}...\")\n",
    "    submission_data = {}\n",
    "    temp_grouping = {}\n",
    "\n",
    "    if aaivr_results:\n",
    "        for item in aaivr_results:\n",
    "            t_id = item.task_id\n",
    "            p_idx = item.original_pair_index\n",
    "            if t_id not in temp_grouping:\n",
    "                temp_grouping[t_id] = {}\n",
    "\n",
    "            top_grids = item.selected_outputs[:2]\n",
    "            if not top_grids:\n",
    "                top_grids = [[[0]]]  # Fallback\n",
    "\n",
    "            pair_dict = {\n",
    "                \"attempt_1\": top_grids[0],\n",
    "                \"attempt_2\": top_grids[1] if len(top_grids) > 1 else top_grids[0],\n",
    "            }\n",
    "            temp_grouping[t_id][p_idx] = pair_dict\n",
    "\n",
    "        for t_id, pairs_map in temp_grouping.items():\n",
    "            sorted_indices = sorted(pairs_map.keys())\n",
    "            submission_data[t_id] = [pairs_map[idx] for idx in sorted_indices]\n",
    "\n",
    "    with open(submission_path, \"w\") as f:\n",
    "        json.dump(submission_data, f)\n",
    "\n",
    "    print(f\"Finished {run_name}. Submission saved to {submission_path}\")\n",
    "\n",
    "\n",
    "# --- Execute the Loop (Modified with Timing) ---\n",
    "timing_path = Path(\"runs/timing.txt\")\n",
    "\n",
    "for name, aug_count, d_path in EVAL_CONFIGS:  # <--- Unpack 3 items\n",
    "    t_start = perf_counter()\n",
    "\n",
    "    run_evaluation_pipeline(name, aug_count, d_path, device)\n",
    "\n",
    "    t_duration = perf_counter() - t_start\n",
    "    print(f\"Run {name} took {t_duration:.2f}s\")\n",
    "\n",
    "    with open(timing_path, \"a\") as f:\n",
    "        f.write(f\"Evaluation {name}: {t_duration:.4f} s\\n\")\n",
    "\n",
    "print(\"\\nAll evaluation runs completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cab738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare mode requires the solutions file; submission mode does not\n",
    "# visualisation\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "EVAL_SUB_FOLDER = EVAL_CONFIGS[0][0]\n",
    "VIS_MODE = \"submission\"  # \"!\" = compare vs solutions, \"submission\" = attempts-only\n",
    "\n",
    "submission_file = Path(f\"runs/{EVAL_SUB_FOLDER}/submission.json\")\n",
    "# solutions_file = Path(\"assets/solutions.json\")\n",
    "\n",
    "if not submission_file.exists():\n",
    "    print(f\"Error: Could not find submission file: {submission_file}\")\n",
    "elif VIS_MODE == \"!\":\n",
    "    if not solutions_file.exists():\n",
    "        print(\n",
    "            f\"Error: Could not find solutions file for compare mode:\\n{solutions_file}\"\n",
    "        )\n",
    "    else:\n",
    "        # Load Data\n",
    "        with open(submission_file, \"r\") as f:\n",
    "            subs = json.load(f)\n",
    "        with open(solutions_file, \"r\") as f:\n",
    "            sols = json.load(f)\n",
    "\n",
    "        print(f\"Visualizing comparison for {len(subs)} tasks...\")\n",
    "\n",
    "        for task_id, attempts_list in subs.items():\n",
    "            # Get Ground Truth (list of grids)\n",
    "            if task_id not in sols:\n",
    "                print(f\"Warning: Task {task_id} not found in solutions.json\")\n",
    "                continue\n",
    "\n",
    "            gt_grids = sols[task_id]\n",
    "            print(gt_grids)\n",
    "            for i, attempts in enumerate(attempts_list):\n",
    "                if i >= len(gt_grids):\n",
    "                    break\n",
    "\n",
    "                # 1. Retrieve Grids\n",
    "                gt = gt_grids[i]\n",
    "                att1 = attempts.get(\"attempt_1\")\n",
    "                att2 = attempts.get(\"attempt_2\")\n",
    "\n",
    "                # 2. Check Correctness\n",
    "                pass1 = (att1 == gt) if att1 is not None else False\n",
    "                pass2 = (att2 == gt) if att2 is not None else False\n",
    "\n",
    "                if pass1 and pass2:\n",
    "                    status = \"Pass - both\"\n",
    "                elif pass1:\n",
    "                    status = \"Pass - 1\"\n",
    "                elif pass2:\n",
    "                    status = \"Pass - 2\"\n",
    "                else:\n",
    "                    status = \"Fail\"\n",
    "\n",
    "                # 3. Visualize\n",
    "                # Construct list: [Ground Truth, Attempt 1, Attempt 2]\n",
    "                grids_to_plot = [gt]\n",
    "                if att1 is not None:\n",
    "                    grids_to_plot.append(att1)\n",
    "                if att2 is not None:\n",
    "                    grids_to_plot.append(att2)\n",
    "\n",
    "                header = f\"Task: {task_id} | Pair: {i} | Status: {status}\"\n",
    "                print(f\"Plotting {header}\")\n",
    "\n",
    "                # utils.plot_grids handles the matplotlib figure creation\n",
    "                try:\n",
    "                    utils.plot_grids(grids_to_plot, title=header)\n",
    "                except Exception as e:\n",
    "                    print(f\"Skipping plot for {task_id} due to error: {e}\")\n",
    "else:\n",
    "    # Submission-only visualization (attempts without ground truth)\n",
    "    with open(submission_file, \"r\") as f:\n",
    "        subs = json.load(f)\n",
    "\n",
    "    print(f\"Visualizing submissions for {len(subs)} tasks (no solutions)...\")\n",
    "\n",
    "    for task_id, attempts_list in subs.items():\n",
    "        for i, attempts in enumerate(attempts_list):\n",
    "            att1 = attempts.get(\"attempt_1\")\n",
    "            att2 = attempts.get(\"attempt_2\")\n",
    "\n",
    "            grids_to_plot = []\n",
    "            if att1 is not None:\n",
    "                grids_to_plot.append(att1)\n",
    "            if att2 is not None:\n",
    "                grids_to_plot.append(att2)\n",
    "\n",
    "            if not grids_to_plot:\n",
    "                print(f\"Skipping {task_id} pair {i} (no attempts)\")\n",
    "                continue\n",
    "\n",
    "            header = f\"Task: {task_id} | Pair: {i} | Status: submission-only\"\n",
    "            print(f\"Plotting {header}\")\n",
    "\n",
    "            try:\n",
    "                utils.plot_grids(grids_to_plot, title=header)\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping plot for {task_id} due to error: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
