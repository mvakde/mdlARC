{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f99df6f",
   "metadata": {},
   "source": [
    "# Sanitized reproduction (no Kaggle account)\n",
    "This notebook is a clean, self‑contained reproduction that **does not require Kaggle credentials**. It downloads the official ARC datasets from GitHub, constructs a training/eval dataset that excludes eval solutions, trains the model from scratch, and runs inference on eval inputs only.\n",
    "\n",
    "## Why this proves there is no leakage\n",
    "- The dataset is built from ARC‑1 training pairs (inputs + outputs), ARC‑1 eval **inputs only**, and optional ConceptARC. Eval solutions are never constructed\n",
    "- After building `assets/challenges_dihedral_both.json`, every file (other than core model files) is deleted, so there’s nowhere for solutions to hide.\n",
    "- Training starts from scratch (`checkpoint_path=None`) and uses only the cleaned dataset file.\n",
    "- Inference runs with `SOLUTIONS_PRESENT=False` and produces `submission.json` without reading any solutions.\n",
    "- If you want to be extra strict, delete the final visualization cell and score the submission yourself.\n",
    "\n",
    "FYI:\n",
    "- The core files do not make any calls to the internet, or use extra libraries (written in pure pytorch), and there's no loading of checkpoints\n",
    "\n",
    "## What to change\n",
    "- Pick a runconfig (with or without ConceptARC) in the config cell below.\n",
    "- Reduce `EVAL_BATCH_SIZE` if your GPU is smaller than A100.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbcfb30",
   "metadata": {},
   "source": [
    "Steps:\n",
    "- Upload this on colab/modal\n",
    "    - No need to mount your google drive or modal volume\n",
    "- Decide which experiment you want to reproduce (list below) and modify config accordingly\n",
    "- Choose A100\n",
    "- hit run all\n",
    "\n",
    "The original run used an extra dataset called conceptARC. This dataset is clean. But to reduce the burden of verification, I add a config where this dataset is not used (performance reduces a bit)\n",
    "\n",
    "Configs\n",
    "- Run with ConceptARC for 11 epochs (10 color augments) - about 8-9%\n",
    "- Run with ConceptARC for 101 epochs (100 color augments) - 25-28%\n",
    "- Run without ConceptARC for 11 epochs (10 color augments) - about 7%\n",
    "- Run without ConceptARC for 101 epochs (100 color augments) - about 23%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faac0923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose runconfig\n",
    "runconfig = [\"concept\", 11]  # expect 8-9%\n",
    "# runconfig = [\"concept\", 21]  # expect 16%\n",
    "# runconfig = [\"concept\", 101] # expect 25-28%\n",
    "\n",
    "# This runconfig has less verification burden by removing exrtra dataset, but perf drops slightly\n",
    "# runconfig = [\"no_concept\", 11] # expect 7%\n",
    "# runconfig = [\"no_concept\", 11] # expect 14%\n",
    "# runconfig = [\"no_concept\", 101] # expect 23%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5607152d",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_folder = \"root\"\n",
    "# root_folder = \"content\" # for colab\n",
    "\n",
    "%cd /$root_folder/\n",
    "!git clone https://github.com/mvakde/mdlARC.git # `-b <branch_name> --single-branch` if branch\n",
    "%cd /$root_folder/mdlARC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f7973f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if runconfig[0] == \"concept\":\n",
    "    !python dataset_building_scripts/build_datasets.py --datasets arc1 conceptarc  --splits train eval --cleanup none\n",
    "else:\n",
    "    !python dataset_building_scripts/build_datasets.py --datasets arc1  --splits train eval --cleanup none\n",
    "!python dataset_building_scripts/augment_dataset_dihedral.py\n",
    "\n",
    "# Delete all files, especially solutions\n",
    "!find \"assets\" -mindepth 1 ! -path \"assets/challenges_dihedral_both.json\" -exec rm -rf -- {} +\n",
    "!rm -rf /$root_folder/mdlARC/run-script.ipynb\n",
    "!rm -rf /$root_folder/mdlARC/sanitised-env-run-script.ipynb\n",
    "!rm -rf /$root_folder/mdlARC/dataset_building_scripts\n",
    "!rm -rf /$root_folder/mdlARC/readme.md\n",
    "!rm -rf /$root_folder/mdlARC/img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1095cef3",
   "metadata": {},
   "source": [
    "## Data is now “solution‑free”\n",
    "At this point, the only dataset file we keep is `assets/challenges_dihedral_both.json`.\n",
    "- It contains **train inputs/outputs** and **eval inputs only**.\n",
    "- All other files and folders (including anything that could contain solutions) are deleted.\n",
    "\n",
    "You can inspect this file before continuing if you want to verify it manually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2c9571",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import argparse\n",
    "import importlib\n",
    "import sys\n",
    "\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "SRC_DIR = PROJECT_ROOT / \"src\"\n",
    "if SRC_DIR.exists() and str(SRC_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC_DIR))\n",
    "\n",
    "import utils, tinytransformer, train\n",
    "\n",
    "importlib.reload(utils)  # pick up code changes during iteration\n",
    "importlib.reload(tinytransformer)\n",
    "importlib.reload(train)\n",
    "\n",
    "args = {\n",
    "    # run config\n",
    "    \"num_workers\": 0,\n",
    "    \"device\": \"cuda\",  # 'cuda' | 'mps' | 'cpu'\n",
    "    \"do_validate\": False,\n",
    "    \"name\": \"arc1-cleanenv-30M-vvwide-bs32-101ep-100color-ccdb-18dec0430\",  # download file name\n",
    "    \"GPU\": \"A100-noaugreg\",  # just for logging purposes\n",
    "    # paths - must pass as Path(\"<path_to_dir>\")\n",
    "    \"train_log_file\": Path(\"runs/training_log.txt\"),\n",
    "    \"save_path\": Path(\"runs/tiny.pt\"),\n",
    "    \"checkpoint_path\": None,  # Path(\"runs/tiny.pt\"),  # or None to start from scratch\n",
    "    \"data_path\": Path(\"assets/challenges_dihedral_both.json\"),\n",
    "    # hyperparameters\n",
    "    \"epochs\": runconfig[1],\n",
    "    \"batch_size\": 32,\n",
    "    \"val_batch_size\": 300,\n",
    "    \"enable_color_aug_train\": True,\n",
    "    \"max_color_augments_train\": (runconfig[1] - 1),\n",
    "    \"color_aug_seed\": 42,\n",
    "    \"lr\": 3e-4,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"grad_clip\": 1.0,\n",
    "    \"dropout\": 0.1,\n",
    "    \"seed\": 42,\n",
    "    # Model Architecture\n",
    "    \"d_model\": 768,  # 128, 256, 512, 768 | 128, 384, 640\n",
    "    \"n_heads\": 12,  # 4, 8, 8/16, 12 | 4, 12, 10\n",
    "    \"d_ff\": 3072,  # 512, 1024, 2048, 3072 | 512, 1536, 2560\n",
    "    \"n_layers\": 4,  # 4, 6, 16, 16 | 24, 28, 24\n",
    "    # Visibility toggles\n",
    "    \"log_train_strings\": False,\n",
    "    \"log_train_limit\": 10,\n",
    "    \"log_inference_prompt\": False,\n",
    "    \"inference_temperature\": None,\n",
    "    \"inference_top_k\": None,\n",
    "}\n",
    "cfg = argparse.Namespace(**args)\n",
    "\n",
    "runs_dir = Path(\"runs\")\n",
    "runs_dir.mkdir(parents=True, exist_ok=True)\n",
    "with (runs_dir / \"config.txt\").open(\"w\") as f:\n",
    "    for k, v in args.items():\n",
    "        f.write(f\"{k}: {v}\\n\")\n",
    "\n",
    "model, dataset, dataloader, device, data_path = train.build_model_and_data(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0088a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training only\n",
    "\n",
    "from time import perf_counter\n",
    "\n",
    "t_start = perf_counter()\n",
    "\n",
    "# ---\n",
    "# direct\n",
    "train.train_model(\n",
    "    cfg,\n",
    "    model=model,\n",
    "    dataloader=dataloader,\n",
    "    dataset=dataset,\n",
    "    device=device,\n",
    "    data_path=data_path,\n",
    ")\n",
    "\n",
    "\n",
    "# # periodic checkpointing\n",
    "# cfg.save_path = Path(f\"runs/tiny-{cfg.epochs}.pt\")\n",
    "# for i in range(3):\n",
    "#   if i != 0:\n",
    "#     cfg.checkpoint_path = cfg.save_path\n",
    "#     cfg.save_path = Path(f\"runs/tiny-{cfg.epochs*(i+1)}.pt\")\n",
    "#   train.train_model(cfg, model=model, dataloader=dataloader, dataset=dataset, device=device, data_path=data_path)\n",
    "# ---\n",
    "\n",
    "t_duration = perf_counter() - t_start\n",
    "print(f\"Training took {t_duration:.2f}s\")\n",
    "\n",
    "with open(Path(\"runs/timing.txt\"), \"w\") as f:\n",
    "    f.write(f\"Training: {t_duration:.4f} s\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbdab1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning up memory to run inference\n",
    "utils.cleanup_memory(globals())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcd927d",
   "metadata": {},
   "source": [
    "## Submission is generated without solutions\n",
    "The next cells run inference and generate `runs/<run_name>/submission.json` with **no access to eval solutions**.  \n",
    "Everything stays solution‑free unless you choose to add your own scoring later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27492f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import importlib\n",
    "import evaluations\n",
    "import utils\n",
    "\n",
    "importlib.reload(evaluations)\n",
    "importlib.reload(utils)\n",
    "\n",
    "PATH_BOTH = Path(\"assets/challenges_dihedral_both.json\")\n",
    "\n",
    "EVAL_CONFIGS = [(\"eval\", runconfig[1] - 1, PATH_BOTH)]\n",
    "\n",
    "EVAL_BATCH_SIZE = 1300\n",
    "SPLITS = [\"test\"]\n",
    "CHECKPOINT_PATH = Path(\"runs/tiny.pt\")\n",
    "SOLUTIONS_PRESENT = False\n",
    "EVAL_TASK_IDS = None  # Set to None to evaluate full dataset, or [\"00576224\", ...] for specific tasks\n",
    "LOG_CORRECT_GRIDS = False  # Print the actual grid, IDs, and augmentation indices for fully correct grids\n",
    "\n",
    "evaluations.run_evaluation_configs(\n",
    "    cfg,\n",
    "    EVAL_CONFIGS,\n",
    "    eval_batch_size=EVAL_BATCH_SIZE,\n",
    "    splits=SPLITS,\n",
    "    checkpoint_path=CHECKPOINT_PATH,\n",
    "    include_targets=SOLUTIONS_PRESENT,\n",
    "    task_ids=EVAL_TASK_IDS,\n",
    "    log_correct_grids=LOG_CORRECT_GRIDS,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cab738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation\n",
    "EVAL_SUB_FOLDER = EVAL_CONFIGS[0][0]\n",
    "VIS_MODE = \"submission\"  # \"!\" = compare vs solutions, \"submission\" = attempts-only\n",
    "utils.visualize_eval_submissions(EVAL_SUB_FOLDER, mode=VIS_MODE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2581eee9",
   "metadata": {},
   "source": [
    "## Stop here for a strict no‑solutions run\n",
    "At this point, the model has already produced `runs/<run_name>/submission.json` without any access to solutions. **This means the run is clean!**.\n",
    "\n",
    "**The next 2 cells score the submission and visualise differences with ground truth. This requires downloading the solutions**\n",
    "\n",
    "If you want a strict no‑solutions audit, stop here and score the submission yourself manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c89ba6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /$root_folder/mdlARC\n",
    "!rm -rf /$root_folder/mdlARC/assets/\n",
    "!python dataset_building_scripts/build_datasets.py --datasets arc1  --splits eval --cleanup none --with-solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b9638e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import utils\n",
    "\n",
    "SOLUTIONS_FILE = Path(\"assets/solutions.json\")\n",
    "SUBMISSION_FILE = Path(f\"runs/{EVAL_SUB_FOLDER}/submission.json\")\n",
    "\n",
    "utils.score_arc_submission(SOLUTIONS_FILE, SUBMISSION_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7193ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise and compare the differences between the ground truth solutions and the correct answers\n",
    "EVAL_SUB_FOLDER = EVAL_CONFIGS[0][0]\n",
    "utils.visualize_eval_submissions(\n",
    "    EVAL_SUB_FOLDER,\n",
    "    submission_base=\"mdlARC/runs\",\n",
    "    solutions_file=\"asolutions.json\",\n",
    "    mode=\"compare\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
