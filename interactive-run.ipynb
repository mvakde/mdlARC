{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93163d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_folder = \"content\" # DEFINE THE PATH TO YOUR WORKSPACE HERE\n",
    "\n",
    "%cd /$root_folder/\n",
    "!git clone https://github.com/mvakde/mdlARC.git # `-b <branch_name> --single-branch` if branch\n",
    "%cd /$root_folder/mdlARC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44cfa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python dataset_building_scripts/python download_and_group.py    \n",
    "!python dataset_building_scripts/build_datasets.py arc1 --add-conceptarc --with-filtered\n",
    "\n",
    "!rm -r /$root_folder/mdlARC/assets_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac8c54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import argparse\n",
    "import importlib\n",
    "import sys\n",
    "\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "SRC_DIR = PROJECT_ROOT / \"src\"\n",
    "if SRC_DIR.exists() and str(SRC_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC_DIR))\n",
    "\n",
    "import utils, tinytransformer, train, build\n",
    "importlib.reload(utils)  # pick up code changes during iteration\n",
    "importlib.reload(tinytransformer)\n",
    "importlib.reload(train)\n",
    "importlib.reload(build)\n",
    "\n",
    "args = {\n",
    "    # run config\n",
    "    \"name\": \"arc1-refactor\",  # download file name\n",
    "    \"GPU\": \"RTX_5090\",  # just for logging purposes\n",
    "    # paths - must pass as Path(\"<path_to_dir>\")\n",
    "    \"data_path\": Path(\"assets/challenges.json\"),\n",
    "    \"train_log_file\": Path(\"runs/training_log.txt\"),\n",
    "    \"save_path\": Path(\"runs/tiny.pt\"),\n",
    "    \"checkpoint_path\": None, #Path(\"runs/tiny.pt\"),  # or None to start from scratch\n",
    "    \"checkpoint_epochs\": [],  # int N for every N epochs, or list [5, 10, 25]\n",
    "    \n",
    "    \n",
    "    # hyperparameters\n",
    "    \"epochs\": 240,\n",
    "    \"batch_size\": 32,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"do_validate\": False,\n",
    "    \"val_batch_size\": 140,\n",
    "\n",
    "    \"enable_aug\": True,\n",
    "    \"max_augments\": 80,\n",
    "    \"enable_color_aug\": True,\n",
    "    \"color_apply_to_test\": True,\n",
    "    \"enable_dihedral_aug\": True,\n",
    "    \"dihedral_apply_to_test\": True,\n",
    "\n",
    "\n",
    "    \"optimizer\": \"normuon\",  # \"adamw\" | \"normuon\"\n",
    "    \"normuon_lr\": 1.66e-3,\n",
    "    \"normuon_momentum\": 0.95,\n",
    "    \"normuon_beta2\": 0.95,\n",
    "    \"lr\": 3e-4, #adamw lr\n",
    "\n",
    "    \"warmup_pct\": 0.02,\n",
    "    \"wsd_decay_start_pct\": 0.8,  # 1.0 = no decay (start at last epoch)\n",
    "    \"lr_floor\": 0.0,\n",
    "\n",
    "    \"weight_decay\": 0.1,\n",
    "    \"attention_weight_decay\": 0.01,\n",
    "    \"token_embedding_weight_decay\": 0.01,\n",
    "    \"task_embedding_weight_decay\": 0.01,\n",
    "\n",
    "    \"grad_clip\": 1.0,\n",
    "    \"dropout\": 0.1,\n",
    "    \"seed\": 42,\n",
    "\n",
    "    # Model Architecture\n",
    "    \"d_model\": 768,  # 128, 256, 512, 768 | 128, 384, 640\n",
    "    \"n_heads\": 12,  # 4, 8, 8/16, 12 | 4, 12, 10\n",
    "    \"d_ff\": 3072,  # 512, 1024, 2048, 3072 | 512, 1536, 2560\n",
    "    \"n_layers\": 8,  # 4, 6, 16, 16 | 24, 28, 24\n",
    "\n",
    "    \"inference_temperature\": None,\n",
    "    \"inference_top_k\": None,\n",
    "}\n",
    "cfg = argparse.Namespace(**args)\n",
    "\n",
    "runs_dir = Path(\"runs\")\n",
    "runs_dir.mkdir(parents=True, exist_ok=True)\n",
    "with (runs_dir / \"config.txt\").open(\"w\") as f:\n",
    "    for k, v in args.items():\n",
    "        f.write(f\"{k}: {v}\\n\")\n",
    "\n",
    "model, dataset, dataloader, device, data_path = build.build_model_and_data(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfa7324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training only\n",
    "from time import perf_counter\n",
    "\n",
    "t_start = perf_counter()\n",
    "train.train_model(cfg,model=model,dataloader=dataloader,dataset=dataset,device=device,data_path=data_path)\n",
    "t_duration = perf_counter() - t_start\n",
    "\n",
    "print(f\"Training took {t_duration:.2f}s\")\n",
    "with open(Path(\"runs/timing.txt\"), \"w\") as f:\n",
    "    f.write(f\"Training: {t_duration:.4f} s\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c84a83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.cleanup_memory(globals()) # cleaning up memory to run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283309b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pickle\n",
    "import importlib\n",
    "import evaluate\n",
    "import utils\n",
    "importlib.reload(evaluate)\n",
    "importlib.reload(utils)\n",
    "\n",
    "EVAL_NAME = \"eval_1\"\n",
    "eval_result = evaluate.run_evaluation(\n",
    "    cfg,\n",
    "    run_name=EVAL_NAME,\n",
    "    max_augments=cfg.max_augments, # or choose another int\n",
    "    data_path=cfg.data_path, # path to dataset\n",
    "    checkpoint_path=cfg.save_path, # or manually define like Path(\"runs/tiny.pt\"),\n",
    "    batch_size=100, # DEFINE THIS BASED ON GPU RAM\n",
    "    splits=[\"test\"], # can also choose [\"train\"] or both\n",
    "    task_ids=None, # Specific task IDs to evaluate (None = all tasks, or [\"00576224\", ...] for specific tasks)\n",
    ")\n",
    "\n",
    "Path(\"runs/eval_results.pkl\").write_bytes(pickle.dumps([eval_result]))  # Wrap in list for backward compat\n",
    "print(\"Saved runs/eval_results.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ec3c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scoring\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "if \"eval_result\" not in globals():\n",
    "    eval_results = pickle.loads(Path(\"runs/eval_results.pkl\").read_bytes())\n",
    "    EVAL_SUB_FOLDER = eval_results[0][0]\n",
    "elif \"EVAL_NAME\" in globals():\n",
    "    EVAL_SUB_FOLDER = EVAL_NAME\n",
    "else:\n",
    "    EVAL_SUB_FOLDER = eval_result[0]\n",
    "\n",
    "SOLUTIONS_FILE = Path(\"assets/solutions.json\")\n",
    "SUBMISSION_FILE = Path(f\"runs/{EVAL_SUB_FOLDER}/submission.json\")\n",
    "\n",
    "score = utils.score_arc_submission(SOLUTIONS_FILE, SUBMISSION_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4ce6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "if \"eval_result\" not in globals():\n",
    "    eval_results = pickle.loads(Path(\"runs/eval_results.pkl\").read_bytes())\n",
    "    EVAL_SUB_FOLDER = eval_results[0][0]\n",
    "elif \"EVAL_NAME\" in globals():\n",
    "    EVAL_SUB_FOLDER = EVAL_NAME\n",
    "else:\n",
    "    EVAL_SUB_FOLDER = eval_result[0]\n",
    "\n",
    "VIS_MODE = \"!\"  # \"!\" = compare vs solutions, \"submission\" = attempts-only\n",
    "utils.visualize_submissions(Path(\"runs\") / EVAL_SUB_FOLDER / \"submission.json\", solutions_file=\"assets/solutions.json\", mode=VIS_MODE)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
