{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93163d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# root_folder, mount_folder = \"root\", \"mnt/mithil-arc\" # for modal - REPLACE /mithil-arc WITH YOUR VOLUME NAME\n",
    "root_folder, mount_folder = \"content\", \"content/drive/MyDrive\"  # for colab\n",
    "\n",
    "%cd /$root_folder/\n",
    "!git clone https://github.com/mvakde/mdlARC.git # `-b <branch_name> --single-branch` if branch\n",
    "%cd /$root_folder/mdlARC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44cfa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python dataset_building_scripts/build_datasets.py --datasets arc1 conceptarc  --splits train eval --with-solutions --cleanup none\n",
    "\n",
    "!rm -rf /$root_folder/mdlARC/interactive-run.ipynb\n",
    "!rm -rf /$root_folder/mdlARC/clean-env-run.ipynb\n",
    "!rm -rf /$root_folder/mdlARC/max-clean-env-run.ipynb\n",
    "!rm -rf /$root_folder/mdlARC/dataset_building_scripts\n",
    "!rm -rf /$root_folder/mdlARC/readme.md\n",
    "!rm -rf /$root_folder/mdlARC/img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac8c54a",
   "metadata": {},
   "outputs": [],
   "source": "from pathlib import Path\nimport argparse\nimport importlib\nimport sys\n\nPROJECT_ROOT = Path.cwd()\nSRC_DIR = PROJECT_ROOT / \"src\"\nif SRC_DIR.exists() and str(SRC_DIR) not in sys.path:\n    sys.path.insert(0, str(SRC_DIR))\n\nimport utils, tinytransformer, train, build\nimportlib.reload(utils)  # pick up code changes during iteration\nimportlib.reload(tinytransformer)\nimportlib.reload(train)\nimportlib.reload(build)\n\nargs = {\n    # run config\n    \"name\": \"arc1-refactor\",  # download file name\n    \"GPU\": \"A100\",  # just for logging purposes\n    # paths - must pass as Path(\"<path_to_dir>\")\n    \"data_path\": Path(\"assets/challenges.json\"),\n    \"train_log_file\": Path(\"runs/training_log.txt\"),\n    \"save_path\": Path(\"runs/tiny.pt\"),\n    \"checkpoint_path\": None, #Path(\"runs/tiny.pt\"),  # or None to start from scratch\n    \"checkpoint_epochs\": [300,400,500,600,605],  # int N for every N epochs, or list [5, 10, 25]\n    \n    \n    # hyperparameters\n    \"epochs\": 20,\n    \"batch_size\": 32,\n    \"gradient_accumulation_steps\": 1,\n    \"do_validate\": True,\n    \"val_batch_size\": 140,\n\n    \"enable_aug\": True,\n    \"max_augments\": 10,\n    \"enable_color_aug\": True,\n    \"color_apply_to_test\": True,\n    \"enable_dihedral_aug\": True,\n    \"dihedral_apply_to_test\": True,\n\n\n    \"optimizer\": \"normuon\",  # \"adamw\" | \"normuon\"\n    \"normuon_lr\": 1.66e-3,\n    \"normuon_momentum\": 0.95,\n    \"normuon_beta2\": 0.95,\n    \"lr\": 3e-4, #adamw lr\n\n    \"warmup_pct\": 0.02,\n    \"wsd_decay_start_pct\": 0.8,  # 1.0 = no decay (start at last epoch)\n    \"lr_floor\": 0.0,\n\n    \"weight_decay\": 0.1,\n    \"attention_weight_decay\": 0.01,\n    \"token_embedding_weight_decay\": 0.01,\n    \"task_embedding_weight_decay\": 0.01,\n\n    \"grad_clip\": 1.0,\n    \"dropout\": 0.1,\n    \"seed\": 42,\n\n    # Model Architecture\n    \"d_model\": 768,  # 128, 256, 512, 768 | 128, 384, 640\n    \"n_heads\": 12,  # 4, 8, 8/16, 12 | 4, 12, 10\n    \"d_ff\": 3072,  # 512, 1024, 2048, 3072 | 512, 1536, 2560\n    \"n_layers\": 4,  # 4, 6, 16, 16 | 24, 28, 24\n\n    \"inference_temperature\": None,\n    \"inference_top_k\": None,\n}\ncfg = argparse.Namespace(**args)\n\nruns_dir = Path(\"runs\")\nruns_dir.mkdir(parents=True, exist_ok=True)\nwith (runs_dir / \"config.txt\").open(\"w\") as f:\n    for k, v in args.items():\n        f.write(f\"{k}: {v}\\n\")\n\nmodel, dataset, dataloader, device, data_path = build.build_model_and_data(cfg)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfa7324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training only\n",
    "from time import perf_counter\n",
    "\n",
    "t_start = perf_counter()\n",
    "train.train_model(cfg,model=model,dataloader=dataloader,dataset=dataset,device=device,data_path=data_path)\n",
    "t_duration = perf_counter() - t_start\n",
    "\n",
    "print(f\"Training took {t_duration:.2f}s\")\n",
    "with open(Path(\"runs/timing.txt\"), \"w\") as f:\n",
    "    f.write(f\"Training: {t_duration:.4f} s\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c84a83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.cleanup_memory(globals()) # cleaning up memory to run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283309b0",
   "metadata": {},
   "outputs": [],
   "source": "from pathlib import Path\nimport pickle\nimport importlib\nimport evaluate\nimport utils\nimportlib.reload(evaluate)\nimportlib.reload(utils)\n\n# === Evaluation Configuration ===\n# Run name: creates output folder at runs/<EVAL_NAME>/\nEVAL_NAME = \"eval_100aug\"\n\n# Max augments: number of augmented variants per example for test-time augmentation (TTA).\n# Higher = more diverse predictions for AAIVR voting, but slower inference.\nEVAL_MAX_AUGMENTS = 100\n\n# Path to dataset (challenges.json)\nEVAL_DATA_PATH = Path(\"assets/challenges.json\")\n\n# Path to model checkpoint\nEVAL_CHECKPOINT_PATH = Path(\"runs/tiny.pt\")\n\n# Batch size for inference\nEVAL_BATCH_SIZE = 1300\n\n# Splits to evaluate\nEVAL_SPLITS = [\"test\"]\n\n# Specific task IDs to evaluate (None = all tasks, or [\"00576224\", ...] for specific tasks)\nEVAL_TASK_IDS = None\n\n# === Run Evaluation ===\neval_result = evaluate.run_evaluation(\n    cfg,\n    run_name=EVAL_NAME,\n    max_augments=EVAL_MAX_AUGMENTS,\n    data_path=EVAL_DATA_PATH,\n    checkpoint_path=EVAL_CHECKPOINT_PATH,\n    batch_size=EVAL_BATCH_SIZE,\n    splits=EVAL_SPLITS,\n    task_ids=EVAL_TASK_IDS,\n)\n\nPath(\"runs/eval_results.pkl\").write_bytes(pickle.dumps([eval_result]))  # Wrap in list for backward compat\nprint(\"Saved runs/eval_results.pkl\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4ce6cd",
   "metadata": {},
   "outputs": [],
   "source": "# visualisation\nfrom pathlib import Path\nimport pickle\n\nif \"eval_result\" not in globals():\n    eval_results = pickle.loads(Path(\"runs/eval_results.pkl\").read_bytes())\n    EVAL_SUB_FOLDER = eval_results[0][0]\nelif \"EVAL_NAME\" in globals():\n    EVAL_SUB_FOLDER = EVAL_NAME\nelse:\n    EVAL_SUB_FOLDER = eval_result[0]\n\nVIS_MODE = \"!\"  # \"!\" = compare vs solutions, \"submission\" = attempts-only\nutils.visualize_submissions(Path(\"runs\") / EVAL_SUB_FOLDER / \"submission.json\", solutions_file=\"assets/solutions.json\", mode=VIS_MODE)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ec3c83",
   "metadata": {},
   "outputs": [],
   "source": "# scoring\nfrom pathlib import Path\nimport pickle\n\nif \"eval_result\" not in globals():\n    eval_results = pickle.loads(Path(\"runs/eval_results.pkl\").read_bytes())\n    EVAL_SUB_FOLDER = eval_results[0][0]\nelif \"EVAL_NAME\" in globals():\n    EVAL_SUB_FOLDER = EVAL_NAME\nelse:\n    EVAL_SUB_FOLDER = eval_result[0]\n\nSOLUTIONS_FILE = Path(\"assets/solutions.json\")\nSUBMISSION_FILE = Path(f\"runs/{EVAL_SUB_FOLDER}/submission.json\")\n\nscore = utils.score_arc_submission(SOLUTIONS_FILE, SUBMISSION_FILE)"
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}