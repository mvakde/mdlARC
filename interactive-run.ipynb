{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e6aae73",
   "metadata": {},
   "source": [
    "## 29% using a 29M transformer\n",
    "Steps to reproduce:  \n",
    "1. Upload this script to google colab or modal\n",
    "2. (optional) If you want to save checkpoints and results, mount your google drive (colab) / your volume (modal)\n",
    "3. Click run-all\n",
    "\n",
    "This script ensures there's no data contamination. \n",
    "This produces a `submission.json` file. The `submission.json` file\n",
    "\n",
    "\n",
    "Notes:\n",
    "1. The config in this notebook has been tuned for an 80GB A100  \n",
    "2. Actual results were obtained by running this exact file in 2 phases.  \n",
    "    - Training on a 40GB A100\n",
    "    - Take the final checkpoint, and run the inference on an 80GB A100\n",
    "\n",
    "This will work on smaller GPUs too, but will take longer to train  \n",
    "For very constrained environments, disable the \"do_validate\" flag. This avoids checking the validation loss every epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93163d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# root_folder, mount_folder = \"root\", \"mnt/mithil-arc\" # for modal - REPLACE /mithil-arc WITH YOUR VOLUME NAME\n",
    "root_folder, mount_folder = \"content\", \"content/drive/MyDrive\"  # for colab\n",
    "\n",
    "%cd /$root_folder/\n",
    "!git clone https://github.com/mvakde/mdlARC.git # `-b <branch_name> --single-branch` if branch\n",
    "%cd /$root_folder/mdlARC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44cfa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python dataset_building_scripts/build_datasets.py --datasets arc1 conceptarc  --splits train eval --with-solutions --cleanup none\n",
    "\n",
    "!rm -rf /$root_folder/mdlARC/interactive-run.ipynb\n",
    "!rm -rf /$root_folder/mdlARC/clean-env-run.ipynb\n",
    "!rm -rf /$root_folder/mdlARC/max-clean-env-run.ipynb\n",
    "!rm -rf /$root_folder/mdlARC/dataset_building_scripts\n",
    "!rm -rf /$root_folder/mdlARC/readme.md\n",
    "!rm -rf /$root_folder/mdlARC/img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac8c54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import argparse\n",
    "import importlib\n",
    "import sys\n",
    "\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "SRC_DIR = PROJECT_ROOT / \"src\"\n",
    "if SRC_DIR.exists() and str(SRC_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC_DIR))\n",
    "\n",
    "import utils, tinytransformer, train\n",
    "importlib.reload(utils)  # pick up code changes during iteration\n",
    "importlib.reload(tinytransformer)\n",
    "importlib.reload(train)\n",
    "\n",
    "args = {\n",
    "    # run config\n",
    "    \"name\": \"arc1-refactor\",  # download file name\n",
    "    \"GPU\": \"A100\",  # just for logging purposes\n",
    "    # paths - must pass as Path(\"<path_to_dir>\")\n",
    "    \"data_path\": Path(\"assets/challenges.json\"),\n",
    "    \"train_log_file\": Path(\"runs/training_log.txt\"),\n",
    "    \"save_path\": Path(\"runs/tiny.pt\"),\n",
    "    \"checkpoint_path\": None, #Path(\"runs/tiny.pt\"),  # or None to start from scratch\n",
    "    \"checkpoint_epochs\": [300,400,500,600,605],  # int N for every N epochs, or list [5, 10, 25]\n",
    "    \n",
    "    \n",
    "    # hyperparameters\n",
    "    \"epochs\": 20,\n",
    "    \"batch_size\": 32,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"do_validate\": True,\n",
    "    \"val_batch_size\": 140,\n",
    "\n",
    "    \"enable_aug\": True,\n",
    "    \"max_augments\": 10,\n",
    "    \"enable_color_aug\": True,\n",
    "    \"color_apply_to_test\": True,\n",
    "    \"enable_dihedral_aug\": True,\n",
    "    \"dihedral_apply_to_test\": True,\n",
    "\n",
    "\n",
    "    \"optimizer\": \"normuon\",  # \"adamw\" | \"normuon\"\n",
    "    \"normuon_lr\": 1.66e-3,\n",
    "    \"normuon_momentum\": 0.95,\n",
    "    \"normuon_beta2\": 0.95,\n",
    "    \"lr\": 3e-4, #adamw lr\n",
    "\n",
    "    \"warmup_pct\": 0.02,\n",
    "    \"wsd_decay_start_pct\": 0.8,  # 1.0 = no decay (start at last epoch)\n",
    "    \"lr_floor\": 0.0,\n",
    "\n",
    "    \"weight_decay\": 0.1,\n",
    "    \"attention_weight_decay\": 0.01,\n",
    "    \"token_embedding_weight_decay\": 0.01,\n",
    "    \"task_embedding_weight_decay\": 0.01,\n",
    "\n",
    "    \"grad_clip\": 1.0,\n",
    "    \"dropout\": 0.1,\n",
    "    \"seed\": 42,\n",
    "\n",
    "    # Model Architecture\n",
    "    \"d_model\": 768,  # 128, 256, 512, 768 | 128, 384, 640\n",
    "    \"n_heads\": 12,  # 4, 8, 8/16, 12 | 4, 12, 10\n",
    "    \"d_ff\": 3072,  # 512, 1024, 2048, 3072 | 512, 1536, 2560\n",
    "    \"n_layers\": 4,  # 4, 6, 16, 16 | 24, 28, 24\n",
    "\n",
    "    \"inference_temperature\": None,\n",
    "    \"inference_top_k\": None,\n",
    "}\n",
    "cfg = argparse.Namespace(**args)\n",
    "\n",
    "runs_dir = Path(\"runs\")\n",
    "runs_dir.mkdir(parents=True, exist_ok=True)\n",
    "with (runs_dir / \"config.txt\").open(\"w\") as f:\n",
    "    for k, v in args.items():\n",
    "        f.write(f\"{k}: {v}\\n\")\n",
    "\n",
    "model, dataset, dataloader, device, data_path = train.build_model_and_data(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfa7324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training only\n",
    "from time import perf_counter\n",
    "\n",
    "t_start = perf_counter()\n",
    "train.train_model(cfg,model=model,dataloader=dataloader,dataset=dataset,device=device,data_path=data_path)\n",
    "t_duration = perf_counter() - t_start\n",
    "\n",
    "print(f\"Training took {t_duration:.2f}s\")\n",
    "with open(Path(\"runs/timing.txt\"), \"w\") as f:\n",
    "    f.write(f\"Training: {t_duration:.4f} s\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c84a83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.cleanup_memory(globals()) # cleaning up memory to run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d36c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data immediately in case eval fails\n",
    "archive_state = utils.save_run_archive(cfg.name, root_folder, mount_folder, globals_dict=globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283309b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pickle\n",
    "import importlib\n",
    "import evaluations\n",
    "import utils\n",
    "importlib.reload(evaluations)\n",
    "importlib.reload(utils)\n",
    "\n",
    "PATH_BOTH = Path(\"assets/challenges.json\")\n",
    "# aug_count = max_augments (0 = no aug)\n",
    "\n",
    "EVAL_CONFIGS = [\n",
    "    # (\"eval_125color_both\", 125, PATH_BOTH),\n",
    "    (\"eval_100color_both\", 100, PATH_BOTH),\n",
    "    # (\"eval_10color_both\", 10, PATH_BOTH),\n",
    "    # (\"eval_0color_both\", 0, PATH_BOTH),\n",
    "    # (\"eval_0color_train\", 0, PATH_TRAIN)  # Uses TRAIN path (No Geom TTA on Test)\n",
    "]\n",
    "\n",
    "EVAL_BATCH_SIZE = 1300\n",
    "SPLITS = [\"test\"]\n",
    "CHECKPOINT_PATH = Path(\"runs/tiny.pt\")\n",
    "SOLUTIONS_PRESENT = False\n",
    "EVAL_TASK_IDS = None  # Set to None to evaluate full dataset, or [\"00576224\", ...] for specific tasks\n",
    "LOG_CORRECT_GRIDS = False  # Print the actual grid, IDs, and augmentation indices for fully correct grids\n",
    "\n",
    "eval_results = evaluations.run_evaluation_configs(\n",
    "    cfg,\n",
    "    EVAL_CONFIGS,\n",
    "    eval_batch_size=EVAL_BATCH_SIZE,\n",
    "    splits=SPLITS,\n",
    "    checkpoint_path=CHECKPOINT_PATH,\n",
    "    include_targets=SOLUTIONS_PRESENT,\n",
    "    task_ids=EVAL_TASK_IDS,\n",
    "    log_correct_grids=LOG_CORRECT_GRIDS,\n",
    ")\n",
    "\n",
    "Path(\"runs/eval_results.pkl\").write_bytes(pickle.dumps(eval_results))\n",
    "print(\"Saved runs/eval_results.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb77a5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# refresh Drive zip\n",
    "archive_state = utils.update_run_archive(cfg.name, root_folder, mount_folder, globals_dict=globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4ce6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "if \"eval_results\" not in globals():\n",
    "    eval_results = pickle.loads(Path(\"runs/eval_results.pkl\").read_bytes())\n",
    "\n",
    "if \"EVAL_CONFIGS\" in globals():\n",
    "    EVAL_SUB_FOLDER = EVAL_CONFIGS[0][0]\n",
    "else:\n",
    "    EVAL_SUB_FOLDER = eval_results[0][0]\n",
    "\n",
    "VIS_MODE = \"!\"  # \"!\" = compare vs solutions, \"submission\" = attempts-only\n",
    "utils.visualize_eval_submissions(EVAL_SUB_FOLDER, mode=VIS_MODE, solutions_file=\"assets/solutions.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ec3c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scoring\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "if \"eval_results\" not in globals():\n",
    "    eval_results = pickle.loads(Path(\"runs/eval_results.pkl\").read_bytes())\n",
    "\n",
    "EVAL_SUB_FOLDER = eval_results[0][0]\n",
    "SOLUTIONS_FILE = Path(\"assets/solutions.json\")\n",
    "SUBMISSION_FILE = Path(f\"runs/{EVAL_SUB_FOLDER}/submission.json\")\n",
    "\n",
    "score = utils.score_arc_submission(SOLUTIONS_FILE, SUBMISSION_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AAIVR flow visualization for one task/pair (augmented inputs + outputs)\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import aaivr\n",
    "import utils\n",
    "\n",
    "if \"eval_results\" not in globals():\n",
    "    eval_results = pickle.loads(Path(\"runs/eval_results.pkl\").read_bytes())\n",
    "\n",
    "if \"EVAL_CONFIGS\" not in globals():\n",
    "    raise ValueError(\"EVAL_CONFIGS not found; rerun eval cell to set configs.\")\n",
    "\n",
    "AAIVR_CONFIG_INDEX = 0  # which eval config to inspect\n",
    "AAIVR_TASK_ID = None  # set to a task id string to override task index\n",
    "AAIVR_TASK_INDEX = 0  # 0-based in evaluation pipeline order\n",
    "AAIVR_INPUT_INDEX = 0  # which test pair (base index before dihedral aug)\n",
    "\n",
    "eval_entry = eval_results[AAIVR_CONFIG_INDEX]\n",
    "eval_data = eval_entry[1]\n",
    "test_results = eval_data.get(\"test\", {}).get(\"results\", [])\n",
    "\n",
    "eval_config = EVAL_CONFIGS[AAIVR_CONFIG_INDEX]\n",
    "dataset_path = eval_config[2]\n",
    "dihedral_enabled = False\n",
    "color_mappings_by_task = None\n",
    "\n",
    "aug_ctx = eval_data.get(\"_aug\", {})\n",
    "if aug_ctx:\n",
    "    color_mappings_by_task = aug_ctx.get(\"color_mappings_by_split\", {}).get(\"test\")\n",
    "    dihedral_enabled = aug_ctx.get(\"dihedral_augmented_by_split\", {}).get(\"test\", False)\n",
    "\n",
    "aaivr.visualize_aaivr_flow(\n",
    "    test_results,\n",
    "    dataset_path=dataset_path,\n",
    "    input_index=AAIVR_INPUT_INDEX,\n",
    "    task_id=AAIVR_TASK_ID,\n",
    "    task_index=AAIVR_TASK_INDEX,\n",
    "    is_dihedral_augmented=dihedral_enabled,\n",
    "    color_mappings_by_task=color_mappings_by_task,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
