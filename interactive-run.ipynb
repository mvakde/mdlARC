{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93163d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# root_folder, mount_folder = \"root\", \"mnt/mithil-arc\" # for modal - REPLACE /mithil-arc WITH YOUR VOLUME NAME\n",
    "root_folder, mount_folder = \"content\", \"content/drive/MyDrive\"  # for colab\n",
    "\n",
    "%cd /$root_folder/\n",
    "!git clone https://github.com/mvakde/mdlARC.git # `-b <branch_name> --single-branch` if branch\n",
    "%cd /$root_folder/mdlARC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44cfa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python dataset_building_scripts/build_datasets.py --datasets arc1 conceptarc  --splits train eval --with-solutions --cleanup none\n",
    "\n",
    "!rm -rf /$root_folder/mdlARC/interactive-run.ipynb\n",
    "!rm -rf /$root_folder/mdlARC/clean-env-run.ipynb\n",
    "!rm -rf /$root_folder/mdlARC/max-clean-env-run.ipynb\n",
    "!rm -rf /$root_folder/mdlARC/dataset_building_scripts\n",
    "!rm -rf /$root_folder/mdlARC/readme.md\n",
    "!rm -rf /$root_folder/mdlARC/img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac8c54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import argparse\n",
    "import importlib\n",
    "import sys\n",
    "\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "SRC_DIR = PROJECT_ROOT / \"src\"\n",
    "if SRC_DIR.exists() and str(SRC_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC_DIR))\n",
    "\n",
    "import utils, tinytransformer, train\n",
    "importlib.reload(utils)  # pick up code changes during iteration\n",
    "importlib.reload(tinytransformer)\n",
    "importlib.reload(train)\n",
    "\n",
    "args = {\n",
    "    # run config\n",
    "    \"name\": \"arc1-refactor\",  # download file name\n",
    "    \"GPU\": \"A100\",  # just for logging purposes\n",
    "    # paths - must pass as Path(\"<path_to_dir>\")\n",
    "    \"data_path\": Path(\"assets/challenges.json\"),\n",
    "    \"train_log_file\": Path(\"runs/training_log.txt\"),\n",
    "    \"save_path\": Path(\"runs/tiny.pt\"),\n",
    "    \"checkpoint_path\": None, #Path(\"runs/tiny.pt\"),  # or None to start from scratch\n",
    "    \"checkpoint_epochs\": [300,400,500,600,605],  # int N for every N epochs, or list [5, 10, 25]\n",
    "    \n",
    "    \n",
    "    # hyperparameters\n",
    "    \"epochs\": 20,\n",
    "    \"batch_size\": 32,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"do_validate\": True,\n",
    "    \"val_batch_size\": 140,\n",
    "\n",
    "    \"enable_aug\": True,\n",
    "    \"max_augments\": 10,\n",
    "    \"enable_color_aug\": True,\n",
    "    \"color_apply_to_test\": True,\n",
    "    \"enable_dihedral_aug\": True,\n",
    "    \"dihedral_apply_to_test\": True,\n",
    "\n",
    "\n",
    "    \"optimizer\": \"normuon\",  # \"adamw\" | \"normuon\"\n",
    "    \"normuon_lr\": 1.66e-3,\n",
    "    \"normuon_momentum\": 0.95,\n",
    "    \"normuon_beta2\": 0.95,\n",
    "    \"lr\": 3e-4, #adamw lr\n",
    "\n",
    "    \"warmup_pct\": 0.02,\n",
    "    \"wsd_decay_start_pct\": 0.8,  # 1.0 = no decay (start at last epoch)\n",
    "    \"lr_floor\": 0.0,\n",
    "\n",
    "    \"weight_decay\": 0.1,\n",
    "    \"attention_weight_decay\": 0.01,\n",
    "    \"token_embedding_weight_decay\": 0.01,\n",
    "    \"task_embedding_weight_decay\": 0.01,\n",
    "\n",
    "    \"grad_clip\": 1.0,\n",
    "    \"dropout\": 0.1,\n",
    "    \"seed\": 42,\n",
    "\n",
    "    # Model Architecture\n",
    "    \"d_model\": 768,  # 128, 256, 512, 768 | 128, 384, 640\n",
    "    \"n_heads\": 12,  # 4, 8, 8/16, 12 | 4, 12, 10\n",
    "    \"d_ff\": 3072,  # 512, 1024, 2048, 3072 | 512, 1536, 2560\n",
    "    \"n_layers\": 4,  # 4, 6, 16, 16 | 24, 28, 24\n",
    "\n",
    "    \"inference_temperature\": None,\n",
    "    \"inference_top_k\": None,\n",
    "}\n",
    "cfg = argparse.Namespace(**args)\n",
    "\n",
    "runs_dir = Path(\"runs\")\n",
    "runs_dir.mkdir(parents=True, exist_ok=True)\n",
    "with (runs_dir / \"config.txt\").open(\"w\") as f:\n",
    "    for k, v in args.items():\n",
    "        f.write(f\"{k}: {v}\\n\")\n",
    "\n",
    "model, dataset, dataloader, device, data_path = train.build_model_and_data(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfa7324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training only\n",
    "from time import perf_counter\n",
    "\n",
    "t_start = perf_counter()\n",
    "train.train_model(cfg,model=model,dataloader=dataloader,dataset=dataset,device=device,data_path=data_path)\n",
    "t_duration = perf_counter() - t_start\n",
    "\n",
    "print(f\"Training took {t_duration:.2f}s\")\n",
    "with open(Path(\"runs/timing.txt\"), \"w\") as f:\n",
    "    f.write(f\"Training: {t_duration:.4f} s\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c84a83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.cleanup_memory(globals()) # cleaning up memory to run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283309b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pickle\n",
    "import importlib\n",
    "import evaluations\n",
    "import utils\n",
    "importlib.reload(evaluations)\n",
    "importlib.reload(utils)\n",
    "\n",
    "PATH_BOTH = Path(\"assets/challenges.json\")\n",
    "# aug_count = max_augments (0 = no aug)\n",
    "\n",
    "EVAL_CONFIGS = [\n",
    "    # (\"eval_125color_both\", 125, PATH_BOTH),\n",
    "    (\"eval_100color_both\", 100, PATH_BOTH),\n",
    "    # (\"eval_10color_both\", 10, PATH_BOTH),\n",
    "    # (\"eval_0color_both\", 0, PATH_BOTH),\n",
    "    # (\"eval_0color_train\", 0, PATH_TRAIN)  # Uses TRAIN path (No Geom TTA on Test)\n",
    "]\n",
    "\n",
    "EVAL_BATCH_SIZE = 1300\n",
    "SPLITS = [\"test\"]\n",
    "CHECKPOINT_PATH = Path(\"runs/tiny.pt\")\n",
    "SOLUTIONS_PRESENT = False\n",
    "EVAL_TASK_IDS = None  # Set to None to evaluate full dataset, or [\"00576224\", ...] for specific tasks\n",
    "LOG_CORRECT_GRIDS = False  # Print the actual grid, IDs, and augmentation indices for fully correct grids\n",
    "\n",
    "eval_results = evaluations.run_evaluation_configs(\n",
    "    cfg,\n",
    "    EVAL_CONFIGS,\n",
    "    eval_batch_size=EVAL_BATCH_SIZE,\n",
    "    splits=SPLITS,\n",
    "    checkpoint_path=CHECKPOINT_PATH,\n",
    "    include_targets=SOLUTIONS_PRESENT,\n",
    "    task_ids=EVAL_TASK_IDS,\n",
    "    log_correct_grids=LOG_CORRECT_GRIDS,\n",
    ")\n",
    "\n",
    "Path(\"runs/eval_results.pkl\").write_bytes(pickle.dumps(eval_results))\n",
    "print(\"Saved runs/eval_results.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4ce6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "if \"eval_results\" not in globals():\n",
    "    eval_results = pickle.loads(Path(\"runs/eval_results.pkl\").read_bytes())\n",
    "\n",
    "if \"EVAL_CONFIGS\" in globals():\n",
    "    EVAL_SUB_FOLDER = EVAL_CONFIGS[0][0]\n",
    "else:\n",
    "    EVAL_SUB_FOLDER = eval_results[0][0]\n",
    "\n",
    "VIS_MODE = \"!\"  # \"!\" = compare vs solutions, \"submission\" = attempts-only\n",
    "utils.visualize_submissions(Path(\"runs\") / EVAL_SUB_FOLDER / \"submission.json\", solutions_file=\"assets/solutions.json\", mode=VIS_MODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ec3c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scoring\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "if \"eval_results\" not in globals():\n",
    "    eval_results = pickle.loads(Path(\"runs/eval_results.pkl\").read_bytes())\n",
    "\n",
    "EVAL_SUB_FOLDER = eval_results[0][0]\n",
    "SOLUTIONS_FILE = Path(\"assets/solutions.json\")\n",
    "SUBMISSION_FILE = Path(f\"runs/{EVAL_SUB_FOLDER}/submission.json\")\n",
    "\n",
    "score = utils.score_arc_submission(SOLUTIONS_FILE, SUBMISSION_FILE)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
