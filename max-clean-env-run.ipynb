{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hard\u2011check reproduction (official Kaggle dataset)\n",
        "This notebook is the strongest leakage test. It uses the **official ARC Prize 2024 public dataset available on Kaggle**, deletes the eval solutions immediately, builds a clean training/eval dataset, and trains from scratch. Only after inference do we re\u2011download solutions for scoring.\n",
        "\n",
        "## Why this proves there is no leakage\n",
        "- The eval solutions file is deleted before dataset construction and is **not present** during training/inference.\n",
        "- The constructed `/content/challenges_dihedral_both.json` is built from train challenges + train solutions and eval challenges only; eval outputs are absent by construction.\n",
        "- The repo is stripped to `src/*.py`, so there are no hidden files or checkpoints.\n",
        "- Inference uses `SOLUTIONS_PRESENT=False` and produces `submission.json` without any access to solutions.\n",
        "- Scoring happens **after** inference in the final section; you can skip it and score elsewhere.\n",
        "- The core files are written in pytorch/python without external libraries, do not call the internet and there is no checkpoint\n",
        "\n",
        "**Note: this notebook has only been tested on Google Colab**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The original result had 3 runs\n",
        "Training - 40GB A100  \n",
        "Inference - 80GB A100  \n",
        "Total eval tasks - 400  \n",
        "\n",
        "1. 8.75% for 21 cents in lifetime compute (887s training, 404s inference)\n",
        "    - 11 epochs with 10 color augmentations\n",
        "2. 16% for 38 cents in lifetime compute (1629s training, 700s inference)\n",
        "    - 21 epochs with 20 color augmentations\n",
        "3. 27.5% for $1.7 in lifetime compute (7896s training, 2954 inference)\n",
        "    - 101 epochs with 100 color augmentations\n",
        "\n",
        "Note: \n",
        "- All 3 runs above had an extra dataset called ConceptARC added in training. This dataset is clean\n",
        "- To reduce burden of verification, this dataset is removed. Performance drops only slightly\n",
        "\n",
        "**Reproduced result without extra dataset** (same epoch and color augments)  \n",
        "1. 7.88% for 18 cents (720s training, 339s inference)\n",
        "2. 14.38% for 32 cents (1309s training, 636s inference)\n",
        "3. 23.38% for $1.44 (6044s training, 2701s inference)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vmrtcVrWggeg"
      },
      "outputs": [],
      "source": [
        "# Choose reproduction configuration\n",
        "# runconfig = [\"no_concept\", 11]  # runs the fastest, expect 7%\n",
        "runconfig = [\"no_concept\", 21]  # second fastest, expect 7%\n",
        "# runconfig = [\"no_concept\", 101] # expect 23%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## To run this, you need a kaggle legacy API key\n",
        "1. Create a kaggle account\n",
        "2. In settings, generate a Legacy API key (it has to be legacy, the newer api keys seem to have weird problems on Google Colab)\n",
        "    - This downloads a json file with your username and api key\n",
        "3. Copy the username and api key and add it to the cell below\n",
        "4. Remember to expire your api key after verification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b36hWck09hVs",
        "outputId": "e98a3c98-67b2-45e0-e330-713d78855b39"
      },
      "outputs": [],
      "source": [
        "!rm -rf /content/sample_data/\n",
        "import os\n",
        "\n",
        "os.environ[\"KAGGLE_USERNAME\"] = \"USERNAME\"\n",
        "os.environ[\"KAGGLE_KEY\"] = \"LEGACY_API_KEY\"\n",
        "\n",
        "# Check if it works\n",
        "!kaggle competitions list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UiOSswSUHV25",
        "outputId": "6c2503ea-7309-46f4-d4c9-b61a0742bb33"
      },
      "outputs": [],
      "source": [
        "# Download the actual file, unzip\n",
        "# and then delete the solutions (Eval Solutions)\n",
        "# also delete everything other than the required grids (Train Challenges, Train Answers and Evaluation Challenges).\n",
        "!kaggle competitions download -c arc-prize-2024\n",
        "\n",
        "!unzip /content/arc-prize-2024.zip -d /content/arc-prize-2024\n",
        "\n",
        "!rm /content/arc-prize-2024.zip\n",
        "!rm /content/arc-prize-2024/arc-agi_evaluation_solutions.json\n",
        "!rm /content/arc-prize-2024/arc-agi_test_challenges.json\n",
        "!rm /content/arc-prize-2024/sample_submission.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vr51RCu1fVmS"
      },
      "source": [
        "Feel free to inspect the folder.  \n",
        "This is the official public datasets without the solutions file  \n",
        "\n",
        "Nothing else other than train datapoints and test inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3b6XKRo4fVVp",
        "outputId": "2740f683-4f29-41cf-d7bc-320a511baf0e"
      },
      "outputs": [],
      "source": [
        "# creating dataset by combining eval challenges, train challenges and train solutions\n",
        "# and then augments dihedrally\n",
        "# (NO EVAL SOLUTIONS)\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "DATA_ROOT = Path(\"/content/arc-prize-2024\")\n",
        "TRAIN_CHALLENGES = DATA_ROOT / \"arc-agi_training_challenges.json\"\n",
        "TRAIN_SOLUTIONS = DATA_ROOT / \"arc-agi_training_solutions.json\"\n",
        "EVAL_CHALLENGES = DATA_ROOT / \"arc-agi_evaluation_challenges.json\"\n",
        "\n",
        "OUT_PATH = Path(\"/content/challenges_dihedral_both.json\")\n",
        "OUT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "def load_json(path):\n",
        "    with path.open(\"r\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "\n",
        "def extract_output(sol):\n",
        "    return sol[\"output\"] if isinstance(sol, dict) and \"output\" in sol else sol\n",
        "\n",
        "\n",
        "def attach_train_solutions(challenges, solutions):\n",
        "    combined = {}\n",
        "    for task_id, task in challenges.items():\n",
        "        train_pairs = [dict(pair) for pair in task.get(\"train\", [])]\n",
        "        test_pairs = [dict(pair) for pair in task.get(\"test\", [])]\n",
        "        sol_list = solutions.get(task_id)\n",
        "        if sol_list is None:\n",
        "            raise KeyError(f\"Missing solutions for task {task_id}\")\n",
        "        if len(test_pairs) != len(sol_list):\n",
        "            raise ValueError(\n",
        "                f\"Solution count mismatch for {task_id}: {len(test_pairs)} test vs {len(sol_list)} sols\"\n",
        "            )\n",
        "        for pair, sol in zip(test_pairs, sol_list):\n",
        "            pair[\"output\"] = extract_output(sol)\n",
        "        task_copy = dict(task)\n",
        "        task_copy[\"train\"] = train_pairs\n",
        "        task_copy[\"test\"] = test_pairs\n",
        "        combined[task_id] = task_copy\n",
        "    return combined\n",
        "\n",
        "\n",
        "def move_test_to_train(task_map):\n",
        "    moved = {}\n",
        "    for task_id, task in task_map.items():\n",
        "        train_pairs = [dict(pair) for pair in task.get(\"train\", [])]\n",
        "        test_pairs = [dict(pair) for pair in task.get(\"test\", [])]\n",
        "        new_task = dict(task)\n",
        "        new_task[\"train\"] = train_pairs + test_pairs\n",
        "        new_task.pop(\"test\", None)\n",
        "        new_task.pop(\"name\", None)\n",
        "        moved[task_id] = new_task\n",
        "    return moved\n",
        "\n",
        "\n",
        "def merge_task_maps(*maps):\n",
        "    merged = {}\n",
        "    for task_map in maps:\n",
        "        for task_id, task in task_map.items():\n",
        "            if task_id in merged:\n",
        "                raise ValueError(f\"Duplicate task id: {task_id}\")\n",
        "            merged[task_id] = task\n",
        "    return {task_id: merged[task_id] for task_id in sorted(merged)}\n",
        "\n",
        "\n",
        "def copy_grid(grid):\n",
        "    return [list(row) for row in grid]\n",
        "\n",
        "\n",
        "def rotate90(grid):\n",
        "    if not grid:\n",
        "        return []\n",
        "    return [list(row) for row in zip(*grid[::-1])]\n",
        "\n",
        "\n",
        "def rotate180(grid):\n",
        "    return [list(reversed(row)) for row in reversed(grid)]\n",
        "\n",
        "\n",
        "def rotate270(grid):\n",
        "    if not grid:\n",
        "        return []\n",
        "    return [list(row) for row in zip(*grid)][::-1]\n",
        "\n",
        "\n",
        "def flip_horizontal(grid):\n",
        "    return [list(reversed(row)) for row in grid]\n",
        "\n",
        "\n",
        "def flip_vertical(grid):\n",
        "    return [list(row) for row in reversed(grid)]\n",
        "\n",
        "\n",
        "def flip_main_diagonal(grid):\n",
        "    if not grid:\n",
        "        return []\n",
        "    return [list(row) for row in zip(*grid)]\n",
        "\n",
        "\n",
        "def flip_anti_diagonal(grid):\n",
        "    return flip_vertical(rotate90(grid))\n",
        "\n",
        "\n",
        "TRANSFORMS = [\n",
        "    (\"identity\", copy_grid),\n",
        "    (\"rot90\", rotate90),\n",
        "    (\"rot180\", rotate180),\n",
        "    (\"rot270\", rotate270),\n",
        "    (\"flip_horizontal\", flip_horizontal),\n",
        "    (\"flip_vertical\", flip_vertical),\n",
        "    (\"flip_main_diagonal\", flip_main_diagonal),\n",
        "    (\"flip_anti_diagonal\", flip_anti_diagonal),\n",
        "]\n",
        "\n",
        "\n",
        "def augment_pairs(pairs):\n",
        "    augmented = []\n",
        "    for pair in pairs:\n",
        "        input_grid = pair[\"input\"]\n",
        "        output_grid = pair.get(\"output\")\n",
        "        for _, transform in TRANSFORMS:\n",
        "            new_pair = {\"input\": transform(input_grid)}\n",
        "            if output_grid is not None:\n",
        "                new_pair[\"output\"] = transform(output_grid)\n",
        "            augmented.append(new_pair)\n",
        "    return augmented\n",
        "\n",
        "\n",
        "def augment_dataset(challenges):\n",
        "    augmented = {}\n",
        "    for task_id, payload in challenges.items():\n",
        "        new_payload = dict(payload)\n",
        "        if \"train\" in payload:\n",
        "            new_payload[\"train\"] = augment_pairs(list(payload.get(\"train\", [])))\n",
        "        if \"test\" in payload:\n",
        "            new_payload[\"test\"] = augment_pairs(list(payload.get(\"test\", [])))\n",
        "        augmented[task_id] = new_payload\n",
        "    return augmented\n",
        "\n",
        "\n",
        "train_challenges = load_json(TRAIN_CHALLENGES)\n",
        "train_solutions = load_json(TRAIN_SOLUTIONS)\n",
        "eval_challenges = load_json(EVAL_CHALLENGES)\n",
        "\n",
        "train_both = move_test_to_train(\n",
        "    attach_train_solutions(train_challenges, train_solutions)\n",
        ")\n",
        "combined = merge_task_maps(train_both, eval_challenges)\n",
        "\n",
        "augmented = augment_dataset(combined)\n",
        "OUT_PATH.write_text(json.dumps(augmented, indent=2))\n",
        "print(f\"Wrote {OUT_PATH} (tasks: {len(augmented)})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Clean dataset built\n",
        "We now have `/content/challenges_dihedral_both.json`, which contains:\n",
        "- Training inputs + outputs (from the official training set)\n",
        "- Evaluation inputs only (no eval solutions)\n",
        "\n",
        "The original Kaggle dataset folder is deleted next so nothing else remains on disk.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5AbJsujh4rT"
      },
      "source": [
        "For safety, we delete the arc-prize folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZhR7n6ciYMv"
      },
      "outputs": [],
      "source": [
        "!rm -rf /content/arc-prize-2024/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmwFTSGdgxoN"
      },
      "source": [
        "Now we download the model files defined in pytorch\n",
        "\n",
        "Feel free to inspect - there are no checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYRBFT39gJ5u",
        "outputId": "eaac9c9a-9341-4a89-9a1f-1582f7aac32d"
      },
      "outputs": [],
      "source": [
        "root_folder = \"content\"  # for colab\n",
        "\n",
        "%cd /$root_folder/\n",
        "!git clone https://github.com/mvakde/mdlARC.git # `-b <branch_name> --single-branch` if branch\n",
        "%cd /$root_folder/mdlARC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_IZu2ptg_Q3"
      },
      "source": [
        "For safety, we delete every single file other than the 4 python source files\n",
        "\n",
        "You can inspect the source files, there's no hardcoding and there's no internet calls. No external libraries, just pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AojzIR03gduQ"
      },
      "outputs": [],
      "source": [
        "# delete everything\n",
        "!rm -rf /$root_folder/mdlARC/interactive-run.ipynb\n",
        "!rm -rf /$root_folder/mdlARC/clean-env-run.ipynb\n",
        "!rm -rf /$root_folder/mdlARC/dataset_building_scripts\n",
        "!rm -rf /$root_folder/mdlARC/readme.md\n",
        "!rm -rf /$root_folder/mdlARC/img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVFbol6SktPB",
        "outputId": "10d13ada-c087-4cb0-ebfb-1c4292b53080"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import argparse\n",
        "import importlib\n",
        "import sys\n",
        "\n",
        "PROJECT_ROOT = Path.cwd()\n",
        "SRC_DIR = PROJECT_ROOT / \"src\"\n",
        "if SRC_DIR.exists() and str(SRC_DIR) not in sys.path:\n",
        "    sys.path.insert(0, str(SRC_DIR))\n",
        "\n",
        "import utils, tinytransformer, train\n",
        "\n",
        "importlib.reload(utils)  # pick up code changes during iteration\n",
        "importlib.reload(tinytransformer)\n",
        "importlib.reload(train)\n",
        "\n",
        "args = {\n",
        "    # run config\n",
        "    \"num_workers\": 0,\n",
        "    \"device\": \"cuda\",  # 'cuda' | 'mps' | 'cpu'\n",
        "    \"do_validate\": False,\n",
        "    \"name\": \"arc1-37M-bs32-101ep-100color-ccdb\",  # download file name\n",
        "    \"GPU\": \"A100\",  # just for logging purposes\n",
        "    # paths - must pass as Path(\"<path_to_dir>\")\n",
        "    \"train_log_file\": Path(\"runs/training_log.txt\"),\n",
        "    \"save_path\": Path(\"runs/tiny.pt\"),\n",
        "    \"checkpoint_path\": None,  # Path(\"runs/tiny.pt\"),  # or None to start from scratch\n",
        "    \"data_path\": Path(\"../challenges_dihedral_both.json\"),\n",
        "    \"dihedral_augmented\": True,\n",
        "    # hyperparameters\n",
        "    \"epochs\": runconfig[1],\n",
        "    \"batch_size\": 32,\n",
        "    \"val_batch_size\": 300,\n",
        "    \"enable_color_aug_train\": True,\n",
        "    \"enable_color_on_aug_test_split_during_training\": True,\n",
        "    \"max_color_augments_train\": (runconfig[1] - 1),\n",
        "    \"disable_color_aug_last_epochs\": 1,\n",
        "    \"color_aug_seed\": 42,\n",
        "    \"lr\": 3e-4,\n",
        "    \"warmup_pct\": 0.02,\n",
        "    \"wsd_decay_start_pct\": 0.8,  # 1.0 = no decay (start at last epoch)\n",
        "    \"lr_floor\": 0.01,\n",
        "    \"weight_decay\": 0.01,\n",
        "    \"grad_clip\": 1.0,\n",
        "    \"dropout\": 0.1,\n",
        "    \"seed\": 42,\n",
        "    # Model Architecture\n",
        "    \"d_model\": 768,  # 128, 256, 512, 768 | 128, 384, 640\n",
        "    \"n_heads\": 12,  # 4, 8, 8/16, 12 | 4, 12, 10\n",
        "    \"d_ff\": 3072,  # 512, 1024, 2048, 3072 | 512, 1536, 2560\n",
        "    \"n_layers\": 4,  # 4, 6, 16, 16 | 24, 28, 24\n",
        "    # Visibility toggles\n",
        "    \"log_train_strings\": False,\n",
        "    \"log_train_limit\": 10,\n",
        "    \"log_inference_prompt\": False,\n",
        "    \"inference_temperature\": None,\n",
        "    \"inference_top_k\": None,\n",
        "}\n",
        "cfg = argparse.Namespace(**args)\n",
        "\n",
        "runs_dir = Path(\"runs\")\n",
        "runs_dir.mkdir(parents=True, exist_ok=True)\n",
        "with (runs_dir / \"config.txt\").open(\"w\") as f:\n",
        "    for k, v in args.items():\n",
        "        f.write(f\"{k}: {v}\\n\")\n",
        "\n",
        "model, dataset, dataloader, device, data_path = train.build_model_and_data(cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vaSnRxTl-oW"
      },
      "source": [
        "Train the model from scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMdms_ZWmDrh",
        "outputId": "caef1a4a-86d4-48c9-98b7-3b5b6dfe785f"
      },
      "outputs": [],
      "source": [
        "# Training only\n",
        "\n",
        "from time import perf_counter\n",
        "\n",
        "t_start = perf_counter()\n",
        "\n",
        "train.train_model(\n",
        "    cfg,\n",
        "    model=model,\n",
        "    dataloader=dataloader,\n",
        "    dataset=dataset,\n",
        "    device=device,\n",
        "    data_path=data_path,\n",
        ")\n",
        "\n",
        "t_duration = perf_counter() - t_start\n",
        "print(f\"Training took {t_duration:.2f}s\")\n",
        "\n",
        "with open(Path(\"runs/timing.txt\"), \"w\") as f:\n",
        "    f.write(f\"Training: {t_duration:.4f} s\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WY3RPpEtmNEa",
        "outputId": "4b50cd79-8223-4097-d69a-ec0a2a87705d"
      },
      "outputs": [],
      "source": [
        "# cleaning up memory to run inference\n",
        "utils.cleanup_memory(globals())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KsPC2GEQmP2C",
        "outputId": "75f31624-cb9c-47ab-a7f1-3af0adad9f7d"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import importlib\n",
        "import evaluations\n",
        "import utils\n",
        "\n",
        "importlib.reload(evaluations)\n",
        "importlib.reload(utils)\n",
        "\n",
        "PATH_BOTH = Path(\"../challenges_dihedral_both.json\")\n",
        "\n",
        "EVAL_CONFIGS = [(\"eval\", runconfig[1] - 1, PATH_BOTH)]\n",
        "\n",
        "EVAL_BATCH_SIZE = 1300\n",
        "SPLITS = [\"test\"]\n",
        "CHECKPOINT_PATH = Path(\"runs/tiny.pt\")\n",
        "SOLUTIONS_PRESENT = False\n",
        "EVAL_TASK_IDS = None  # Set to None to evaluate full dataset, or [\"00576224\", ...] for specific tasks\n",
        "LOG_CORRECT_GRIDS = False  # Print the actual grid, IDs, and augmentation indices for fully correct grids\n",
        "\n",
        "eval_results = evaluations.run_evaluation_configs(\n",
        "    cfg,\n",
        "    EVAL_CONFIGS,\n",
        "    eval_batch_size=EVAL_BATCH_SIZE,\n",
        "    splits=SPLITS,\n",
        "    checkpoint_path=CHECKPOINT_PATH,\n",
        "    include_targets=SOLUTIONS_PRESENT,\n",
        "    task_ids=EVAL_TASK_IDS,\n",
        "    log_correct_grids=LOG_CORRECT_GRIDS,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "B7Z22D7LpB2K",
        "outputId": "8e079b9b-2d4c-4b29-d529-0c9bc0c12658"
      },
      "outputs": [],
      "source": [
        "# visualisation WITHOUT loading solutions\n",
        "EVAL_SUB_FOLDER = EVAL_CONFIGS[0][0]\n",
        "utils.visualize_eval_submissions(EVAL_SUB_FOLDER, mode=\"submission\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWY0sxcTqRqN"
      },
      "source": [
        "## Stop here for a strict no\u2011solutions run\n",
        "At this point, the model has already produced `runs/<run_name>/submission.json` without any access to solutions. **This means the run is clean!**.\n",
        "\n",
        "**The next 2 cells score the submission and visualise differences with ground truth. This requires downloading the solutions**\n",
        "\n",
        "If you want a strict no\u2011solutions audit, stop here and score the submission yourself manually"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36jsnT-TqRFU",
        "outputId": "a9471fd7-3ce6-4ee2-fda0-be4cc6f1aed5"
      },
      "outputs": [],
      "source": [
        "# Download dataset, unzip and delete everything except solutions\n",
        "%cd /content\n",
        "!kaggle competitions download -c arc-prize-2024\n",
        "\n",
        "!unzip /content/arc-prize-2024.zip -d /content/arc-prize-2024\n",
        "\n",
        "!rm /content/arc-prize-2024.zip\n",
        "!rm /content/arc-prize-2024/arc-agi_training_challenges.json\n",
        "!rm /content/arc-prize-2024/arc-agi_training_solutions.json\n",
        "!rm /content/arc-prize-2024/arc-agi_evaluation_challenges.json\n",
        "!rm /content/arc-prize-2024/arc-agi_test_challenges.json\n",
        "!rm /content/arc-prize-2024/sample_submission.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sREwkBg5pDO3",
        "outputId": "abbd8116-891e-407a-b9fd-a2fde8fdcecc"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import utils\n",
        "\n",
        "SOLUTIONS_FILE = Path(\"arc-prize-2024/arc-agi_evaluation_solutions.json\")\n",
        "SUBMISSION_FILE = Path(f\"mdlARC/runs/{EVAL_SUB_FOLDER}/submission.json\")\n",
        "\n",
        "utils.score_arc_submission(SOLUTIONS_FILE, SUBMISSION_FILE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZB6jd6ASpGGT",
        "outputId": "508cad30-8ccc-4de5-cdc1-315f9a7de1c6"
      },
      "outputs": [],
      "source": [
        "# Visualise and compare the differences between the ground truth solutions and the correct answers\n",
        "EVAL_SUB_FOLDER = EVAL_CONFIGS[0][0]\n",
        "utils.visualize_eval_submissions(\n",
        "    EVAL_SUB_FOLDER,\n",
        "    submission_base=\"mdlARC/runs\",\n",
        "    solutions_file=\"arc-prize-2024/arc-agi_evaluation_solutions.json\",\n",
        "    mode=\"compare\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# AAIVR flow visualization for one task/pair (augmented inputs + outputs)\n",
        "import aaivr\n",
        "\n",
        "AAIVR_CONFIG_INDEX = 0  # which eval config to inspect\n",
        "AAIVR_TASK_ID = None  # set to a task id string to override task index\n",
        "AAIVR_TASK_INDEX = 0  # 0-based in evaluation pipeline order\n",
        "AAIVR_INPUT_INDEX = 0  # which test pair (base index before dihedral aug)\n",
        "\n",
        "AAIVR_DATASET_PATH = EVAL_CONFIGS[AAIVR_CONFIG_INDEX][2]\n",
        "AAIVR_MAX_COLOR_AUG = EVAL_CONFIGS[AAIVR_CONFIG_INDEX][1]\n",
        "if len(EVAL_CONFIGS[AAIVR_CONFIG_INDEX]) > 3:\n",
        "    AAIVR_DIHEDRAL = bool(EVAL_CONFIGS[AAIVR_CONFIG_INDEX][3])\n",
        "else:\n",
        "    AAIVR_DIHEDRAL = bool(getattr(cfg, \"dihedral_augmented\", False))\n",
        "\n",
        "AAIVR_COLOR_SEED = getattr(cfg, \"color_aug_seed_eval\", None)\n",
        "if AAIVR_COLOR_SEED is None:\n",
        "    AAIVR_COLOR_SEED = getattr(cfg, \"color_aug_seed\", None)\n",
        "if AAIVR_COLOR_SEED is None:\n",
        "    AAIVR_COLOR_SEED = getattr(cfg, \"seed\", 42)\n",
        "\n",
        "aaivr.visualize_aaivr_flow(\n",
        "    eval_results[AAIVR_CONFIG_INDEX][1][\"test\"][\"results\"],\n",
        "    dataset_path=AAIVR_DATASET_PATH,\n",
        "    input_index=AAIVR_INPUT_INDEX,\n",
        "    task_id=AAIVR_TASK_ID,\n",
        "    task_index=AAIVR_TASK_INDEX,\n",
        "    is_dihedral_augmented=AAIVR_DIHEDRAL,\n",
        "    max_color_augments=AAIVR_MAX_COLOR_AUG,\n",
        "    color_aug_seed=AAIVR_COLOR_SEED,\n",
        ")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}